{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i6lkqeev8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "# Clear GPU memory and set environment\n",
    "torch.cuda.empty_cache()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def setup_llava_model():\n",
    "    \"\"\"Setup LLaVA model with proper error handling and retries\"\"\"\n",
    "    print(\"üöÄ Setting up LLaVA model...\")\n",
    "    \n",
    "    # Use LLaVA-NeXT which is more stable and faster to download\n",
    "    model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üì• Loading processor from {model_id}...\")\n",
    "        processor = LlavaNextProcessor.from_pretrained(model_id)\n",
    "        print(\"‚úÖ Processor loaded!\")\n",
    "        \n",
    "        print(f\"üì• Loading model from {model_id}...\")\n",
    "        model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"‚úÖ Model loaded!\")\n",
    "        \n",
    "        return processor, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {model_id}: {e}\")\n",
    "        print(\"üîÑ Trying alternative model...\")\n",
    "        \n",
    "        # Fallback to smaller, more reliable model\n",
    "        fallback_model = \"llava-hf/llava-1.5-7b-hf\"\n",
    "        try:\n",
    "            print(f\"üì• Loading fallback processor from {fallback_model}...\")\n",
    "            processor = LlavaNextProcessor.from_pretrained(fallback_model)\n",
    "            print(\"‚úÖ Fallback processor loaded!\")\n",
    "            \n",
    "            print(f\"üì• Loading fallback model from {fallback_model}...\")\n",
    "            model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "                fallback_model,\n",
    "                torch_dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            print(\"‚úÖ Fallback model loaded!\")\n",
    "            \n",
    "            return processor, model\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Fallback also failed: {e2}\")\n",
    "            raise Exception(\"Both primary and fallback models failed to load\")\n",
    "\n",
    "def process_single_image_llava(image_path, processor, model, output_folder):\n",
    "    \"\"\"Process a single image with LLaVA\"\"\"\n",
    "    filename = os.path.basename(image_path)\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "    \n",
    "    try:\n",
    "        # Load and prepare image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Resize if too large (important for GPU memory)\n",
    "        max_size = 512\n",
    "        if max(image.size) > max_size:\n",
    "            image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Prepare conversation\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"Extract all text from this image. Provide only the extracted text without any additional commentary or explanations.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template and tokenize\n",
    "        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        # Generate with optimized settings\n",
    "        print(f\"üîç Processing {filename}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=False,\n",
    "                temperature=0.1,\n",
    "                pad_token_id=processor.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = processor.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Clean up the output - extract only the assistant's response\n",
    "        if \"assistant\" in generated_text.lower():\n",
    "            # Find the last occurrence of assistant and extract text after it\n",
    "            assistant_pos = generated_text.lower().rfind(\"assistant\")\n",
    "            if assistant_pos != -1:\n",
    "                extracted_text = generated_text[assistant_pos + len(\"assistant\"):].strip()\n",
    "            else:\n",
    "                extracted_text = generated_text.strip()\n",
    "        else:\n",
    "            extracted_text = generated_text.strip()\n",
    "        \n",
    "        # Remove any remaining conversation artifacts\n",
    "        lines = extracted_text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and not any(skip in line.lower() for skip in ['user:', 'assistant:', 'extract all text', 'provide only']):\n",
    "                cleaned_lines.append(line)\n",
    "        \n",
    "        final_text = '\\n'.join(cleaned_lines).strip()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Save result\n",
    "        with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_text)\n",
    "        \n",
    "        print(f\"‚úÖ Processed in {end_time - start_time:.2f}s\")\n",
    "        if final_text:\n",
    "            preview = final_text[:100] + \"...\" if len(final_text) > 100 else final_text\n",
    "            print(f\"üìù Extracted: {preview}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No text found\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "        # Save error file\n",
    "        with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"ERROR: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def llava_ocr_main(images_folder=\"part_2_images\", output_folder=\"extracted_text/llava_fixed\"):\n",
    "    \"\"\"Main LLaVA OCR function\"\"\"\n",
    "    print(\"üî• STARTING FIXED LLaVA OCR PROCESSING üî•\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Setup model\n",
    "    try:\n",
    "        processor, model = setup_llava_model()\n",
    "    except Exception as e:\n",
    "        print(f\"üíÄ Failed to setup model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Find images\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"‚ùå No images found in {images_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üñºÔ∏è Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Process images one by one\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        print(f\"\\nüì∑ [{i}/{len(image_files)}] Processing: {os.path.basename(image_path)}\")\n",
    "        \n",
    "        success = process_single_image_llava(image_path, processor, model, output_folder)\n",
    "        \n",
    "        if success:\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "        \n",
    "        # Clear GPU memory after each image\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nüéâ LLaVA OCR COMPLETE! üéâ\")\n",
    "    print(f\"‚úÖ Successful: {successful}\")\n",
    "    print(f\"‚ùå Failed: {failed}\")\n",
    "    print(f\"üìÅ Results saved to: {output_folder}\")\n",
    "\n",
    "# Run the fixed LLaVA implementation\n",
    "if __name__ == \"__main__\":\n",
    "    llava_ocr_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l389tfphgx8",
   "metadata": {},
   "source": [
    "# FIXED LLaVA Implementation - WORKING VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hv2b614vjfw",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import easyocr\n",
    "import pyttsx3\n",
    "from langdetect import detect\n",
    "import time\n",
    "\n",
    "def simple_ocr_solution(images_folder=\"part_2_images\", output_folder=\"extracted_text\"):\n",
    "    \"\"\"\n",
    "    Simple working OCR solution using EasyOCR that actually works\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "    # Initialize EasyOCR with multiple languages\n",
    "    print(\"Initializing EasyOCR...\")\n",
    "    reader = easyocr.Reader(['en', 'ar', 'ur'], gpu=True, verbose=False)\n",
    "    print(\"‚úÖ EasyOCR ready!\")\n",
    "    \n",
    "    # Initialize TTS\n",
    "    tts_engine = pyttsx3.init()\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"‚ùå No images found in {images_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    successful_extractions = 0\n",
    "    \n",
    "    # Process each image\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "        print(f\"\\n[{i}/{len(image_files)}] Processing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Read image with OpenCV\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"‚ùå Could not read image: {image_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract text using EasyOCR\n",
    "            start_time = time.time()\n",
    "            results = reader.readtext(image, paragraph=True, detail=0)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Combine all text\n",
    "            extracted_text = \" \".join(results).strip()\n",
    "            \n",
    "            if extracted_text:\n",
    "                # Try to detect language\n",
    "                try:\n",
    "                    detected_lang = detect(extracted_text)\n",
    "                    print(f\"‚úÖ Text extracted in {end_time - start_time:.2f}s - Language: {detected_lang}\")\n",
    "                    \n",
    "                    # Save to file with language info\n",
    "                    with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(f\"LANG:{detected_lang}\\n\")\n",
    "                        f.write(extracted_text)\n",
    "                    \n",
    "                    print(f\"üíæ Saved to: {text_file_path}\")\n",
    "                    print(f\"üìù Preview: {extracted_text[:100]}...\")\n",
    "                    \n",
    "                    successful_extractions += 1\n",
    "                    \n",
    "                except Exception as lang_error:\n",
    "                    print(f\"‚ö†Ô∏è Language detection failed: {lang_error}\")\n",
    "                    with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(\"LANG:unknown\\n\")\n",
    "                        f.write(extracted_text)\n",
    "                    successful_extractions += 1\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No text detected\")\n",
    "                # Save empty file\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(\"LANG:none\\n\")\n",
    "                    f.write(\"\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n=== Processing Complete ===\")\n",
    "    print(f\"‚úÖ Successfully extracted text from {successful_extractions}/{len(image_files)} images\")\n",
    "    \n",
    "    # Ask if user wants to read files aloud\n",
    "    if successful_extractions > 0:\n",
    "        try:\n",
    "            response = input(f\"\\nüîä Read {successful_extractions} text files aloud? (y/n): \").lower()\n",
    "            if response in ['y', 'yes']:\n",
    "                read_text_files_aloud(output_folder, tts_engine)\n",
    "        except:\n",
    "            print(\"Skipping TTS...\")\n",
    "    \n",
    "    return output_folder\n",
    "\n",
    "def read_text_files_aloud(text_folder, tts_engine):\n",
    "    \"\"\"Read text files aloud\"\"\"\n",
    "    text_files = glob.glob(os.path.join(text_folder, \"*.txt\"))\n",
    "    \n",
    "    for i, text_file in enumerate(text_files, 1):\n",
    "        print(f\"\\nüîä Reading file {i}/{len(text_files)}: {os.path.basename(text_file)}\")\n",
    "        \n",
    "        try:\n",
    "            with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            if len(lines) > 1:\n",
    "                content = \"\".join(lines[1:]).strip()  # Skip language line\n",
    "                if content:\n",
    "                    tts_engine.say(content)\n",
    "                    tts_engine.runAndWait()\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    print(\"No content to read\")\n",
    "            else:\n",
    "                print(\"File is empty\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {text_file}: {e}\")\n",
    "\n",
    "# Run the simple solution\n",
    "if __name__ == \"__main__\":\n",
    "    simple_ocr_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1zvuqr88me",
   "metadata": {},
   "source": [
    "# Working Simple OCR Solution (EasyOCR + TTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qp57ab7sq9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download LLaVA model in smaller chunks with retries\n",
    "import os\n",
    "import time\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Fix for slow downloads\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '0'\n",
    "os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '3600'  # 1 hour timeout\n",
    "\n",
    "def download_llava_model():\n",
    "    \"\"\"Download LLaVA model with retry mechanism\"\"\"\n",
    "    max_retries = 3\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries}\")\n",
    "            \n",
    "            # Try processor first\n",
    "            print(\"Downloading processor...\")\n",
    "            processor = AutoProcessor.from_pretrained(\n",
    "                \"llava-hf/llava-1.5-7b-hf\", \n",
    "                use_fast=False,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=\"./models\"  # Local cache\n",
    "            )\n",
    "            print(\"‚úÖ Processor downloaded!\")\n",
    "            \n",
    "            # Then model\n",
    "            print(\"Downloading model (this will take a while)...\")\n",
    "            model = LlavaForConditionalGeneration.from_pretrained(\n",
    "                \"llava-hf/llava-1.5-7b-hf\",\n",
    "                torch_dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=\"./models\"  # Local cache\n",
    "            )\n",
    "            print(\"‚úÖ Model downloaded successfully!\")\n",
    "            \n",
    "            return processor, model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"Retrying in 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(\"All attempts failed. Check your internet connection.\")\n",
    "                return None, None\n",
    "\n",
    "# Run the download\n",
    "processor, model = download_llava_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uq1mto0emu9",
   "metadata": {},
   "source": [
    "# Working LLaVA Setup (Fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-report-2025",
   "metadata": {},
   "source": [
    "## Environment Report\n",
    "\n",
    "**Current Package Versions (Baseline before LLAVA/SHIKRA integration):**\n",
    "\n",
    "- **Python**: 3.13.3\n",
    "- **torch**: 2.7.1+cu128\n",
    "- **torchvision**: 0.22.1+cu128\n",
    "- **torchaudio**: 2.7.1+cu128\n",
    "- **easyocr**: 1.7.2\n",
    "- **opencv-python**: 4.12.0.88\n",
    "- **pillow**: 11.2.1\n",
    "- **numpy**: 2.2.6\n",
    "- **pyttsx3**: 2.99\n",
    "- **langdetect**: 1.0.9\n",
    "- **transformers**: 4.52.3\n",
    "- **gradio**: 5.31.0\n",
    "- **CUDA Available**: Yes\n",
    "- **CUDA Version**: 12.8\n",
    "- **GPU**: NVIDIA GeForce RTX 2060\n",
    "- **GPU Memory**: 12.88 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05407b00",
   "metadata": {},
   "source": [
    "# Imports and Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932d8684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osama/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-11 23:06:10.304046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752257170.342471    5747 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752257170.353169    5747 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752257170.428920    5747 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752257170.428954    5747 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752257170.428957    5747 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752257170.428960    5747 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-11 23:06:10.440418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLAVA OCR Processing ===\n",
      "GPU: NVIDIA GeForce RTX 2050\n",
      "GPU Memory: 3.96 GB\n",
      "HuggingFace cache location: default\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import easyocr\n",
    "import pyttsx3\n",
    "from langdetect import detect\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "## Moving Hugging Face default Download Dir\n",
    "\n",
    "# # LLAVA OCR Cell - GPU Accelerated Implementation\n",
    "# import os\n",
    "\n",
    "# # IMPORTANT: Set cache BEFORE any imports from transformers/huggingface\n",
    "# os.environ['HF_HOME'] = 'D:/HuggingFaceCache'\n",
    "# os.environ['TRANSFORMERS_CACHE'] = 'D:/HuggingFaceCache/transformers'\n",
    "# os.environ['HUGGINGFACE_HUB_CACHE'] = 'D:/HuggingFaceCache/hub'\n",
    "\n",
    "# # Now import everything else\n",
    "# import glob\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# import time\n",
    "# import gc\n",
    "# from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "# Force GPU usage\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Check GPU availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: CUDA not available. LLAVA requires GPU acceleration.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"=== LLAVA OCR Processing ===\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"HuggingFace cache location: {os.environ.get('HF_HOME', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8207ed",
   "metadata": {},
   "source": [
    "# Simple TTS using pyttsx3 Gpu based:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force GPU usage and optimization for RTX 2060\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU\n",
    "torch.backends.cudnn.benchmark = True  # Enable cudnn autotuner for performance\n",
    "\n",
    "# Check GPU and setup\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: CUDA not available. This script requires GPU acceleration.\")\n",
    "    print(\"Please check your NVIDIA drivers and PyTorch installation.\")\n",
    "    exit(1)\n",
    "\n",
    "# Display GPU information\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Initialize EasyOCR reader with explicit GPU settings\n",
    "print(\"Initializing EasyOCR with GPU acceleration...\")\n",
    "reader = easyocr.Reader(\n",
    "    [\"ar\", \"ur\", \"en\"], \n",
    "    gpu=True,\n",
    "    verbose=False,\n",
    "    # For RTX 2060, set reasonable batch size and model parameters\n",
    "    detector=True,\n",
    "    recognizer=True\n",
    ")\n",
    "\n",
    "# Initialize TTS engine\n",
    "tts_engine = pyttsx3.init()\n",
    "\n",
    "def log_gpu_memory():\n",
    "    \"\"\"Log current GPU memory usage\"\"\"\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "\n",
    "def process_images_ocr_save_text(images_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process all images in the folder with GPU-accelerated OCR and save to text files\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output folder: {output_folder}\")\n",
    "    \n",
    "    # Get all image files in the folder\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Report initial GPU memory\n",
    "    log_gpu_memory()\n",
    "\n",
    "    # Process files\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "        print(f\"\\n--- Processing Image {i}/{len(image_files)}: {image_filename} ---\")\n",
    "\n",
    "        try:\n",
    "            # Read image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Could not read image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Optimize image for GPU processing (resize large images)\n",
    "            h, w = image.shape[:2]\n",
    "            max_dim = 2000  # Optimal for RTX 2060 memory\n",
    "            if max(h, w) > max_dim:\n",
    "                scale = max_dim / max(h, w)\n",
    "                image = cv2.resize(image, (int(w * scale), int(h * scale)))\n",
    "                print(f\"Resized image to {image.shape[1]}x{image.shape[0]} to optimize GPU memory\")\n",
    "            \n",
    "            # Report GPU memory before OCR\n",
    "            log_gpu_memory()\n",
    "\n",
    "            # Extract text using GPU-accelerated EasyOCR\n",
    "            print(\"Extracting text with GPU acceleration...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # For RTX 2060, use appropriate batch size\n",
    "            results = reader.readtext(\n",
    "                image,\n",
    "                batch_size=2,  # Adjust based on your GPU memory\n",
    "                paragraph=True,  # Group text into paragraphs\n",
    "                detail=0  # 0 for more accuracy\n",
    "            )\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "            # Combine all detected text with confidence filtering\n",
    "            extracted_text = \" \".join(\n",
    "                [result[1] for result in results if result[2] > 0.5]\n",
    "            )  # confidence > 0.5\n",
    "\n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Extracted text: {extracted_text}\")\n",
    "            \n",
    "            # Try to detect language\n",
    "            try:\n",
    "                detected_lang = detect(extracted_text)\n",
    "                print(f\"Detected language: {detected_lang}\")\n",
    "                \n",
    "                # Save text with language information to file\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                    text_file.write(f\"LANG:{detected_lang}\\n\")\n",
    "                    text_file.write(extracted_text)\n",
    "                \n",
    "                print(f\"Saved text to: {text_file_path}\")\n",
    "                \n",
    "            except Exception as lang_error:\n",
    "                print(f\"Language detection failed: {lang_error}\")\n",
    "                # Save text without language information\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                    text_file.write(f\"LANG:unknown\\n\")\n",
    "                    text_file.write(extracted_text)\n",
    "                \n",
    "                print(f\"Saved text to: {text_file_path} (language unknown)\")\n",
    "\n",
    "            # Clear GPU memory after each image\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            # Clear GPU memory on error\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "    # Final GPU memory cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"\\n--- OCR Processing Complete ---\")\n",
    "    log_gpu_memory()\n",
    "    \n",
    "    return output_folder\n",
    "\n",
    "def read_text_files_aloud(text_folder):\n",
    "    \"\"\"\n",
    "    Read all text files in the folder aloud using TTS, one by one with clear separation\n",
    "    \"\"\"\n",
    "    # Get all text files\n",
    "    text_files = glob.glob(os.path.join(text_folder, \"*.txt\"))\n",
    "    \n",
    "    if not text_files:\n",
    "        print(f\"No text files found in {text_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(text_files)} text files to read\")\n",
    "    \n",
    "    # Language mapping for TTS\n",
    "    lang_mapping = {\n",
    "        \"en\": \"english\",\n",
    "        \"ar\": \"arabic\",\n",
    "        \"ur\": \"urdu\",\n",
    "        \"hi\": \"hindi\",\n",
    "        \"fa\": \"persian\",\n",
    "        \"ps\": \"pashto\"\n",
    "    }\n",
    "    \n",
    "    # Sort text files alphabetically to ensure consistent reading order\n",
    "    text_files.sort()\n",
    "    \n",
    "    for i, text_file_path in enumerate(text_files, 1):\n",
    "        file_name = os.path.basename(text_file_path)\n",
    "        print(f\"\\n===== Reading File {i}/{len(text_files)}: {file_name} =====\")\n",
    "        \n",
    "        # Announce the file being read (optional)\n",
    "        tts_engine.setProperty(\"rate\", 150)\n",
    "        announcement = f\"Reading file {i} of {len(text_files)}: {os.path.splitext(file_name)[0]}\"\n",
    "        print(announcement)\n",
    "        tts_engine.say(announcement)\n",
    "        tts_engine.runAndWait()\n",
    "        \n",
    "        # Pause between announcement and content\n",
    "        time.sleep(1)\n",
    "        \n",
    "        try:\n",
    "            # Read text file\n",
    "            with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                \n",
    "            if not lines:\n",
    "                print(f\"File is empty: {text_file_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Extract language information from first line\n",
    "            lang_line = lines[0].strip()\n",
    "            if lang_line.startswith(\"LANG:\"):\n",
    "                detected_lang = lang_line[5:]\n",
    "                print(f\"Language: {detected_lang}\")\n",
    "                # Remove the language line\n",
    "                content = \"\".join(lines[1:])\n",
    "            else:\n",
    "                # No language information, treat all lines as content\n",
    "                detected_lang = \"unknown\"\n",
    "                content = \"\".join(lines)\n",
    "            \n",
    "            if not content.strip():\n",
    "                print(\"No content to read\")\n",
    "                continue\n",
    "                \n",
    "            # Print a preview of the content\n",
    "            content_preview = content[:100] + \"...\" if len(content) > 100 else content\n",
    "            print(f\"Text content: {content_preview}\")\n",
    "            \n",
    "            # Get available voices\n",
    "            voices = tts_engine.getProperty(\"voices\")\n",
    "            \n",
    "            # Try to set appropriate voice based on language\n",
    "            voice_set = False\n",
    "            for voice in voices:\n",
    "                tts_lang = lang_mapping.get(detected_lang, \"english\")\n",
    "                if (tts_lang.lower() in voice.name.lower() or \n",
    "                    detected_lang in voice.id.lower()):\n",
    "                    tts_engine.setProperty(\"voice\", voice.id)\n",
    "                    voice_set = True\n",
    "                    print(f\"Using voice: {voice.name}\")\n",
    "                    break\n",
    "            \n",
    "            if not voice_set:\n",
    "                print(f\"No specific voice found for {detected_lang}, using default\")\n",
    "            \n",
    "            # Adjust speech rate based on language\n",
    "            if detected_lang in [\"ar\", \"ur\"]:\n",
    "                # Slower for Arabic and Urdu\n",
    "                tts_engine.setProperty(\"rate\", 130)\n",
    "            else:\n",
    "                tts_engine.setProperty(\"rate\", 150)\n",
    "            \n",
    "            # Read the text aloud\n",
    "            print(f\"Reading text aloud...\")\n",
    "            tts_engine.say(content)\n",
    "            tts_engine.runAndWait()\n",
    "            \n",
    "            # Pause between files to clearly separate them\n",
    "            print(\"Finished reading file.\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {text_file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n===== All Text Files Have Been Read =====\")\n",
    "\n",
    "def get_user_confirmation():\n",
    "    \"\"\"\n",
    "    Ask the user if they want to proceed to the TTS reading phase\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        response = input(\"\\nOCR processing complete. Proceed with reading text files? (y/n): \").lower()\n",
    "        if response in ['y', 'yes']:\n",
    "            return True\n",
    "        elif response in ['n', 'no']:\n",
    "            return False\n",
    "        else:\n",
    "            print(\"Please enter 'y' or 'n'\")\n",
    "\n",
    "# Main process\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Define folders\n",
    "        images_folder = \"part_2_images\"\n",
    "        output_folder = \"extracted_text\"\n",
    "        \n",
    "        # Print GPU optimization message\n",
    "        print(\"=== Running GPU-Optimized OCR for RTX 2060 ===\")\n",
    "        \n",
    "        # Step 1: Process images with OCR and save text\n",
    "        start_time = time.time()\n",
    "        text_folder = process_images_ocr_save_text(images_folder, output_folder)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"OCR processing completed in {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Optional: Ask for user confirmation before proceeding to TTS\n",
    "        if get_user_confirmation():\n",
    "            # Step 2: Read the saved text files aloud one by one\n",
    "            read_text_files_aloud(text_folder)\n",
    "        else:\n",
    "            print(\"TTS reading canceled. Text files are saved in the output folder.\")\n",
    "        \n",
    "        print(\"Process completed successfully!\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user\")\n",
    "        # Clean up GPU memory on interrupt\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Clean up GPU memory on error\n",
    "        torch.cuda.empty_cache()\n",
    "    finally:\n",
    "        # Final cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a smaller vision model first\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\", use_fast=False)\n",
    "print(\"Downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15f199",
   "metadata": {},
   "source": [
    "# LLava Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9acecd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLAVA OCR Processing ===\n",
      "GPU: NVIDIA GeForce RTX 2050\n",
      "GPU Memory: 3.96 GB\n",
      "Using GPU acceleration\n",
      "Output folder: extracted_text/llava\n",
      "Loading LLAVA model...\n",
      "Downloading/Loading processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/3 [05:47<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 477\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# Run LLAVA OCR processing\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# Uncomment the line below if you need to install requirements\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# install_requirements()\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m     \u001b[43mprocess_images_with_llava\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 224\u001b[0m, in \u001b[0;36mprocess_images_with_llava\u001b[0;34m(images_folder, output_folder)\u001b[0m\n\u001b[1;32m    219\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    220\u001b[0m     model_name, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    221\u001b[0m )\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading/Loading model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 224\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlavaForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use fp16 only if GPU available\u001b[39;49;00m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use auto device mapping only if GPU available\u001b[39;49;00m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# Move model to appropriate device\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_gpu \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mis_cuda:\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/transformers/modeling_utils.py:3351\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3350\u001b[0m     \u001b[38;5;66;03m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3351\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3360\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3363\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3367\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3368\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3369\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3370\u001b[0m ):\n\u001b[1;32m   3371\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/transformers/utils/hub.py:1017\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1016\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/transformers/utils/hub.py:389\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1006\u001b[0m     )\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1161\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1161\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1174\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1725\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1719\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1720\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo, but the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1721\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to regular HTTP download. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1722\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1723\u001b[0m             )\n\u001b[0;32m-> 1725\u001b[0m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1734\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1735\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:494\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    492\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    496\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/urllib3/response.py:1091\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1091\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1094\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/urllib3/response.py:980\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 980\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/urllib3/response.py:904\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    901\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/urllib3/response.py:887\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # LLAVA OCR Cell - GPU Accelerated Implementation\n",
    "\n",
    "# # Force GPU usage\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# os.environ['HF_HOME'] = 'D:/HuggingFaceCache'\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# # Check GPU availability\n",
    "# if not torch.cuda.is_available():\n",
    "#     print(\"ERROR: CUDA not available. LLAVA requires GPU acceleration.\")\n",
    "#     exit(1)\n",
    "\n",
    "# print(\"=== LLAVA OCR Processing ===\")\n",
    "# print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "# print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# def process_images_with_llava(images_folder=\"part_2_images\", output_folder=\"extracted_text/llava\"):\n",
    "#     \"\"\"\n",
    "#     Process images using LLAVA model for OCR with GPU acceleration\n",
    "#     \"\"\"\n",
    "#     # Create output directory if it doesn't exist\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     print(f\"Output folder: {output_folder}\")\n",
    "\n",
    "#     # # Clear GPU memory before starting\n",
    "#     # torch.cuda.empty_cache()\n",
    "#     # gc.collect()\n",
    "\n",
    "#     # Initialize LLAVA model and processor\n",
    "#     print(\"Loading LLAVA model...\")\n",
    "#     try:\n",
    "#         # TODO: Replace with your specific LLAVA model path/name\n",
    "#         model_name = \"llava-hf/llava-1.5-7b-hf\"  # Example model name\n",
    "\n",
    "#         processor = AutoProcessor.from_pretrained(model_name, use_fast=True)\n",
    "#         model = LlavaForConditionalGeneration.from_pretrained(\n",
    "#             model_name,\n",
    "#             torch_dtype=torch.float16,  # Use fp16 for RTX 2060 efficiency\n",
    "#             device_map=\"cuda\",\n",
    "#             low_cpu_mem_usage=True\n",
    "#         )\n",
    "#         model.eval()\n",
    "#         print(\"LLAVA model loaded successfully!\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading LLAVA model: {e}\")\n",
    "#         print(\"Please ensure you have the correct model name/path\")\n",
    "#         return\n",
    "\n",
    "#     # Get all image files\n",
    "#     image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "#     image_files = []\n",
    "\n",
    "#     for ext in image_extensions:\n",
    "#         image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "#         image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "\n",
    "#     if not image_files:\n",
    "#         print(f\"No images found in {images_folder}\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Found {len(image_files)} images to process\")\n",
    "\n",
    "#     # Process each image\n",
    "#     processed_count = 0\n",
    "#     skipped_count = 0\n",
    "\n",
    "#     for i, image_path in enumerate(image_files, 1):\n",
    "#         image_filename = os.path.basename(image_path)\n",
    "#         base_name = os.path.splitext(image_filename)[0]\n",
    "#         text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "\n",
    "#         # Check if text file already exists (efficiency check)\n",
    "#         if os.path.exists(text_file_path):\n",
    "#             print(f\"\\n[{i}/{len(image_files)}] Skipping {image_filename} - already processed\")\n",
    "#             skipped_count += 1\n",
    "#             continue\n",
    "\n",
    "#         print(f\"\\n[{i}/{len(image_files)}] Processing {image_filename}...\")\n",
    "\n",
    "#         try:\n",
    "#             # Load and preprocess image\n",
    "#             image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "#             # Resize large images to optimize GPU memory\n",
    "#             max_dim = 1024  # Adjust based on your GPU memory\n",
    "#             if max(image.size) > max_dim:\n",
    "#                 image.thumbnail((max_dim, max_dim), Image.Resampling.LANCZOS)\n",
    "#                 print(f\"Resized image to {image.size} for GPU optimization\")\n",
    "\n",
    "#             # Prepare prompt for OCR task\n",
    "#             prompt = \"USER: <image>\\nExtract all text from this image. Provide only the extracted text without any additional commentary.\\nASSISTANT:\"\n",
    "\n",
    "#             # Process with LLAVA\n",
    "#             start_time = time.time()\n",
    "\n",
    "#             # TODO: Adjust this section based on your specific LLAVA implementation\n",
    "#             inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "#             # Generate text with GPU\n",
    "#             with torch.no_grad():\n",
    "#                 generated_ids = model.generate(\n",
    "#                     **inputs,\n",
    "#                     max_new_tokens=1024,\n",
    "#                     temperature=0.1,  # Low temperature for more accurate OCR\n",
    "#                     do_sample=False,\n",
    "#                     use_cache=True\n",
    "#                 )\n",
    "\n",
    "#             # Decode the generated text\n",
    "#             extracted_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "#             # Remove the prompt from the output\n",
    "#             if \"ASSISTANT:\" in extracted_text:\n",
    "#                 extracted_text = extracted_text.split(\"ASSISTANT:\")[-1].strip()\n",
    "\n",
    "#             end_time = time.time()\n",
    "#             print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "#             if not extracted_text.strip():\n",
    "#                 print(\"No text detected in this image\")\n",
    "#                 # Save empty file to avoid reprocessing\n",
    "#                 with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                     f.write(\"\")\n",
    "#                 continue\n",
    "\n",
    "#             # Save extracted text\n",
    "#             with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(extracted_text)\n",
    "\n",
    "#             print(f\"Saved text to: {text_file_path}\")\n",
    "#             print(f\"Text preview: {extracted_text[:100]}...\" if len(extracted_text) > 100 else f\"Text: {extracted_text}\")\n",
    "\n",
    "#             processed_count += 1\n",
    "\n",
    "#             # Clear GPU memory after each image\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {image_filename}: {e}\")\n",
    "#             # Save error file to avoid reprocessing\n",
    "#             with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(f\"ERROR: {str(e)}\")\n",
    "#             torch.cuda.empty_cache()\n",
    "#             continue\n",
    "\n",
    "#     # Final cleanup\n",
    "#     del model\n",
    "#     del processor\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "#     print(f\"\\n=== LLAVA Processing Complete ===\")\n",
    "#     print(f\"Processed: {processed_count} images\")\n",
    "#     print(f\"Skipped: {skipped_count} images (already processed)\")\n",
    "#     print(f\"Total: {len(image_files)} images\")\n",
    "\n",
    "# # Run LLAVA OCR processing\n",
    "# if __name__ == \"__main__\":\n",
    "#     process_images_with_llava()\n",
    "\n",
    "\n",
    "# LLAVA OCR Cell - GPU Accelerated Implementation\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Force GPU usage\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ['HF_HOME'] = 'D:/HuggingFaceCache'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(\"=== LLAVA OCR Processing ===\")\n",
    "if use_gpu:\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\"\n",
    "    )\n",
    "    print(\"Using GPU acceleration\")\n",
    "else:\n",
    "    print(\"GPU not available - using CPU (will be slower)\")\n",
    "    print(\"For better performance, install CUDA-compatible PyTorch\")\n",
    "\n",
    "\n",
    "def process_images_with_llava(\n",
    "    images_folder=\"part_2_images\", output_folder=\"extracted_text/llava\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Process images using LLAVA model for OCR with GPU acceleration\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "\n",
    "    # Clear GPU memory before starting (only if GPU available)\n",
    "    if use_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Initialize LLAVA model and processor\n",
    "    print(\"Loading LLAVA model...\")\n",
    "    try:\n",
    "        # Updated model name - use the correct LLAVA model\n",
    "        model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "        print(\"Downloading/Loading processor...\")\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_name, use_fast=False, trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        print(\"Downloading/Loading model...\")\n",
    "        model = LlavaForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16\n",
    "            if use_gpu\n",
    "            else torch.float32,  # Use fp16 only if GPU available\n",
    "            device_map=\"auto\"\n",
    "            if use_gpu\n",
    "            else None,  # Use auto device mapping only if GPU available\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Move model to appropriate device\n",
    "        if use_gpu and not next(model.parameters()).is_cuda:\n",
    "            model = model.to(device)\n",
    "        elif not use_gpu:\n",
    "            model = model.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        print(\"LLAVA model loaded successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LLAVA model: {e}\")\n",
    "        print(\"Trying alternative approach...\")\n",
    "\n",
    "        # Alternative: Try different model or installation\n",
    "        try:\n",
    "            # Alternative model names to try\n",
    "            alternative_models = [\n",
    "                \"llava-hf/llava-1.5-13b-hf\",\n",
    "                \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "                \"llava-hf/llava-v1.6-vicuna-7b-hf\",\n",
    "            ]\n",
    "\n",
    "            for alt_model in alternative_models:\n",
    "                try:\n",
    "                    print(f\"Trying {alt_model}...\")\n",
    "                    processor = AutoProcessor.from_pretrained(\n",
    "                        alt_model, trust_remote_code=True\n",
    "                    )\n",
    "                    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "                        alt_model,\n",
    "                        torch_dtype=torch.float16 if use_gpu else torch.float32,\n",
    "                        device_map=\"auto\" if use_gpu else None,\n",
    "                        low_cpu_mem_usage=True,\n",
    "                        trust_remote_code=True,\n",
    "                    )\n",
    "                    if use_gpu and not next(model.parameters()).is_cuda:\n",
    "                        model = model.to(device)\n",
    "                    elif not use_gpu:\n",
    "                        model = model.to(device)\n",
    "                    model.eval()\n",
    "                    print(f\"Successfully loaded {alt_model}!\")\n",
    "                    model_name = alt_model\n",
    "                    break\n",
    "                except Exception as alt_e:\n",
    "                    print(f\"Failed to load {alt_model}: {alt_e}\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(\"All model loading attempts failed.\")\n",
    "                print(\"Please ensure you have installed the requirements:\")\n",
    "                print(\"pip install transformers torch torchvision accelerate\")\n",
    "                print(\"pip install git+https://github.com/huggingface/transformers.git\")\n",
    "                return\n",
    "\n",
    "        except Exception as final_e:\n",
    "            print(f\"Final error: {final_e}\")\n",
    "            return\n",
    "\n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "\n",
    "    # Process each image\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "\n",
    "        # Check if text file already exists (efficiency check)\n",
    "        if os.path.exists(text_file_path):\n",
    "            print(\n",
    "                f\"\\n[{i}/{len(image_files)}] Skipping {image_filename} - already processed\"\n",
    "            )\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[{i}/{len(image_files)}] Processing {image_filename}...\")\n",
    "\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "            # Resize large images to optimize memory\n",
    "            max_dim = 1024 if use_gpu else 512  # Smaller images for CPU processing\n",
    "            if max(image.size) > max_dim:\n",
    "                image.thumbnail((max_dim, max_dim), Image.Resampling.LANCZOS)\n",
    "                print(\n",
    "                    f\"Resized image to {image.size} for {'GPU' if use_gpu else 'CPU'} optimization\"\n",
    "                )\n",
    "\n",
    "            # Prepare prompt for OCR task - Updated format\n",
    "            prompt = \"USER: <image>\\nExtract all text from this image. Provide only the extracted text without any additional commentary.\\nASSISTANT:\"\n",
    "\n",
    "            # Process with LLAVA\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Updated processing approach\n",
    "            inputs = processor(prompt, image, return_tensors=\"pt\")\n",
    "\n",
    "            # Move inputs to appropriate device\n",
    "            if use_gpu:\n",
    "                inputs = {\n",
    "                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                    for k, v in inputs.items()\n",
    "                }\n",
    "            else:\n",
    "                inputs = {\n",
    "                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                    for k, v in inputs.items()\n",
    "                }\n",
    "\n",
    "            # Generate text with appropriate device\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=1024 if use_gpu else 512,  # Reduce tokens for CPU\n",
    "                    temperature=0.1,  # Low temperature for more accurate OCR\n",
    "                    do_sample=False,\n",
    "                    use_cache=True,\n",
    "                    pad_token_id=processor.tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            # Decode the generated text\n",
    "            generated_text = processor.decode(\n",
    "                generated_ids[0], skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Clean up the output - remove prompt\n",
    "            if \"ASSISTANT:\" in generated_text:\n",
    "                extracted_text = generated_text.split(\"ASSISTANT:\")[-1].strip()\n",
    "            else:\n",
    "                extracted_text = generated_text.strip()\n",
    "\n",
    "            # Remove any remaining prompt artifacts\n",
    "            if \"USER:\" in extracted_text:\n",
    "                extracted_text = extracted_text.split(\"USER:\")[-1].strip()\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                # Save empty file to avoid reprocessing\n",
    "                with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(\"\")\n",
    "                continue\n",
    "\n",
    "            # Save extracted text\n",
    "            with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(extracted_text)\n",
    "\n",
    "            print(f\"Saved text to: {text_file_path}\")\n",
    "            print(\n",
    "                f\"Text preview: {extracted_text[:100]}...\"\n",
    "                if len(extracted_text) > 100\n",
    "                else f\"Text: {extracted_text}\"\n",
    "            )\n",
    "\n",
    "            processed_count += 1\n",
    "\n",
    "            # Clear memory after each image (only if GPU available)\n",
    "            if use_gpu:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_filename}: {e}\")\n",
    "            # Save error file to avoid reprocessing\n",
    "            with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"ERROR: {str(e)}\")\n",
    "            if use_gpu:\n",
    "                torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "    # Final cleanup\n",
    "    del model\n",
    "    del processor\n",
    "    if use_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"\\n=== LLAVA Processing Complete ===\")\n",
    "    print(f\"Device used: {'GPU' if use_gpu else 'CPU'}\")\n",
    "    print(f\"Processed: {processed_count} images\")\n",
    "    print(f\"Skipped: {skipped_count} images (already processed)\")\n",
    "    print(f\"Total: {len(image_files)} images\")\n",
    "\n",
    "\n",
    "# Install requirements function\n",
    "def install_requirements():\n",
    "    \"\"\"Install required packages if not already installed\"\"\"\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    required_packages = [\n",
    "        \"transformers>=4.36.0\",\n",
    "        \"torch\",\n",
    "        \"torchvision\",\n",
    "        \"accelerate\",\n",
    "        \"pillow\",\n",
    "        \"bitsandbytes\",  # For efficient loading\n",
    "    ]\n",
    "\n",
    "    print(\"Installing/updating required packages...\")\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {package}: {e}\")\n",
    "\n",
    "    # Install latest transformers from git for LLAVA support\n",
    "    try:\n",
    "        subprocess.check_call(\n",
    "            [\n",
    "                sys.executable,\n",
    "                \"-m\",\n",
    "                \"pip\",\n",
    "                \"install\",\n",
    "                \"git+https://github.com/huggingface/transformers.git\",\n",
    "            ]\n",
    "        )\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install transformers from git: {e}\")\n",
    "\n",
    "\n",
    "# Run LLAVA OCR processing\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment the line below if you need to install requirements\n",
    "    # install_requirements()\n",
    "\n",
    "    process_images_with_llava()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03004154",
   "metadata": {},
   "source": [
    "# Shikra Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bec15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SHIKRA OCR Processing ===\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "GPU Memory: 12.88 GB\n",
      "Output folder: extracted_text/shikra\n",
      "Loading SHIKRA model...\n",
      "Error loading SHIKRA model: shikras/shikra-7b-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "Please ensure you have the correct model name/path\n"
     ]
    }
   ],
   "source": [
    "# SHIKRA OCR Cell - GPU Accelerated Implementation\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# Force GPU usage\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Check GPU availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: CUDA not available. SHIKRA requires GPU acceleration.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"=== SHIKRA OCR Processing ===\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "def process_images_with_shikra(images_folder=\"part_2_images\", output_folder=\"extracted_text/shikra\"):\n",
    "    \"\"\"\n",
    "    Process images using SHIKRA model for OCR with GPU acceleration\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "    # Clear GPU memory before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Initialize SHIKRA model and processor\n",
    "    print(\"Loading SHIKRA model...\")\n",
    "    try:\n",
    "        # TODO: Replace with your specific SHIKRA model path/name\n",
    "        model_name = \"shikras/shikra-7b-v1\"  # Example model name\n",
    "        \n",
    "        processor = AutoProcessor.from_pretrained(model_name, use_fast=False)\n",
    "        model = AutoModelForVision2Seq.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,  # Use fp16 for RTX 2060 efficiency\n",
    "            device_map=\"cuda\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        model.eval()\n",
    "        print(\"SHIKRA model loaded successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SHIKRA model: {e}\")\n",
    "        print(\"Please ensure you have the correct model name/path\")\n",
    "        return\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Process each image\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "        # Check if text file already exists (efficiency check)\n",
    "        if os.path.exists(text_file_path):\n",
    "            print(f\"\\n[{i}/{len(image_files)}] Skipping {image_filename} - already processed\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[{i}/{len(image_files)}] Processing {image_filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Resize large images to optimize GPU memory\n",
    "            max_dim = 1024  # Adjust based on your GPU memory\n",
    "            if max(image.size) > max_dim:\n",
    "                image.thumbnail((max_dim, max_dim), Image.Resampling.LANCZOS)\n",
    "                print(f\"Resized image to {image.size} for GPU optimization\")\n",
    "            \n",
    "            # Prepare prompt for OCR task\n",
    "            # SHIKRA might use a different prompt format - adjust as needed\n",
    "            prompt = \"<image> Extract and transcribe all text visible in this image.\"\n",
    "            \n",
    "            # Process with SHIKRA\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # TODO: Adjust this section based on your specific SHIKRA implementation\n",
    "            inputs = processor(\n",
    "                text=prompt,\n",
    "                images=image,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            # Generate text with GPU\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=1024,\n",
    "                    temperature=0.1,  # Low temperature for more accurate OCR\n",
    "                    do_sample=False,\n",
    "                    num_beams=1,  # Adjust based on accuracy vs speed tradeoff\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            extracted_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Clean up the output (remove prompt if included)\n",
    "            if prompt in extracted_text:\n",
    "                extracted_text = extracted_text.replace(prompt, \"\").strip()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "            \n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                # Save empty file to avoid reprocessing\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(\"\")\n",
    "                continue\n",
    "            \n",
    "            # Save extracted text\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(extracted_text)\n",
    "            \n",
    "            print(f\"Saved text to: {text_file_path}\")\n",
    "            print(f\"Text preview: {extracted_text[:100]}...\" if len(extracted_text) > 100 else f\"Text: {extracted_text}\")\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "            # Clear GPU memory after each image\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_filename}: {e}\")\n",
    "            # Save error file to avoid reprocessing\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"ERROR: {str(e)}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "    \n",
    "    # Final cleanup\n",
    "    del model\n",
    "    del processor\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n=== SHIKRA Processing Complete ===\")\n",
    "    print(f\"Processed: {processed_count} images\")\n",
    "    print(f\"Skipped: {skipped_count} images (already processed)\")\n",
    "    print(f\"Total: {len(image_files)} images\")\n",
    "\n",
    "# Run SHIKRA OCR processing\n",
    "if __name__ == \"__main__\":\n",
    "    process_images_with_shikra()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc8c7b",
   "metadata": {},
   "source": [
    "# Hugging face version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x5vxmvv74qb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osama/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-11 02:05:52.383426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752181552.395305   18129 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752181552.398922   18129 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752181552.409883   18129 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752181552.409900   18129 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752181552.409902   18129 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752181552.409903   18129 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-11 02:05:52.413687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAVA model for batch processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "# Load the model in half-precision\n",
    "print(\"Loading LLAVA model for batch processing...\")\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"liuhaotian/llava-v1.6-vicuna-7b\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", use_fast=False)\n",
    "\n",
    "def process_images_batch(images_folder=\"part_2_images\", output_folder=\"extracted_text/llava_batch\", batch_size=2):\n",
    "    \"\"\"\n",
    "    Process images from folder using batch processing with LLAVA\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process in batches of {batch_size}\")\n",
    "    \n",
    "    # Process images in batches\n",
    "    for i in range(0, len(image_files), batch_size):\n",
    "        batch_files = image_files[i:i + batch_size]\n",
    "        print(f\"\\nProcessing batch {i//batch_size + 1}: {len(batch_files)} images\")\n",
    "        \n",
    "        # Prepare conversations for this batch\n",
    "        conversations = []\n",
    "        valid_files = []\n",
    "        \n",
    "        for image_path in batch_files:\n",
    "            try:\n",
    "                # Load and validate image\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                \n",
    "                # Create conversation for this image\n",
    "                conversation = [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"image\", \"image\": image},\n",
    "                            {\"type\": \"text\", \"text\": \"Extract all text from this image. Provide only the extracted text without any additional commentary.\"},\n",
    "                        ],\n",
    "                    },\n",
    "                ]\n",
    "                \n",
    "                conversations.append(conversation)\n",
    "                valid_files.append(image_path)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {image_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not conversations:\n",
    "            print(\"No valid images in this batch, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Process batch\n",
    "            print(f\"Processing {len(conversations)} images...\")\n",
    "            \n",
    "            inputs = processor.apply_chat_template(\n",
    "                conversations,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_dict=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device, torch.float16)\n",
    "            \n",
    "            # Generate text for all images in batch\n",
    "            with torch.no_grad():\n",
    "                generate_ids = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
    "            \n",
    "            # Decode results\n",
    "            results = processor.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Save results for each image\n",
    "            for j, (image_path, result) in enumerate(zip(valid_files, results)):\n",
    "                filename = os.path.basename(image_path)\n",
    "                base_name = os.path.splitext(filename)[0]\n",
    "                text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "                \n",
    "                # Clean up the result text\n",
    "                if \"assistant\" in result.lower():\n",
    "                    # Find the last occurrence of assistant and take text after it\n",
    "                    parts = result.lower().split(\"assistant\")\n",
    "                    if len(parts) > 1:\n",
    "                        extracted_text = result[result.lower().rfind(\"assistant\") + len(\"assistant\"):].strip()\n",
    "                    else:\n",
    "                        extracted_text = result.strip()\n",
    "                else:\n",
    "                    extracted_text = result.strip()\n",
    "                \n",
    "                # Remove any remaining artifacts\n",
    "                lines = extracted_text.split('\\n')\n",
    "                cleaned_lines = []\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    if line and not line.lower().startswith(('user:', 'assistant:', 'extract all text')):\n",
    "                        cleaned_lines.append(line)\n",
    "                \n",
    "                final_text = '\\n'.join(cleaned_lines).strip()\n",
    "                \n",
    "                # Save to file\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(final_text)\n",
    "                \n",
    "                print(f\"Saved: {filename} -> {text_file_path}\")\n",
    "                if final_text:\n",
    "                    preview = final_text[:100] + \"...\" if len(final_text) > 100 else final_text\n",
    "                    print(f\"Preview: {preview}\")\n",
    "                else:\n",
    "                    print(\"No text extracted\")\n",
    "            \n",
    "            # Clear GPU memory after each batch\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "            # Save error files for this batch\n",
    "            for image_path in valid_files:\n",
    "                filename = os.path.basename(image_path)\n",
    "                base_name = os.path.splitext(filename)[0]\n",
    "                text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"ERROR: {str(e)}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n=== Batch Processing Complete ===\")\n",
    "    print(f\"Results saved to: {output_folder}\")\n",
    "\n",
    "# Run the batch processing\n",
    "if __name__ == \"__main__\":\n",
    "    process_images_batch(batch_size=2)  # Adjust batch_size based on your GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0456593c",
   "metadata": {},
   "source": [
    "# LLava via Olama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edc7b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 images to process\n",
      "[1/3] Processing: IMG_20250629_214514_439.jpg\n",
      "‚úÖ Saved: extracted_text/ollama_direct/IMG_20250629_214514_439.txt\n",
      "üìù Text: The image is blurry and rotated, making it difficult to read the text with certainty. However, I wil...\n",
      "[2/3] Processing: IMG_20250629_214324_528.jpg\n",
      "‚úÖ Saved: extracted_text/ollama_direct/IMG_20250629_214324_528.txt\n",
      "üìù Text: The image shows a page with handwritten notes, and it appears to be a personal study or workbook. He...\n",
      "[3/3] Processing: textbook_img.jpg\n",
      "‚úÖ Saved: extracted_text/ollama_direct/textbook_img.txt\n",
      "üìù Text: The text in the image reads:\n",
      "\n",
      "\"Episode 25: When the Moon Splits\n",
      "\n",
      "Synopsis:\n",
      "\n",
      "The war for Ryloth rages...\n",
      "\n",
      "üéâ OCR Complete! Successfully processed 3/3 images\n",
      "üìÅ Results saved to: extracted_text/ollama_direct\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import ollama\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def direct_ollama_llava_ocr(\n",
    "    images_folder=\"part_2_images\", output_folder=\"extracted_text/ollama_direct\"\n",
    "):\n",
    "    \"\"\"Process images with Ollama LLaVA directly (no HTTP requests)\"\"\"\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "\n",
    "    successful = 0\n",
    "\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "\n",
    "        print(f\"[{i}/{len(image_files)}] Processing: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # Use Ollama directly with the image file\n",
    "            response = ollama.generate(\n",
    "                model=\"llava\",\n",
    "                prompt=\"Extract all text from this image. Provide only the extracted text without any commentary.\",\n",
    "                images=[image_path],\n",
    "            )\n",
    "\n",
    "            extracted_text = response[\"response\"].strip()\n",
    "\n",
    "            # Save extracted text\n",
    "            with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(extracted_text)\n",
    "\n",
    "            print(f\"‚úÖ Saved: {text_file_path}\")\n",
    "            if extracted_text:\n",
    "                preview = (\n",
    "                    extracted_text[:100] + \"...\"\n",
    "                    if len(extracted_text) > 100\n",
    "                    else extracted_text\n",
    "                )\n",
    "                print(f\"üìù Text: {preview}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No text extracted\")\n",
    "\n",
    "            successful += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "            with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"ERROR: {str(e)}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nüéâ OCR Complete! Successfully processed {successful}/{len(image_files)} images\"\n",
    "    )\n",
    "    print(f\"üìÅ Results saved to: {output_folder}\")\n",
    "\n",
    "\n",
    "# Run the OCR processing\n",
    "if __name__ == \"__main__\":\n",
    "    direct_ollama_llava_ocr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mon-reader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
