{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env-report-2025",
   "metadata": {},
   "source": [
    "## Environment Report\n",
    "\n",
    "**Current Package Versions (Baseline before LLAVA/SHIKRA integration):**\n",
    "\n",
    "- **Python**: 3.13.3\n",
    "- **torch**: 2.7.1+cu128\n",
    "- **torchvision**: 0.22.1+cu128\n",
    "- **torchaudio**: 2.7.1+cu128\n",
    "- **easyocr**: 1.7.2\n",
    "- **opencv-python**: 4.12.0.88\n",
    "- **pillow**: 11.2.1\n",
    "- **numpy**: 2.2.6\n",
    "- **pyttsx3**: 2.99\n",
    "- **langdetect**: 1.0.9\n",
    "- **transformers**: 4.52.3\n",
    "- **gradio**: 5.31.0\n",
    "- **CUDA Available**: Yes\n",
    "- **CUDA Version**: 12.8\n",
    "- **GPU**: NVIDIA GeForce RTX 2060\n",
    "- **GPU Memory**: 12.88 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05407b00",
   "metadata": {},
   "source": [
    "# Imports and Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932d8684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osama/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-11 01:52:08.266639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752180728.279169   14124 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752180728.282756   14124 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752180728.293164   14124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752180728.293180   14124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752180728.293181   14124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752180728.293182   14124 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-11 01:52:08.296669: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLAVA OCR Processing ===\n",
      "GPU: NVIDIA GeForce RTX 2050\n",
      "GPU Memory: 3.96 GB\n",
      "HuggingFace cache location: default\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import easyocr\n",
    "import pyttsx3\n",
    "from langdetect import detect\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "## Moving Hugging Face default Download Dir\n",
    "\n",
    "# # LLAVA OCR Cell - GPU Accelerated Implementation\n",
    "# import os\n",
    "\n",
    "# # IMPORTANT: Set cache BEFORE any imports from transformers/huggingface\n",
    "# os.environ['HF_HOME'] = 'D:/HuggingFaceCache'\n",
    "# os.environ['TRANSFORMERS_CACHE'] = 'D:/HuggingFaceCache/transformers'\n",
    "# os.environ['HUGGINGFACE_HUB_CACHE'] = 'D:/HuggingFaceCache/hub'\n",
    "\n",
    "# # Now import everything else\n",
    "# import glob\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# import time\n",
    "# import gc\n",
    "# from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "# Force GPU usage\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Check GPU availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: CUDA not available. LLAVA requires GPU acceleration.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"=== LLAVA OCR Processing ===\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"HuggingFace cache location: {os.environ.get('HF_HOME', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8207ed",
   "metadata": {},
   "source": [
    "# Simple TTS using pyttsx3 Gpu based:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force GPU usage and optimization for RTX 2060\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU\n",
    "torch.backends.cudnn.benchmark = True  # Enable cudnn autotuner for performance\n",
    "\n",
    "# Check GPU and setup\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: CUDA not available. This script requires GPU acceleration.\")\n",
    "    print(\"Please check your NVIDIA drivers and PyTorch installation.\")\n",
    "    exit(1)\n",
    "\n",
    "# Display GPU information\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Initialize EasyOCR reader with explicit GPU settings\n",
    "print(\"Initializing EasyOCR with GPU acceleration...\")\n",
    "reader = easyocr.Reader(\n",
    "    [\"ar\", \"ur\", \"en\"], \n",
    "    gpu=True,\n",
    "    verbose=False,\n",
    "    # For RTX 2060, set reasonable batch size and model parameters\n",
    "    detector=True,\n",
    "    recognizer=True\n",
    ")\n",
    "\n",
    "# Initialize TTS engine\n",
    "tts_engine = pyttsx3.init()\n",
    "\n",
    "def log_gpu_memory():\n",
    "    \"\"\"Log current GPU memory usage\"\"\"\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "\n",
    "def process_images_ocr_save_text(images_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process all images in the folder with GPU-accelerated OCR and save to text files\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output folder: {output_folder}\")\n",
    "    \n",
    "    # Get all image files in the folder\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Report initial GPU memory\n",
    "    log_gpu_memory()\n",
    "\n",
    "    # Process files\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "        print(f\"\\n--- Processing Image {i}/{len(image_files)}: {image_filename} ---\")\n",
    "\n",
    "        try:\n",
    "            # Read image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Could not read image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Optimize image for GPU processing (resize large images)\n",
    "            h, w = image.shape[:2]\n",
    "            max_dim = 2000  # Optimal for RTX 2060 memory\n",
    "            if max(h, w) > max_dim:\n",
    "                scale = max_dim / max(h, w)\n",
    "                image = cv2.resize(image, (int(w * scale), int(h * scale)))\n",
    "                print(f\"Resized image to {image.shape[1]}x{image.shape[0]} to optimize GPU memory\")\n",
    "            \n",
    "            # Report GPU memory before OCR\n",
    "            log_gpu_memory()\n",
    "\n",
    "            # Extract text using GPU-accelerated EasyOCR\n",
    "            print(\"Extracting text with GPU acceleration...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # For RTX 2060, use appropriate batch size\n",
    "            results = reader.readtext(\n",
    "                image,\n",
    "                batch_size=2,  # Adjust based on your GPU memory\n",
    "                paragraph=True,  # Group text into paragraphs\n",
    "                detail=0  # 0 for more accuracy\n",
    "            )\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "            # Combine all detected text with confidence filtering\n",
    "            extracted_text = \" \".join(\n",
    "                [result[1] for result in results if result[2] > 0.5]\n",
    "            )  # confidence > 0.5\n",
    "\n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Extracted text: {extracted_text}\")\n",
    "            \n",
    "            # Try to detect language\n",
    "            try:\n",
    "                detected_lang = detect(extracted_text)\n",
    "                print(f\"Detected language: {detected_lang}\")\n",
    "                \n",
    "                # Save text with language information to file\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                    text_file.write(f\"LANG:{detected_lang}\\n\")\n",
    "                    text_file.write(extracted_text)\n",
    "                \n",
    "                print(f\"Saved text to: {text_file_path}\")\n",
    "                \n",
    "            except Exception as lang_error:\n",
    "                print(f\"Language detection failed: {lang_error}\")\n",
    "                # Save text without language information\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                    text_file.write(f\"LANG:unknown\\n\")\n",
    "                    text_file.write(extracted_text)\n",
    "                \n",
    "                print(f\"Saved text to: {text_file_path} (language unknown)\")\n",
    "\n",
    "            # Clear GPU memory after each image\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            # Clear GPU memory on error\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "    # Final GPU memory cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"\\n--- OCR Processing Complete ---\")\n",
    "    log_gpu_memory()\n",
    "    \n",
    "    return output_folder\n",
    "\n",
    "def read_text_files_aloud(text_folder):\n",
    "    \"\"\"\n",
    "    Read all text files in the folder aloud using TTS, one by one with clear separation\n",
    "    \"\"\"\n",
    "    # Get all text files\n",
    "    text_files = glob.glob(os.path.join(text_folder, \"*.txt\"))\n",
    "    \n",
    "    if not text_files:\n",
    "        print(f\"No text files found in {text_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(text_files)} text files to read\")\n",
    "    \n",
    "    # Language mapping for TTS\n",
    "    lang_mapping = {\n",
    "        \"en\": \"english\",\n",
    "        \"ar\": \"arabic\",\n",
    "        \"ur\": \"urdu\",\n",
    "        \"hi\": \"hindi\",\n",
    "        \"fa\": \"persian\",\n",
    "        \"ps\": \"pashto\"\n",
    "    }\n",
    "    \n",
    "    # Sort text files alphabetically to ensure consistent reading order\n",
    "    text_files.sort()\n",
    "    \n",
    "    for i, text_file_path in enumerate(text_files, 1):\n",
    "        file_name = os.path.basename(text_file_path)\n",
    "        print(f\"\\n===== Reading File {i}/{len(text_files)}: {file_name} =====\")\n",
    "        \n",
    "        # Announce the file being read (optional)\n",
    "        tts_engine.setProperty(\"rate\", 150)\n",
    "        announcement = f\"Reading file {i} of {len(text_files)}: {os.path.splitext(file_name)[0]}\"\n",
    "        print(announcement)\n",
    "        tts_engine.say(announcement)\n",
    "        tts_engine.runAndWait()\n",
    "        \n",
    "        # Pause between announcement and content\n",
    "        time.sleep(1)\n",
    "        \n",
    "        try:\n",
    "            # Read text file\n",
    "            with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                \n",
    "            if not lines:\n",
    "                print(f\"File is empty: {text_file_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Extract language information from first line\n",
    "            lang_line = lines[0].strip()\n",
    "            if lang_line.startswith(\"LANG:\"):\n",
    "                detected_lang = lang_line[5:]\n",
    "                print(f\"Language: {detected_lang}\")\n",
    "                # Remove the language line\n",
    "                content = \"\".join(lines[1:])\n",
    "            else:\n",
    "                # No language information, treat all lines as content\n",
    "                detected_lang = \"unknown\"\n",
    "                content = \"\".join(lines)\n",
    "            \n",
    "            if not content.strip():\n",
    "                print(\"No content to read\")\n",
    "                continue\n",
    "                \n",
    "            # Print a preview of the content\n",
    "            content_preview = content[:100] + \"...\" if len(content) > 100 else content\n",
    "            print(f\"Text content: {content_preview}\")\n",
    "            \n",
    "            # Get available voices\n",
    "            voices = tts_engine.getProperty(\"voices\")\n",
    "            \n",
    "            # Try to set appropriate voice based on language\n",
    "            voice_set = False\n",
    "            for voice in voices:\n",
    "                tts_lang = lang_mapping.get(detected_lang, \"english\")\n",
    "                if (tts_lang.lower() in voice.name.lower() or \n",
    "                    detected_lang in voice.id.lower()):\n",
    "                    tts_engine.setProperty(\"voice\", voice.id)\n",
    "                    voice_set = True\n",
    "                    print(f\"Using voice: {voice.name}\")\n",
    "                    break\n",
    "            \n",
    "            if not voice_set:\n",
    "                print(f\"No specific voice found for {detected_lang}, using default\")\n",
    "            \n",
    "            # Adjust speech rate based on language\n",
    "            if detected_lang in [\"ar\", \"ur\"]:\n",
    "                # Slower for Arabic and Urdu\n",
    "                tts_engine.setProperty(\"rate\", 130)\n",
    "            else:\n",
    "                tts_engine.setProperty(\"rate\", 150)\n",
    "            \n",
    "            # Read the text aloud\n",
    "            print(f\"Reading text aloud...\")\n",
    "            tts_engine.say(content)\n",
    "            tts_engine.runAndWait()\n",
    "            \n",
    "            # Pause between files to clearly separate them\n",
    "            print(\"Finished reading file.\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {text_file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n===== All Text Files Have Been Read =====\")\n",
    "\n",
    "def get_user_confirmation():\n",
    "    \"\"\"\n",
    "    Ask the user if they want to proceed to the TTS reading phase\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        response = input(\"\\nOCR processing complete. Proceed with reading text files? (y/n): \").lower()\n",
    "        if response in ['y', 'yes']:\n",
    "            return True\n",
    "        elif response in ['n', 'no']:\n",
    "            return False\n",
    "        else:\n",
    "            print(\"Please enter 'y' or 'n'\")\n",
    "\n",
    "# Main process\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Define folders\n",
    "        images_folder = \"part_2_images\"\n",
    "        output_folder = \"extracted_text\"\n",
    "        \n",
    "        # Print GPU optimization message\n",
    "        print(\"=== Running GPU-Optimized OCR for RTX 2060 ===\")\n",
    "        \n",
    "        # Step 1: Process images with OCR and save text\n",
    "        start_time = time.time()\n",
    "        text_folder = process_images_ocr_save_text(images_folder, output_folder)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"OCR processing completed in {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Optional: Ask for user confirmation before proceeding to TTS\n",
    "        if get_user_confirmation():\n",
    "            # Step 2: Read the saved text files aloud one by one\n",
    "            read_text_files_aloud(text_folder)\n",
    "        else:\n",
    "            print(\"TTS reading canceled. Text files are saved in the output folder.\")\n",
    "        \n",
    "        print(\"Process completed successfully!\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user\")\n",
    "        # Clean up GPU memory on interrupt\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Clean up GPU memory on error\n",
    "        torch.cuda.empty_cache()\n",
    "    finally:\n",
    "        # Final cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a smaller vision model first\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\", use_fast=False)\n",
    "print(\"Downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15f199",
   "metadata": {},
   "source": [
    "# LLava Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acecd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osama/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-11 01:58:30.995160: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752181111.007441   14370 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752181111.011080   14370 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752181111.021525   14370 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752181111.021542   14370 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752181111.021544   14370 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752181111.021545   14370 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-11 01:58:31.024835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLAVA OCR Processing ===\n",
      "GPU: NVIDIA GeForce RTX 2050\n",
      "GPU Memory: 3.96 GB\n",
      "Using GPU acceleration\n",
      "Output folder: extracted_text/llava\n",
      "Loading LLAVA model...\n",
      "Downloading/Loading processor...\n",
      "Error loading LLAVA model: \n",
      "LlamaTokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "Trying alternative approach...\n",
      "Trying llava-hf/llava-1.5-13b-hf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# # LLAVA OCR Cell - GPU Accelerated Implementation\n",
    "\n",
    "# # Force GPU usage\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# os.environ['HF_HOME'] = 'D:/HuggingFaceCache' \n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# # Check GPU availability\n",
    "# if not torch.cuda.is_available():\n",
    "#     print(\"ERROR: CUDA not available. LLAVA requires GPU acceleration.\")\n",
    "#     exit(1)\n",
    "\n",
    "# print(\"=== LLAVA OCR Processing ===\")\n",
    "# print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "# print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# def process_images_with_llava(images_folder=\"part_2_images\", output_folder=\"extracted_text/llava\"):\n",
    "#     \"\"\"\n",
    "#     Process images using LLAVA model for OCR with GPU acceleration\n",
    "#     \"\"\"\n",
    "#     # Create output directory if it doesn't exist\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "#     # # Clear GPU memory before starting\n",
    "#     # torch.cuda.empty_cache()\n",
    "#     # gc.collect()\n",
    "    \n",
    "#     # Initialize LLAVA model and processor\n",
    "#     print(\"Loading LLAVA model...\")\n",
    "#     try:\n",
    "#         # TODO: Replace with your specific LLAVA model path/name\n",
    "#         model_name = \"llava-hf/llava-1.5-7b-hf\"  # Example model name\n",
    "        \n",
    "#         processor = AutoProcessor.from_pretrained(model_name, use_fast=True)\n",
    "#         model = LlavaForConditionalGeneration.from_pretrained(\n",
    "#             model_name,\n",
    "#             torch_dtype=torch.float16,  # Use fp16 for RTX 2060 efficiency\n",
    "#             device_map=\"cuda\",\n",
    "#             low_cpu_mem_usage=True\n",
    "#         )\n",
    "#         model.eval()\n",
    "#         print(\"LLAVA model loaded successfully!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading LLAVA model: {e}\")\n",
    "#         print(\"Please ensure you have the correct model name/path\")\n",
    "#         return\n",
    "    \n",
    "#     # Get all image files\n",
    "#     image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "#     image_files = []\n",
    "    \n",
    "#     for ext in image_extensions:\n",
    "#         image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "#         image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "#     if not image_files:\n",
    "#         print(f\"No images found in {images_folder}\")\n",
    "#         return\n",
    "    \n",
    "#     print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "#     # Process each image\n",
    "#     processed_count = 0\n",
    "#     skipped_count = 0\n",
    "    \n",
    "#     for i, image_path in enumerate(image_files, 1):\n",
    "#         image_filename = os.path.basename(image_path)\n",
    "#         base_name = os.path.splitext(image_filename)[0]\n",
    "#         text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "#         # Check if text file already exists (efficiency check)\n",
    "#         if os.path.exists(text_file_path):\n",
    "#             print(f\"\\n[{i}/{len(image_files)}] Skipping {image_filename} - already processed\")\n",
    "#             skipped_count += 1\n",
    "#             continue\n",
    "        \n",
    "#         print(f\"\\n[{i}/{len(image_files)}] Processing {image_filename}...\")\n",
    "        \n",
    "#         try:\n",
    "#             # Load and preprocess image\n",
    "#             image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "#             # Resize large images to optimize GPU memory\n",
    "#             max_dim = 1024  # Adjust based on your GPU memory\n",
    "#             if max(image.size) > max_dim:\n",
    "#                 image.thumbnail((max_dim, max_dim), Image.Resampling.LANCZOS)\n",
    "#                 print(f\"Resized image to {image.size} for GPU optimization\")\n",
    "            \n",
    "#             # Prepare prompt for OCR task\n",
    "#             prompt = \"USER: <image>\\nExtract all text from this image. Provide only the extracted text without any additional commentary.\\nASSISTANT:\"\n",
    "            \n",
    "#             # Process with LLAVA\n",
    "#             start_time = time.time()\n",
    "            \n",
    "#             # TODO: Adjust this section based on your specific LLAVA implementation\n",
    "#             inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda\")\n",
    "            \n",
    "#             # Generate text with GPU\n",
    "#             with torch.no_grad():\n",
    "#                 generated_ids = model.generate(\n",
    "#                     **inputs,\n",
    "#                     max_new_tokens=1024,\n",
    "#                     temperature=0.1,  # Low temperature for more accurate OCR\n",
    "#                     do_sample=False,\n",
    "#                     use_cache=True\n",
    "#                 )\n",
    "            \n",
    "#             # Decode the generated text\n",
    "#             extracted_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "#             # Remove the prompt from the output\n",
    "#             if \"ASSISTANT:\" in extracted_text:\n",
    "#                 extracted_text = extracted_text.split(\"ASSISTANT:\")[-1].strip()\n",
    "            \n",
    "#             end_time = time.time()\n",
    "#             print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "            \n",
    "#             if not extracted_text.strip():\n",
    "#                 print(\"No text detected in this image\")\n",
    "#                 # Save empty file to avoid reprocessing\n",
    "#                 with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                     f.write(\"\")\n",
    "#                 continue\n",
    "            \n",
    "#             # Save extracted text\n",
    "#             with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(extracted_text)\n",
    "            \n",
    "#             print(f\"Saved text to: {text_file_path}\")\n",
    "#             print(f\"Text preview: {extracted_text[:100]}...\" if len(extracted_text) > 100 else f\"Text: {extracted_text}\")\n",
    "            \n",
    "#             processed_count += 1\n",
    "            \n",
    "#             # Clear GPU memory after each image\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {image_filename}: {e}\")\n",
    "#             # Save error file to avoid reprocessing\n",
    "#             with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(f\"ERROR: {str(e)}\")\n",
    "#             torch.cuda.empty_cache()\n",
    "#             continue\n",
    "    \n",
    "#     # Final cleanup\n",
    "#     del model\n",
    "#     del processor\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "    \n",
    "#     print(f\"\\n=== LLAVA Processing Complete ===\")\n",
    "#     print(f\"Processed: {processed_count} images\")\n",
    "#     print(f\"Skipped: {skipped_count} images (already processed)\")\n",
    "#     print(f\"Total: {len(image_files)} images\")\n",
    "\n",
    "# # Run LLAVA OCR processing\n",
    "# if __name__ == \"__main__\":\n",
    "#     process_images_with_llava()\n",
    "\n",
    "\n",
    "\n",
    "# LLAVA OCR Cell - GPU Accelerated Implementation\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Force GPU usage\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# os.environ['HF_HOME'] = 'D:/HuggingFaceCache' \n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(\"=== LLAVA OCR Processing ===\")\n",
    "if use_gpu:\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(\"Using GPU acceleration\")\n",
    "else:\n",
    "    print(\"GPU not available - using CPU (will be slower)\")\n",
    "    print(\"For better performance, install CUDA-compatible PyTorch\")\n",
    "\n",
    "def process_images_with_llava(images_folder=\"part_2_images\", output_folder=\"extracted_text/llava\"):\n",
    "    \"\"\"\n",
    "    Process images using LLAVA model for OCR with GPU acceleration\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "    # Clear GPU memory before starting (only if GPU available)\n",
    "    if use_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Initialize LLAVA model and processor\n",
    "    print(\"Loading LLAVA model...\")\n",
    "    try:\n",
    "        # Updated model name - use the correct LLAVA model\n",
    "        model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "        \n",
    "        print(\"Downloading/Loading processor...\")\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_name, \n",
    "            use_fast=False,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(\"Downloading/Loading model...\")\n",
    "        model = LlavaForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if use_gpu else torch.float32,  # Use fp16 only if GPU available\n",
    "            device_map=\"auto\" if use_gpu else None,  # Use auto device mapping only if GPU available\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Move model to appropriate device\n",
    "        if use_gpu and not next(model.parameters()).is_cuda:\n",
    "            model = model.to(device)\n",
    "        elif not use_gpu:\n",
    "            model = model.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        print(\"LLAVA model loaded successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LLAVA model: {e}\")\n",
    "        print(\"Trying alternative approach...\")\n",
    "        \n",
    "        # Alternative: Try different model or installation\n",
    "        try:\n",
    "            # Alternative model names to try\n",
    "            alternative_models = [\n",
    "                \"llava-hf/llava-1.5-13b-hf\",\n",
    "                \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "                \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "            ]\n",
    "            \n",
    "            for alt_model in alternative_models:\n",
    "                try:\n",
    "                    print(f\"Trying {alt_model}...\")\n",
    "                    processor = AutoProcessor.from_pretrained(alt_model, trust_remote_code=True)\n",
    "                    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "                        alt_model,\n",
    "                        torch_dtype=torch.float16 if use_gpu else torch.float32,\n",
    "                        device_map=\"auto\" if use_gpu else None,\n",
    "                        low_cpu_mem_usage=True,\n",
    "                        trust_remote_code=True\n",
    "                    )\n",
    "                    if use_gpu and not next(model.parameters()).is_cuda:\n",
    "                        model = model.to(device)\n",
    "                    elif not use_gpu:\n",
    "                        model = model.to(device)\n",
    "                    model.eval()\n",
    "                    print(f\"Successfully loaded {alt_model}!\")\n",
    "                    model_name = alt_model\n",
    "                    break\n",
    "                except Exception as alt_e:\n",
    "                    print(f\"Failed to load {alt_model}: {alt_e}\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(\"All model loading attempts failed.\")\n",
    "                print(\"Please ensure you have installed the requirements:\")\n",
    "                print(\"pip install transformers torch torchvision accelerate\")\n",
    "                print(\"pip install git+https://github.com/huggingface/transformers.git\")\n",
    "                return\n",
    "                \n",
    "        except Exception as final_e:\n",
    "            print(f\"Final error: {final_e}\")\n",
    "            return\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Process each image\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "        # Check if text file already exists (efficiency check)\n",
    "        if os.path.exists(text_file_path):\n",
    "            print(f\"\\n[{i}/{len(image_files)}] Skipping {image_filename} - already processed\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[{i}/{len(image_files)}] Processing {image_filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Resize large images to optimize memory\n",
    "            max_dim = 1024 if use_gpu else 512  # Smaller images for CPU processing\n",
    "            if max(image.size) > max_dim:\n",
    "                image.thumbnail((max_dim, max_dim), Image.Resampling.LANCZOS)\n",
    "                print(f\"Resized image to {image.size} for {'GPU' if use_gpu else 'CPU'} optimization\")\n",
    "            \n",
    "            # Prepare prompt for OCR task - Updated format\n",
    "            prompt = \"USER: <image>\\nExtract all text from this image. Provide only the extracted text without any additional commentary.\\nASSISTANT:\"\n",
    "            \n",
    "            # Process with LLAVA\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Updated processing approach\n",
    "            inputs = processor(prompt, image, return_tensors=\"pt\")\n",
    "            \n",
    "            # Move inputs to appropriate device\n",
    "            if use_gpu:\n",
    "                inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "            else:\n",
    "                inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate text with appropriate device\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=1024 if use_gpu else 512,  # Reduce tokens for CPU\n",
    "                    temperature=0.1,  # Low temperature for more accurate OCR\n",
    "                    do_sample=False,\n",
    "                    use_cache=True,\n",
    "                    pad_token_id=processor.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            generated_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Clean up the output - remove prompt\n",
    "            if \"ASSISTANT:\" in generated_text:\n",
    "                extracted_text = generated_text.split(\"ASSISTANT:\")[-1].strip()\n",
    "            else:\n",
    "                extracted_text = generated_text.strip()\n",
    "            \n",
    "            # Remove any remaining prompt artifacts\n",
    "            if \"USER:\" in extracted_text:\n",
    "                extracted_text = extracted_text.split(\"USER:\")[-1].strip()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "            \n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                # Save empty file to avoid reprocessing\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(\"\")\n",
    "                continue\n",
    "            \n",
    "            # Save extracted text\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(extracted_text)\n",
    "            \n",
    "            print(f\"Saved text to: {text_file_path}\")\n",
    "            print(f\"Text preview: {extracted_text[:100]}...\" if len(extracted_text) > 100 else f\"Text: {extracted_text}\")\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "            # Clear memory after each image (only if GPU available)\n",
    "            if use_gpu:\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_filename}: {e}\")\n",
    "            # Save error file to avoid reprocessing\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"ERROR: {str(e)}\")\n",
    "            if use_gpu:\n",
    "                torch.cuda.empty_cache()\n",
    "            continue\n",
    "    \n",
    "    # Final cleanup\n",
    "    del model\n",
    "    del processor\n",
    "    if use_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n=== LLAVA Processing Complete ===\")\n",
    "    print(f\"Device used: {'GPU' if use_gpu else 'CPU'}\")\n",
    "    print(f\"Processed: {processed_count} images\")\n",
    "    print(f\"Skipped: {skipped_count} images (already processed)\")\n",
    "    print(f\"Total: {len(image_files)} images\")\n",
    "\n",
    "# Install requirements function\n",
    "def install_requirements():\n",
    "    \"\"\"Install required packages if not already installed\"\"\"\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    required_packages = [\n",
    "        \"transformers>=4.36.0\",\n",
    "        \"torch\",\n",
    "        \"torchvision\", \n",
    "        \"accelerate\",\n",
    "        \"pillow\",\n",
    "        \"bitsandbytes\"  # For efficient loading\n",
    "    ]\n",
    "    \n",
    "    print(\"Installing/updating required packages...\")\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {package}: {e}\")\n",
    "    \n",
    "    # Install latest transformers from git for LLAVA support\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/huggingface/transformers.git\"])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install transformers from git: {e}\")\n",
    "\n",
    "# Run LLAVA OCR processing\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment the line below if you need to install requirements\n",
    "    # install_requirements()\n",
    "    \n",
    "    process_images_with_llava()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b0270",
   "metadata": {},
   "source": [
    "# Simplified version LLava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa196b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "def process_images_with_llava(images_folder=\"part_2_images\", output_folder=\"extracted_text/llava\"):\n",
    "    \"\"\"Simple LLAVA OCR processing\"\"\"\n",
    "    \n",
    "    # Setup\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading LLAVA model...\")\n",
    "    model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "    processor = AutoProcessor.from_pretrained(model_name, use_fast=False)\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get images\n",
    "    extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\"]\n",
    "    image_files = []\n",
    "    for ext in extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images\")\n",
    "    \n",
    "    # Process each image\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        filename = os.path.basename(image_path)\n",
    "        text_file = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "        \n",
    "        if os.path.exists(text_file):\n",
    "            print(f\"[{i}/{len(image_files)}] Skipping {filename} - already processed\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"[{i}/{len(image_files)}] Processing {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Resize if too large\n",
    "            if max(image.size) > 1024:\n",
    "                image.thumbnail((1024, 1024), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # OCR prompt\n",
    "            prompt = \"USER: <image>\\nExtract all text from this image.\\nASSISTANT:\"\n",
    "            \n",
    "            # Process\n",
    "            inputs = processor(prompt, image, return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
    "            \n",
    "            # Extract text\n",
    "            text = processor.decode(output[0], skip_special_tokens=True)\n",
    "            if \"ASSISTANT:\" in text:\n",
    "                text = text.split(\"ASSISTANT:\")[-1].strip()\n",
    "            \n",
    "            # Save\n",
    "            with open(text_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "            \n",
    "            print(f\"Extracted: {text[:100]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            with open(text_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"ERROR: {e}\")\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_images_with_llava()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03004154",
   "metadata": {},
   "source": [
    "# Shikra Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bec15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SHIKRA OCR Processing ===\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "GPU Memory: 12.88 GB\n",
      "Output folder: extracted_text/shikra\n",
      "Loading SHIKRA model...\n",
      "Error loading SHIKRA model: shikras/shikra-7b-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "Please ensure you have the correct model name/path\n"
     ]
    }
   ],
   "source": [
    "# SHIKRA OCR Cell - GPU Accelerated Implementation\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# Force GPU usage\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Check GPU availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: CUDA not available. SHIKRA requires GPU acceleration.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"=== SHIKRA OCR Processing ===\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "def process_images_with_shikra(images_folder=\"part_2_images\", output_folder=\"extracted_text/shikra\"):\n",
    "    \"\"\"\n",
    "    Process images using SHIKRA model for OCR with GPU acceleration\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "    # Clear GPU memory before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Initialize SHIKRA model and processor\n",
    "    print(\"Loading SHIKRA model...\")\n",
    "    try:\n",
    "        # TODO: Replace with your specific SHIKRA model path/name\n",
    "        model_name = \"shikras/shikra-7b-v1\"  # Example model name\n",
    "        \n",
    "        processor = AutoProcessor.from_pretrained(model_name, use_fast=False)\n",
    "        model = AutoModelForVision2Seq.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,  # Use fp16 for RTX 2060 efficiency\n",
    "            device_map=\"cuda\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        model.eval()\n",
    "        print(\"SHIKRA model loaded successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SHIKRA model: {e}\")\n",
    "        print(\"Please ensure you have the correct model name/path\")\n",
    "        return\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Process each image\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "        # Check if text file already exists (efficiency check)\n",
    "        if os.path.exists(text_file_path):\n",
    "            print(f\"\\n[{i}/{len(image_files)}] Skipping {image_filename} - already processed\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[{i}/{len(image_files)}] Processing {image_filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Resize large images to optimize GPU memory\n",
    "            max_dim = 1024  # Adjust based on your GPU memory\n",
    "            if max(image.size) > max_dim:\n",
    "                image.thumbnail((max_dim, max_dim), Image.Resampling.LANCZOS)\n",
    "                print(f\"Resized image to {image.size} for GPU optimization\")\n",
    "            \n",
    "            # Prepare prompt for OCR task\n",
    "            # SHIKRA might use a different prompt format - adjust as needed\n",
    "            prompt = \"<image> Extract and transcribe all text visible in this image.\"\n",
    "            \n",
    "            # Process with SHIKRA\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # TODO: Adjust this section based on your specific SHIKRA implementation\n",
    "            inputs = processor(\n",
    "                text=prompt,\n",
    "                images=image,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            # Generate text with GPU\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=1024,\n",
    "                    temperature=0.1,  # Low temperature for more accurate OCR\n",
    "                    do_sample=False,\n",
    "                    num_beams=1,  # Adjust based on accuracy vs speed tradeoff\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            extracted_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Clean up the output (remove prompt if included)\n",
    "            if prompt in extracted_text:\n",
    "                extracted_text = extracted_text.replace(prompt, \"\").strip()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "            \n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                # Save empty file to avoid reprocessing\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(\"\")\n",
    "                continue\n",
    "            \n",
    "            # Save extracted text\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(extracted_text)\n",
    "            \n",
    "            print(f\"Saved text to: {text_file_path}\")\n",
    "            print(f\"Text preview: {extracted_text[:100]}...\" if len(extracted_text) > 100 else f\"Text: {extracted_text}\")\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "            # Clear GPU memory after each image\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_filename}: {e}\")\n",
    "            # Save error file to avoid reprocessing\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"ERROR: {str(e)}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "    \n",
    "    # Final cleanup\n",
    "    del model\n",
    "    del processor\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n=== SHIKRA Processing Complete ===\")\n",
    "    print(f\"Processed: {processed_count} images\")\n",
    "    print(f\"Skipped: {skipped_count} images (already processed)\")\n",
    "    print(f\"Total: {len(image_files)} images\")\n",
    "\n",
    "# Run SHIKRA OCR processing\n",
    "if __name__ == \"__main__\":\n",
    "    process_images_with_shikra()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc8c7b",
   "metadata": {},
   "source": [
    "# Hugging face version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x5vxmvv74qb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osama/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-11 02:05:52.383426: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752181552.395305   18129 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752181552.398922   18129 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752181552.409883   18129 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752181552.409900   18129 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752181552.409902   18129 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752181552.409903   18129 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-11 02:05:52.413687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAVA model for batch processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "# Load the model in half-precision\n",
    "print(\"Loading LLAVA model for batch processing...\")\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"liuhaotian/llava-v1.6-vicuna-7b\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", use_fast=False)\n",
    "\n",
    "def process_images_batch(images_folder=\"part_2_images\", output_folder=\"extracted_text/llava_batch\", batch_size=2):\n",
    "    \"\"\"\n",
    "    Process images from folder using batch processing with LLAVA\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process in batches of {batch_size}\")\n",
    "    \n",
    "    # Process images in batches\n",
    "    for i in range(0, len(image_files), batch_size):\n",
    "        batch_files = image_files[i:i + batch_size]\n",
    "        print(f\"\\nProcessing batch {i//batch_size + 1}: {len(batch_files)} images\")\n",
    "        \n",
    "        # Prepare conversations for this batch\n",
    "        conversations = []\n",
    "        valid_files = []\n",
    "        \n",
    "        for image_path in batch_files:\n",
    "            try:\n",
    "                # Load and validate image\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                \n",
    "                # Create conversation for this image\n",
    "                conversation = [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"image\", \"image\": image},\n",
    "                            {\"type\": \"text\", \"text\": \"Extract all text from this image. Provide only the extracted text without any additional commentary.\"},\n",
    "                        ],\n",
    "                    },\n",
    "                ]\n",
    "                \n",
    "                conversations.append(conversation)\n",
    "                valid_files.append(image_path)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {image_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not conversations:\n",
    "            print(\"No valid images in this batch, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Process batch\n",
    "            print(f\"Processing {len(conversations)} images...\")\n",
    "            \n",
    "            inputs = processor.apply_chat_template(\n",
    "                conversations,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_dict=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device, torch.float16)\n",
    "            \n",
    "            # Generate text for all images in batch\n",
    "            with torch.no_grad():\n",
    "                generate_ids = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
    "            \n",
    "            # Decode results\n",
    "            results = processor.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Save results for each image\n",
    "            for j, (image_path, result) in enumerate(zip(valid_files, results)):\n",
    "                filename = os.path.basename(image_path)\n",
    "                base_name = os.path.splitext(filename)[0]\n",
    "                text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "                \n",
    "                # Clean up the result text\n",
    "                if \"assistant\" in result.lower():\n",
    "                    # Find the last occurrence of assistant and take text after it\n",
    "                    parts = result.lower().split(\"assistant\")\n",
    "                    if len(parts) > 1:\n",
    "                        extracted_text = result[result.lower().rfind(\"assistant\") + len(\"assistant\"):].strip()\n",
    "                    else:\n",
    "                        extracted_text = result.strip()\n",
    "                else:\n",
    "                    extracted_text = result.strip()\n",
    "                \n",
    "                # Remove any remaining artifacts\n",
    "                lines = extracted_text.split('\\n')\n",
    "                cleaned_lines = []\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    if line and not line.lower().startswith(('user:', 'assistant:', 'extract all text')):\n",
    "                        cleaned_lines.append(line)\n",
    "                \n",
    "                final_text = '\\n'.join(cleaned_lines).strip()\n",
    "                \n",
    "                # Save to file\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(final_text)\n",
    "                \n",
    "                print(f\"Saved: {filename} -> {text_file_path}\")\n",
    "                if final_text:\n",
    "                    preview = final_text[:100] + \"...\" if len(final_text) > 100 else final_text\n",
    "                    print(f\"Preview: {preview}\")\n",
    "                else:\n",
    "                    print(\"No text extracted\")\n",
    "            \n",
    "            # Clear GPU memory after each batch\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "            # Save error files for this batch\n",
    "            for image_path in valid_files:\n",
    "                filename = os.path.basename(image_path)\n",
    "                base_name = os.path.splitext(filename)[0]\n",
    "                text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"ERROR: {str(e)}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n=== Batch Processing Complete ===\")\n",
    "    print(f\"Results saved to: {output_folder}\")\n",
    "\n",
    "# Run the batch processing\n",
    "if __name__ == \"__main__\":\n",
    "    process_images_batch(batch_size=2)  # Adjust batch_size based on your GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3yosnwatwwl",
   "metadata": {},
   "source": [
    "# LLaVA-Torch Alternative Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wca1se8mpo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing llava-torch...\n",
      "Collecting llava-torch\n",
      "  Downloading llava_torch-1.2.2.post1-py3-none-any.whl (102 kB)\n",
      "      102.2/102.2 KB 584.4 kB/s eta 0:00:00\n",
      "Collecting torchvision==0.15.2\n",
      "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "      6.0/6.0 MB 5.1 MB/s eta 0:00:00\n",
      "Collecting gradio==4.16.0\n",
      "  Downloading gradio-4.16.0-py3-none-any.whl (16.7 MB)\n",
      "      16.7/16.7 MB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.10/site-packages (from llava-torch) (2.1.3)\n",
      "Collecting httpx==0.24.0\n",
      "  Downloading httpx-0.24.0-py3-none-any.whl (75 kB)\n",
      "      75.3/75.3 KB 3.4 MB/s eta 0:00:00\n",
      "Collecting torch==2.0.1\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "      619.9/619.9 MB 2.0 MB/s eta 0:00:00\n",
      "Collecting transformers==4.36.2\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "      8.2/8.2 MB 3.2 MB/s eta 0:00:00\n",
      "Collecting tokenizers==0.15.0\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "      3.8/3.8 MB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pydantic in ./env/lib/python3.10/site-packages (from llava-torch) (2.11.7)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.10/site-packages (from llava-torch) (2.32.4)\n",
      "Collecting peft==0.4.0\n",
      "  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "      72.9/72.9 KB 3.8 MB/s eta 0:00:00\n",
      "Collecting accelerate==0.21.0\n",
      "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "      244.2/244.2 KB 4.7 MB/s eta 0:00:00\n",
      "Collecting scikit-learn==1.2.2\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "      9.6/9.6 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting timm==0.6.13\n",
      "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
      "      549.1/549.1 KB 4.3 MB/s eta 0:00:00\n",
      "Collecting fastapi\n",
      "  Using cached fastapi-0.116.0-py3-none-any.whl (95 kB)\n",
      "Collecting shortuuid\n",
      "  Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
      "Collecting markdown2[all]\n",
      "  Downloading markdown2-2.5.3-py3-none-any.whl (48 kB)\n",
      "      48.5/48.5 KB 4.1 MB/s eta 0:00:00\n",
      "Collecting gradio-client==0.8.1\n",
      "  Downloading gradio_client-0.8.1-py3-none-any.whl (305 kB)\n",
      "      305.2/305.2 KB 4.9 MB/s eta 0:00:00\n",
      "Collecting bitsandbytes==0.41.0\n",
      "  Downloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n",
      "      92.6/92.6 MB 2.8 MB/s eta 0:00:00\n",
      "Collecting uvicorn\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Collecting sentencepiece==0.1.99\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "      1.3/1.3 MB 3.4 MB/s eta 0:00:00\n",
      "Collecting einops==0.6.1\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "      42.2/42.2 KB 3.8 MB/s eta 0:00:00\n",
      "Collecting einops-exts==0.0.4\n",
      "  Downloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
      "Requirement already satisfied: psutil in ./env/lib/python3.10/site-packages (from accelerate==0.21.0->llava-torch) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./env/lib/python3.10/site-packages (from accelerate==0.21.0->llava-torch) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.10/site-packages (from accelerate==0.21.0->llava-torch) (24.2)\n",
      "Collecting aiofiles<24.0,>=22.0\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Collecting typer[all]<1.0,>=0.9\n",
      "  Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in ./env/lib/python3.10/site-packages (from gradio==4.16.0->llava-torch) (3.1.6)\n",
      "Collecting ruff>=0.1.7\n",
      "  Using cached ruff-0.12.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Collecting semantic-version~=2.0\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: matplotlib~=3.0 in ./env/lib/python3.10/site-packages (from gradio==4.16.0->llava-torch) (3.10.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in ./env/lib/python3.10/site-packages (from gradio==4.16.0->llava-torch) (0.33.2)\n",
      "Collecting altair<6.0,>=4.2.0\n",
      "  Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Collecting importlib-resources<7.0,>=1.3\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in ./env/lib/python3.10/site-packages (from gradio==4.16.0->llava-torch) (4.14.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in ./env/lib/python3.10/site-packages (from gradio==4.16.0->llava-torch) (2.3.1)\n",
      "Requirement already satisfied: orjson~=3.0 in ./env/lib/python3.10/site-packages (from gradio==4.16.0->llava-torch) (3.10.18)\n",
      "Collecting pillow<11.0,>=8.0\n",
      "  Using cached pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Collecting ffmpy\n",
      "  Using cached ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n",
      "Collecting markupsafe~=2.0\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting tomlkit==0.12.0\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Collecting python-multipart\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Collecting pydub\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.10/site-packages (from gradio-client==0.8.1->llava-torch) (2025.5.1)\n",
      "Collecting websockets<12.0,>=10.0\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "      129.9/129.9 KB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in ./env/lib/python3.10/site-packages (from httpx==0.24.0->llava-torch) (2025.6.15)\n",
      "Requirement already satisfied: idna in ./env/lib/python3.10/site-packages (from httpx==0.24.0->llava-torch) (3.10)\n",
      "Requirement already satisfied: sniffio in ./env/lib/python3.10/site-packages (from httpx==0.24.0->llava-torch) (1.3.1)\n",
      "Collecting httpcore<0.18.0,>=0.15.0\n",
      "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
      "      74.5/74.5 KB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: safetensors in ./env/lib/python3.10/site-packages (from peft==0.4.0->llava-torch) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./env/lib/python3.10/site-packages (from scikit-learn==1.2.2->llava-torch) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./env/lib/python3.10/site-packages (from scikit-learn==1.2.2->llava-torch) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./env/lib/python3.10/site-packages (from scikit-learn==1.2.2->llava-torch) (1.15.3)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.10/site-packages (from torch==2.0.1->llava-torch) (3.4.2)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "      98.6/98.6 KB 3.7 MB/s eta 0:00:00\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "      317.1/317.1 MB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.10/site-packages (from torch==2.0.1->llava-torch) (1.14.0)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "      11.8/11.8 MB 5.1 MB/s eta 0:00:00\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "      557.1/557.1 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "      177.1/177.1 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "      173.2/173.2 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "      63.3/63.3 MB 2.8 MB/s eta 0:00:00\n",
      "Collecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "      54.6/54.6 MB 3.6 MB/s eta 0:00:00\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "      849.3/849.3 KB 961.2 kB/s eta 0:00:00\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
      "      168.4/168.4 MB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.10/site-packages (from torch==2.0.1->llava-torch) (3.18.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "      21.0/21.0 MB 4.4 MB/s eta 0:00:00\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "      102.6/102.6 MB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.10/site-packages (from transformers==4.36.2->llava-torch) (2024.11.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.10/site-packages (from transformers==4.36.2->llava-torch) (4.67.1)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->llava-torch) (59.6.0)\n",
      "Requirement already satisfied: wheel in ./env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->llava-torch) (0.37.1)\n",
      "Collecting lit\n",
      "  Downloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
      "      96.4/96.4 KB 3.0 MB/s eta 0:00:00\n",
      "Collecting cmake\n",
      "  Downloading cmake-4.0.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)\n",
      "      27.9/27.9 MB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./env/lib/python3.10/site-packages (from pydantic->llava-torch) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./env/lib/python3.10/site-packages (from pydantic->llava-torch) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./env/lib/python3.10/site-packages (from pydantic->llava-torch) (2.33.2)\n",
      "Requirement already satisfied: h11>=0.8 in ./env/lib/python3.10/site-packages (from uvicorn->llava-torch) (0.16.0)\n",
      "Requirement already satisfied: click>=7.0 in ./env/lib/python3.10/site-packages (from uvicorn->llava-torch) (8.1.8)\n",
      "Collecting starlette<0.47.0,>=0.40.0\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Collecting latex2mathml\n",
      "  Downloading latex2mathml-3.78.0-py3-none-any.whl (73 kB)\n",
      "      73.7/73.7 KB 860.6 kB/s eta 0:00:00\n",
      "Collecting wavedrom\n",
      "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
      "      137.7/137.7 KB 624.7 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pygments>=2.7.3 in ./env/lib/python3.10/site-packages (from markdown2[all]->llava-torch) (2.19.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.10/site-packages (from requests->llava-torch) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./env/lib/python3.10/site-packages (from requests->llava-torch) (3.4.2)\n",
      "Collecting narwhals>=1.14.2\n",
      "  Using cached narwhals-1.46.0-py3-none-any.whl (373 kB)\n",
      "Collecting jsonschema>=3.0\n",
      "  Using cached jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in ./env/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx==0.24.0->llava-torch) (4.9.0)\n",
      "Collecting h11>=0.8\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./env/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio==4.16.0->llava-torch) (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./env/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava-torch) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./env/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava-torch) (3.2.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./env/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava-torch) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./env/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava-torch) (1.4.8)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./env/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava-torch) (4.58.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./env/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava-torch) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==4.16.0->llava-torch) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==4.16.0->llava-torch) (2025.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: typer 0.16.0 does not provide the extra 'all'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./env/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.16.0->llava-torch) (14.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.10/site-packages (from sympy->torch==2.0.1->llava-torch) (1.3.0)\n",
      "Requirement already satisfied: six in ./env/lib/python3.10/site-packages (from wavedrom->markdown2[all]->llava-torch) (1.17.0)\n",
      "Collecting svgwrite\n",
      "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
      "      67.1/67.1 KB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./env/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx==0.24.0->llava-torch) (1.3.0)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Using cached rpds_py-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./env/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava-torch) (25.3.0)\n",
      "Collecting referencing>=0.28.4\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./env/lib/python3.10/site-packages (from rich>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.16.0->llava-torch) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.16.0->llava-torch) (0.1.2)\n",
      "Building wheels for collected packages: wavedrom\n",
      "  Building wheel for wavedrom (setup.py): started\n",
      "  Building wheel for wavedrom (setup.py): finished with status 'done'\n",
      "  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30106 sha256=9cae49b94f8e9a01c77874c7b12995ccc4ceb848eebb29dad9b84728b68afbd4\n",
      "  Stored in directory: /home/osama/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\n",
      "Successfully built wavedrom\n",
      "Installing collected packages: sentencepiece, pydub, lit, bitsandbytes, websockets, tomlkit, svgwrite, shortuuid, shellingham, semantic-version, ruff, rpds-py, python-multipart, pillow, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, narwhals, markupsafe, markdown2, latex2mathml, importlib-resources, h11, ffmpy, einops, cmake, aiofiles, wavedrom, uvicorn, referencing, nvidia-cusolver-cu11, nvidia-cudnn-cu11, einops-exts, typer, tokenizers, starlette, scikit-learn, jsonschema-specifications, httpcore, transformers, jsonschema, httpx, fastapi, gradio-client, altair, gradio, triton, torch, torchvision, accelerate, timm, peft, llava-torch\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.2.1\n",
      "    Uninstalling pillow-11.2.1:\n",
      "      Successfully uninstalled pillow-11.2.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.16.0\n",
      "    Uninstalling h11-0.16.0:\n",
      "      Successfully uninstalled h11-0.16.0\n",
      "  Attempting uninstall: einops\n",
      "    Found existing installation: einops 0.8.1\n",
      "    Uninstalling einops-0.8.1:\n",
      "      Successfully uninstalled einops-0.8.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.7.0\n",
      "    Uninstalling scikit-learn-1.7.0:\n",
      "      Successfully uninstalled scikit-learn-1.7.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.9\n",
      "    Uninstalling httpcore-1.0.9:\n",
      "      Successfully uninstalled httpcore-1.0.9\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.1\n",
      "    Uninstalling transformers-4.53.1:\n",
      "      Successfully uninstalled transformers-4.53.1\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.3.1\n",
      "    Uninstalling triton-3.3.1:\n",
      "      Successfully uninstalled triton-3.3.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.1\n",
      "    Uninstalling torch-2.7.1:\n",
      "      Successfully uninstalled torch-2.7.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.22.1\n",
      "    Uninstalling torchvision-0.22.1:\n",
      "      Successfully uninstalled torchvision-0.22.1\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.8.1\n",
      "    Uninstalling accelerate-1.8.1:\n",
      "      Successfully uninstalled accelerate-1.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.7.1+cu118 requires torch==2.7.1, but you have torch 2.0.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed accelerate-0.21.0 aiofiles-23.2.1 altair-5.5.0 bitsandbytes-0.41.0 cmake-4.0.3 einops-0.6.1 einops-exts-0.0.4 fastapi-0.116.0 ffmpy-0.6.0 gradio-4.16.0 gradio-client-0.8.1 h11-0.14.0 httpcore-0.17.3 httpx-0.24.0 importlib-resources-6.5.2 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 latex2mathml-3.78.0 lit-18.1.8 llava-torch-1.2.2.post1 markdown2-2.5.3 markupsafe-2.1.5 narwhals-1.46.0 numpy-1.26.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 peft-0.4.0 pillow-10.4.0 pydub-0.25.1 python-multipart-0.0.20 referencing-0.36.2 rpds-py-0.26.0 ruff-0.12.2 scikit-learn-1.2.2 semantic-version-2.10.0 sentencepiece-0.1.99 shellingham-1.5.4 shortuuid-1.0.13 starlette-0.46.2 svgwrite-1.4.3 timm-0.6.13 tokenizers-0.15.0 tomlkit-0.12.0 torch-2.0.1 torchvision-0.15.2 transformers-4.36.2 triton-2.0.0 typer-0.16.0 uvicorn-0.35.0 wavedrom-2.0.3.post3 websockets-11.0.3\n",
      " llava-torch installed successfully!\n",
      "Ready to use llava-torch for OCR processing!\n"
     ]
    }
   ],
   "source": [
    "# Install llava-torch package\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_llava_torch():\n",
    "    \"\"\"Install llava-torch package\"\"\"\n",
    "    try:\n",
    "        print(\"Installing llava-torch...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"llava-torch\"])\n",
    "        print(\" llava-torch installed successfully!\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\" Failed to install llava-torch: {e}\")\n",
    "        return False\n",
    "\n",
    "# Install the package\n",
    "install_success = install_llava_torch()\n",
    "\n",
    "if install_success:\n",
    "    print(\"Ready to use llava-torch for OCR processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wrj97tw1klf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osama/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "2025-07-11 02:26:04.685752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752182764.698357   18694 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752182764.701981   18694 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752182764.712514   18694 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752182764.712530   18694 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752182764.712532   18694 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752182764.712533   18694 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-11 02:26:04.715645: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " llava-torch imported successfully!\n",
      "Output folder: extracted_text/llava_torch\n",
      "Using device: cuda\n",
      "Loading model: liuhaotian/llava-v1.5-7b\n",
      "This may take a few minutes for first-time download...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osama/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Downloading shards:   0%|          | 0/2 [07:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 209\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Run the processing\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[43mprocess_images_with_llava_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m, in \u001b[0;36mprocess_images_with_llava_torch\u001b[0;34m(images_folder, output_folder)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may take a few minutes for first-time download...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     tokenizer, model, image_processor, context_len \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Model loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/llava/model/builder.py:116\u001b[0m, in \u001b[0;36mload_pretrained_model\u001b[0;34m(model_path, model_base, model_name, load_8bit, load_4bit, device_map, device, use_flash_attn, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m             tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 116\u001b[0m             model \u001b[38;5;241m=\u001b[39m \u001b[43mLlavaLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# Load language model\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;66;03m# PEFT model\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/transformers/modeling_utils.py:3351\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3350\u001b[0m     \u001b[38;5;66;03m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3351\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3360\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3363\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3367\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3368\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3369\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3370\u001b[0m ):\n\u001b[1;32m   3371\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/transformers/utils/hub.py:1017\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1016\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/transformers/utils/hub.py:389\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1006\u001b[0m     )\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1161\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1161\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1174\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1725\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1719\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1720\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo, but the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1721\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to regular HTTP download. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1722\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1723\u001b[0m             )\n\u001b[0;32m-> 1725\u001b[0m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1734\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1735\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:494\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    492\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    496\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/urllib3/response.py:1091\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1091\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1094\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/urllib3/response.py:980\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 980\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/urllib3/response.py:904\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    901\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/urllib3/response.py:887\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LLaVA-Torch OCR Implementation\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "def process_images_with_llava_torch(images_folder=\"part_2_images\", output_folder=\"extracted_text/llava_torch\"):\n",
    "    \"\"\"\n",
    "    Process images using llava-torch package for OCR\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Import llava-torch components\n",
    "        from llava.model.builder import load_pretrained_model\n",
    "        from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "        from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "        from llava.conversation import conv_templates, SeparatorStyle\n",
    "        \n",
    "        print(\" llava-torch imported successfully!\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\" Failed to import llava-torch: {e}\")\n",
    "        print(\"Make sure llava-torch is installed: pip install llava-torch\")\n",
    "        return\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "    # Setup device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # Load model - using a smaller model path that should work with llava-torch\n",
    "        model_path = \"liuhaotian/llava-v1.5-7b\"  # This should work with llava-torch\n",
    "        model_name = get_model_name_from_path(model_path)\n",
    "        \n",
    "        print(f\"Loading model: {model_path}\")\n",
    "        print(\"This may take a few minutes for first-time download...\")\n",
    "        \n",
    "        tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "            model_path=model_path,\n",
    "            model_base=None,\n",
    "            model_name=model_name,\n",
    "            load_8bit=False,\n",
    "            load_4bit=False,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(\" Model loaded successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Failed to load model: {e}\")\n",
    "        print(\"Trying alternative model...\")\n",
    "        \n",
    "        # Try alternative model paths\n",
    "        alternative_models = [\n",
    "            \"liuhaotian/llava-v1.5-13b\",\n",
    "            \"liuhaotian/llava-v1.6-mistral-7b\",\n",
    "            \"liuhaotian/llava-v1.6-vicuna-7b\"\n",
    "        ]\n",
    "        \n",
    "        model_loaded = False\n",
    "        for alt_model in alternative_models:\n",
    "            try:\n",
    "                print(f\"Trying {alt_model}...\")\n",
    "                model_name = get_model_name_from_path(alt_model)\n",
    "                tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "                    model_path=alt_model,\n",
    "                    model_base=None,\n",
    "                    model_name=model_name,\n",
    "                    load_8bit=False,\n",
    "                    load_4bit=False,\n",
    "                    device=device\n",
    "                )\n",
    "                print(f\" Successfully loaded {alt_model}!\")\n",
    "                model_loaded = True\n",
    "                break\n",
    "            except Exception as alt_e:\n",
    "                print(f\" Failed {alt_model}: {alt_e}\")\n",
    "                continue\n",
    "        \n",
    "        if not model_loaded:\n",
    "            print(\" All model loading attempts failed.\")\n",
    "            return\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Process each image\n",
    "    processed_count = 0\n",
    "    \n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "        # Check if already processed\n",
    "        if os.path.exists(text_file_path):\n",
    "            print(f\"[{i}/{len(image_files)}] Skipping {image_filename} - already processed\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"[{i}/{len(image_files)}] Processing {image_filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load and process image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Resize if too large\n",
    "            max_size = 512  # Conservative size for llava-torch\n",
    "            if max(image.size) > max_size:\n",
    "                image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
    "                print(f\"Resized image to {image.size}\")\n",
    "            \n",
    "            # Process image\n",
    "            image_tensor = process_images([image], image_processor, model.config)\n",
    "            if type(image_tensor) is list:\n",
    "                image_tensor = [image.to(dtype=torch.float16, device=device) for image in image_tensor]\n",
    "            else:\n",
    "                image_tensor = image_tensor.to(dtype=torch.float16, device=device)\n",
    "            \n",
    "            # Prepare conversation\n",
    "            conv_mode = \"llava_v1\"  # Default conversation mode\n",
    "            conv = conv_templates[conv_mode].copy()\n",
    "            \n",
    "            # OCR prompt\n",
    "            inp = \"Extract all text from this image. Provide only the extracted text without any additional commentary.\"\n",
    "            \n",
    "            if model.config.mm_use_im_start_end:\n",
    "                inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n",
    "            else:\n",
    "                inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n",
    "            \n",
    "            conv.append_message(conv.roles[0], inp)\n",
    "            conv.append_message(conv.roles[1], None)\n",
    "            prompt = conv.get_prompt()\n",
    "            \n",
    "            # Tokenize\n",
    "            input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(device)\n",
    "            \n",
    "            # Generate\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                output_ids = model.generate(\n",
    "                    input_ids,\n",
    "                    images=image_tensor,\n",
    "                    image_sizes=[image.size],\n",
    "                    do_sample=False,\n",
    "                    temperature=0.1,\n",
    "                    max_new_tokens=512,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            # Decode output\n",
    "            outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "            \n",
    "            # Clean up output - remove the input prompt\n",
    "            if prompt in outputs:\n",
    "                extracted_text = outputs.replace(prompt, \"\").strip()\n",
    "            else:\n",
    "                extracted_text = outputs.strip()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "            \n",
    "            # Save result\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(extracted_text)\n",
    "            \n",
    "            print(f\"Saved text to: {text_file_path}\")\n",
    "            if extracted_text:\n",
    "                preview = extracted_text[:100] + \"...\" if len(extracted_text) > 100 else extracted_text\n",
    "                print(f\"Preview: {preview}\")\n",
    "            else:\n",
    "                print(\"No text extracted\")\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "            # Clear GPU memory\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_filename}: {e}\")\n",
    "            # Save error file\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"ERROR: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n=== LLaVA-Torch Processing Complete ===\")\n",
    "    print(f\"Processed: {processed_count} images\")\n",
    "    print(f\"Results saved to: {output_folder}\")\n",
    "\n",
    "# Run the processing\n",
    "if __name__ == \"__main__\":\n",
    "    process_images_with_llava_torch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
