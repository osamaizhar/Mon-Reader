{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i6lkqeev8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "# Clear GPU memory and set environment\n",
    "torch.cuda.empty_cache()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def setup_llava_model():\n",
    "    \"\"\"Setup LLaVA model with proper error handling and retries\"\"\"\n",
    "    print(\"üöÄ Setting up LLaVA model...\")\n",
    "    \n",
    "    # Use LLaVA-NeXT which is more stable and faster to download\n",
    "    model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üì• Loading processor from {model_id}...\")\n",
    "        processor = LlavaNextProcessor.from_pretrained(model_id)\n",
    "        print(\"‚úÖ Processor loaded!\")\n",
    "        \n",
    "        print(f\"üì• Loading model from {model_id}...\")\n",
    "        model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        print(\"‚úÖ Model loaded!\")\n",
    "        \n",
    "        return processor, model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {model_id}: {e}\")\n",
    "        print(\"üîÑ Trying alternative model...\")\n",
    "        \n",
    "        # Fallback to smaller, more reliable model\n",
    "        fallback_model = \"llava-hf/llava-1.5-7b-hf\"\n",
    "        try:\n",
    "            print(f\"üì• Loading fallback processor from {fallback_model}...\")\n",
    "            processor = LlavaNextProcessor.from_pretrained(fallback_model)\n",
    "            print(\"‚úÖ Fallback processor loaded!\")\n",
    "            \n",
    "            print(f\"üì• Loading fallback model from {fallback_model}...\")\n",
    "            model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "                fallback_model,\n",
    "                torch_dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            print(\"‚úÖ Fallback model loaded!\")\n",
    "            \n",
    "            return processor, model\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Fallback also failed: {e2}\")\n",
    "            raise Exception(\"Both primary and fallback models failed to load\")\n",
    "\n",
    "def process_single_image_llava(image_path, processor, model, output_folder):\n",
    "    \"\"\"Process a single image with LLaVA\"\"\"\n",
    "    filename = os.path.basename(image_path)\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "    \n",
    "    try:\n",
    "        # Load and prepare image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Resize if too large (important for GPU memory)\n",
    "        max_size = 512\n",
    "        if max(image.size) > max_size:\n",
    "            image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Prepare conversation\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"Extract all text from this image. Provide only the extracted text without any additional commentary or explanations.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template and tokenize\n",
    "        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        # Generate with optimized settings\n",
    "        print(f\"üîç Processing {filename}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=False,\n",
    "                temperature=0.1,\n",
    "                pad_token_id=processor.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = processor.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Clean up the output - extract only the assistant's response\n",
    "        if \"assistant\" in generated_text.lower():\n",
    "            # Find the last occurrence of assistant and extract text after it\n",
    "            assistant_pos = generated_text.lower().rfind(\"assistant\")\n",
    "            if assistant_pos != -1:\n",
    "                extracted_text = generated_text[assistant_pos + len(\"assistant\"):].strip()\n",
    "            else:\n",
    "                extracted_text = generated_text.strip()\n",
    "        else:\n",
    "            extracted_text = generated_text.strip()\n",
    "        \n",
    "        # Remove any remaining conversation artifacts\n",
    "        lines = extracted_text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line and not any(skip in line.lower() for skip in ['user:', 'assistant:', 'extract all text', 'provide only']):\n",
    "                cleaned_lines.append(line)\n",
    "        \n",
    "        final_text = '\\n'.join(cleaned_lines).strip()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Save result\n",
    "        with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_text)\n",
    "        \n",
    "        print(f\"‚úÖ Processed in {end_time - start_time:.2f}s\")\n",
    "        if final_text:\n",
    "            preview = final_text[:100] + \"...\" if len(final_text) > 100 else final_text\n",
    "            print(f\"üìù Extracted: {preview}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No text found\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "        # Save error file\n",
    "        with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"ERROR: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def llava_ocr_main(images_folder=\"part_2_images\", output_folder=\"extracted_text/llava_fixed\"):\n",
    "    \"\"\"Main LLaVA OCR function\"\"\"\n",
    "    print(\"üî• STARTING FIXED LLaVA OCR PROCESSING üî•\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Setup model\n",
    "    try:\n",
    "        processor, model = setup_llava_model()\n",
    "    except Exception as e:\n",
    "        print(f\"üíÄ Failed to setup model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Find images\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"‚ùå No images found in {images_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üñºÔ∏è Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Process images one by one\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        print(f\"\\nüì∑ [{i}/{len(image_files)}] Processing: {os.path.basename(image_path)}\")\n",
    "        \n",
    "        success = process_single_image_llava(image_path, processor, model, output_folder)\n",
    "        \n",
    "        if success:\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "        \n",
    "        # Clear GPU memory after each image\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nüéâ LLaVA OCR COMPLETE! üéâ\")\n",
    "    print(f\"‚úÖ Successful: {successful}\")\n",
    "    print(f\"‚ùå Failed: {failed}\")\n",
    "    print(f\"üìÅ Results saved to: {output_folder}\")\n",
    "\n",
    "# Run the fixed LLaVA implementation\n",
    "if __name__ == \"__main__\":\n",
    "    llava_ocr_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l389tfphgx8",
   "metadata": {},
   "source": [
    "# FIXED LLaVA Implementation - WORKING VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hv2b614vjfw",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import easyocr\n",
    "import pyttsx3\n",
    "from langdetect import detect\n",
    "import time\n",
    "\n",
    "def simple_ocr_solution(images_folder=\"part_2_images\", output_folder=\"extracted_text\"):\n",
    "    \"\"\"\n",
    "    Simple working OCR solution using EasyOCR that actually works\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "    # Initialize EasyOCR with multiple languages\n",
    "    print(\"Initializing EasyOCR...\")\n",
    "    reader = easyocr.Reader(['en', 'ar', 'ur'], gpu=True, verbose=False)\n",
    "    print(\"‚úÖ EasyOCR ready!\")\n",
    "    \n",
    "    # Initialize TTS\n",
    "    tts_engine = pyttsx3.init()\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"‚ùå No images found in {images_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    successful_extractions = 0\n",
    "    \n",
    "    # Process each image\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "        print(f\"\\n[{i}/{len(image_files)}] Processing: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Read image with OpenCV\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"‚ùå Could not read image: {image_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract text using EasyOCR\n",
    "            start_time = time.time()\n",
    "            results = reader.readtext(image, paragraph=True, detail=0)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Combine all text\n",
    "            extracted_text = \" \".join(results).strip()\n",
    "            \n",
    "            if extracted_text:\n",
    "                # Try to detect language\n",
    "                try:\n",
    "                    detected_lang = detect(extracted_text)\n",
    "                    print(f\"‚úÖ Text extracted in {end_time - start_time:.2f}s - Language: {detected_lang}\")\n",
    "                    \n",
    "                    # Save to file with language info\n",
    "                    with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(f\"LANG:{detected_lang}\\n\")\n",
    "                        f.write(extracted_text)\n",
    "                    \n",
    "                    print(f\"üíæ Saved to: {text_file_path}\")\n",
    "                    print(f\"üìù Preview: {extracted_text[:100]}...\")\n",
    "                    \n",
    "                    successful_extractions += 1\n",
    "                    \n",
    "                except Exception as lang_error:\n",
    "                    print(f\"‚ö†Ô∏è Language detection failed: {lang_error}\")\n",
    "                    with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(\"LANG:unknown\\n\")\n",
    "                        f.write(extracted_text)\n",
    "                    successful_extractions += 1\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No text detected\")\n",
    "                # Save empty file\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(\"LANG:none\\n\")\n",
    "                    f.write(\"\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n=== Processing Complete ===\")\n",
    "    print(f\"‚úÖ Successfully extracted text from {successful_extractions}/{len(image_files)} images\")\n",
    "    \n",
    "    # Ask if user wants to read files aloud\n",
    "    if successful_extractions > 0:\n",
    "        try:\n",
    "            response = input(f\"\\nüîä Read {successful_extractions} text files aloud? (y/n): \").lower()\n",
    "            if response in ['y', 'yes']:\n",
    "                read_text_files_aloud(output_folder, tts_engine)\n",
    "        except:\n",
    "            print(\"Skipping TTS...\")\n",
    "    \n",
    "    return output_folder\n",
    "\n",
    "def read_text_files_aloud(text_folder, tts_engine):\n",
    "    \"\"\"Read text files aloud\"\"\"\n",
    "    text_files = glob.glob(os.path.join(text_folder, \"*.txt\"))\n",
    "    \n",
    "    for i, text_file in enumerate(text_files, 1):\n",
    "        print(f\"\\nüîä Reading file {i}/{len(text_files)}: {os.path.basename(text_file)}\")\n",
    "        \n",
    "        try:\n",
    "            with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            if len(lines) > 1:\n",
    "                content = \"\".join(lines[1:]).strip()  # Skip language line\n",
    "                if content:\n",
    "                    tts_engine.say(content)\n",
    "                    tts_engine.runAndWait()\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    print(\"No content to read\")\n",
    "            else:\n",
    "                print(\"File is empty\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {text_file}: {e}\")\n",
    "\n",
    "# Run the simple solution\n",
    "if __name__ == \"__main__\":\n",
    "    simple_ocr_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1zvuqr88me",
   "metadata": {},
   "source": [
    "# Working Simple OCR Solution (EasyOCR + TTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qp57ab7sq9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download LLaVA model in smaller chunks with retries\n",
    "import os\n",
    "import time\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Fix for slow downloads\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '0'\n",
    "os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '3600'  # 1 hour timeout\n",
    "\n",
    "def download_llava_model():\n",
    "    \"\"\"Download LLaVA model with retry mechanism\"\"\"\n",
    "    max_retries = 3\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries}\")\n",
    "            \n",
    "            # Try processor first\n",
    "            print(\"Downloading processor...\")\n",
    "            processor = AutoProcessor.from_pretrained(\n",
    "                \"llava-hf/llava-1.5-7b-hf\", \n",
    "                use_fast=False,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=\"./models\"  # Local cache\n",
    "            )\n",
    "            print(\"‚úÖ Processor downloaded!\")\n",
    "            \n",
    "            # Then model\n",
    "            print(\"Downloading model (this will take a while)...\")\n",
    "            model = LlavaForConditionalGeneration.from_pretrained(\n",
    "                \"llava-hf/llava-1.5-7b-hf\",\n",
    "                torch_dtype=torch.float16,\n",
    "                low_cpu_mem_usage=True,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=\"./models\"  # Local cache\n",
    "            )\n",
    "            print(\"‚úÖ Model downloaded successfully!\")\n",
    "            \n",
    "            return processor, model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"Retrying in 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(\"All attempts failed. Check your internet connection.\")\n",
    "                return None, None\n",
    "\n",
    "# Run the download\n",
    "processor, model = download_llava_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uq1mto0emu9",
   "metadata": {},
   "source": [
    "# Working LLaVA Setup (Fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-report-2025",
   "metadata": {},
   "source": [
    "## Environment Report\n",
    "\n",
    "**Current Package Versions (Baseline before LLAVA/SHIKRA integration):**\n",
    "\n",
    "- **Python**: 3.13.3\n",
    "- **torch**: 2.7.1+cu128\n",
    "- **torchvision**: 0.22.1+cu128\n",
    "- **torchaudio**: 2.7.1+cu128\n",
    "- **easyocr**: 1.7.2\n",
    "- **opencv-python**: 4.12.0.88\n",
    "- **pillow**: 11.2.1\n",
    "- **numpy**: 2.2.6\n",
    "- **pyttsx3**: 2.99\n",
    "- **langdetect**: 1.0.9\n",
    "- **transformers**: 4.52.3\n",
    "- **gradio**: 5.31.0\n",
    "- **CUDA Available**: Yes\n",
    "- **CUDA Version**: 12.8\n",
    "- **GPU**: NVIDIA GeForce RTX 2060\n",
    "- **GPU Memory**: 12.88 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05407b00",
   "metadata": {},
   "source": [
    "# Imports and Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932d8684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osama/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-11 23:06:10.304046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752257170.342471    5747 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752257170.353169    5747 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752257170.428920    5747 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752257170.428954    5747 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752257170.428957    5747 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752257170.428960    5747 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-11 23:06:10.440418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLAVA OCR Processing ===\n",
      "GPU: NVIDIA GeForce RTX 2050\n",
      "GPU Memory: 3.96 GB\n",
      "HuggingFace cache location: default\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import easyocr\n",
    "import pyttsx3\n",
    "from langdetect import detect\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "## Moving Hugging Face default Download Dir\n",
    "\n",
    "# # LLAVA OCR Cell - GPU Accelerated Implementation\n",
    "# import os\n",
    "\n",
    "# # IMPORTANT: Set cache BEFORE any imports from transformers/huggingface\n",
    "# os.environ['HF_HOME'] = 'D:/HuggingFaceCache'\n",
    "# os.environ['TRANSFORMERS_CACHE'] = 'D:/HuggingFaceCache/transformers'\n",
    "# os.environ['HUGGINGFACE_HUB_CACHE'] = 'D:/HuggingFaceCache/hub'\n",
    "\n",
    "# # Now import everything else\n",
    "# import glob\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# import time\n",
    "# import gc\n",
    "# from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "# Force GPU usage\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Check GPU availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: CUDA not available. LLAVA requires GPU acceleration.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"=== LLAVA OCR Processing ===\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"HuggingFace cache location: {os.environ.get('HF_HOME', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8207ed",
   "metadata": {},
   "source": [
    "# Simple TTS using pyttsx3 Gpu based:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force GPU usage and optimization for RTX 2060\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU\n",
    "torch.backends.cudnn.benchmark = True  # Enable cudnn autotuner for performance\n",
    "\n",
    "# Check GPU and setup\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: CUDA not available. This script requires GPU acceleration.\")\n",
    "    print(\"Please check your NVIDIA drivers and PyTorch installation.\")\n",
    "    exit(1)\n",
    "\n",
    "# Display GPU information\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Initialize EasyOCR reader with explicit GPU settings\n",
    "print(\"Initializing EasyOCR with GPU acceleration...\")\n",
    "reader = easyocr.Reader(\n",
    "    [\"ar\", \"ur\", \"en\"], \n",
    "    gpu=True,\n",
    "    verbose=False,\n",
    "    # For RTX 2060, set reasonable batch size and model parameters\n",
    "    detector=True,\n",
    "    recognizer=True\n",
    ")\n",
    "\n",
    "# Initialize TTS engine\n",
    "tts_engine = pyttsx3.init()\n",
    "\n",
    "def log_gpu_memory():\n",
    "    \"\"\"Log current GPU memory usage\"\"\"\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "\n",
    "def process_images_ocr_save_text(images_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process all images in the folder with GPU-accelerated OCR and save to text files\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output folder: {output_folder}\")\n",
    "    \n",
    "    # Get all image files in the folder\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Report initial GPU memory\n",
    "    log_gpu_memory()\n",
    "\n",
    "    # Process files\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "        print(f\"\\n--- Processing Image {i}/{len(image_files)}: {image_filename} ---\")\n",
    "\n",
    "        try:\n",
    "            # Read image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Could not read image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Optimize image for GPU processing (resize large images)\n",
    "            h, w = image.shape[:2]\n",
    "            max_dim = 2000  # Optimal for RTX 2060 memory\n",
    "            if max(h, w) > max_dim:\n",
    "                scale = max_dim / max(h, w)\n",
    "                image = cv2.resize(image, (int(w * scale), int(h * scale)))\n",
    "                print(f\"Resized image to {image.shape[1]}x{image.shape[0]} to optimize GPU memory\")\n",
    "            \n",
    "            # Report GPU memory before OCR\n",
    "            log_gpu_memory()\n",
    "\n",
    "            # Extract text using GPU-accelerated EasyOCR\n",
    "            print(\"Extracting text with GPU acceleration...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # For RTX 2060, use appropriate batch size\n",
    "            results = reader.readtext(\n",
    "                image,\n",
    "                batch_size=2,  # Adjust based on your GPU memory\n",
    "                paragraph=True,  # Group text into paragraphs\n",
    "                detail=0  # 0 for more accuracy\n",
    "            )\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "            # Combine all detected text with confidence filtering\n",
    "            extracted_text = \" \".join(\n",
    "                [result[1] for result in results if result[2] > 0.5]\n",
    "            )  # confidence > 0.5\n",
    "\n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Extracted text: {extracted_text}\")\n",
    "            \n",
    "            # Try to detect language\n",
    "            try:\n",
    "                detected_lang = detect(extracted_text)\n",
    "                print(f\"Detected language: {detected_lang}\")\n",
    "                \n",
    "                # Save text with language information to file\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                    text_file.write(f\"LANG:{detected_lang}\\n\")\n",
    "                    text_file.write(extracted_text)\n",
    "                \n",
    "                print(f\"Saved text to: {text_file_path}\")\n",
    "                \n",
    "            except Exception as lang_error:\n",
    "                print(f\"Language detection failed: {lang_error}\")\n",
    "                # Save text without language information\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                    text_file.write(f\"LANG:unknown\\n\")\n",
    "                    text_file.write(extracted_text)\n",
    "                \n",
    "                print(f\"Saved text to: {text_file_path} (language unknown)\")\n",
    "\n",
    "            # Clear GPU memory after each image\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            # Clear GPU memory on error\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "    # Final GPU memory cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"\\n--- OCR Processing Complete ---\")\n",
    "    log_gpu_memory()\n",
    "    \n",
    "    return output_folder\n",
    "\n",
    "def read_text_files_aloud(text_folder):\n",
    "    \"\"\"\n",
    "    Read all text files in the folder aloud using TTS, one by one with clear separation\n",
    "    \"\"\"\n",
    "    # Get all text files\n",
    "    text_files = glob.glob(os.path.join(text_folder, \"*.txt\"))\n",
    "    \n",
    "    if not text_files:\n",
    "        print(f\"No text files found in {text_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(text_files)} text files to read\")\n",
    "    \n",
    "    # Language mapping for TTS\n",
    "    lang_mapping = {\n",
    "        \"en\": \"english\",\n",
    "        \"ar\": \"arabic\",\n",
    "        \"ur\": \"urdu\",\n",
    "        \"hi\": \"hindi\",\n",
    "        \"fa\": \"persian\",\n",
    "        \"ps\": \"pashto\"\n",
    "    }\n",
    "    \n",
    "    # Sort text files alphabetically to ensure consistent reading order\n",
    "    text_files.sort()\n",
    "    \n",
    "    for i, text_file_path in enumerate(text_files, 1):\n",
    "        file_name = os.path.basename(text_file_path)\n",
    "        print(f\"\\n===== Reading File {i}/{len(text_files)}: {file_name} =====\")\n",
    "        \n",
    "        # Announce the file being read (optional)\n",
    "        tts_engine.setProperty(\"rate\", 150)\n",
    "        announcement = f\"Reading file {i} of {len(text_files)}: {os.path.splitext(file_name)[0]}\"\n",
    "        print(announcement)\n",
    "        tts_engine.say(announcement)\n",
    "        tts_engine.runAndWait()\n",
    "        \n",
    "        # Pause between announcement and content\n",
    "        time.sleep(1)\n",
    "        \n",
    "        try:\n",
    "            # Read text file\n",
    "            with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                \n",
    "            if not lines:\n",
    "                print(f\"File is empty: {text_file_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Extract language information from first line\n",
    "            lang_line = lines[0].strip()\n",
    "            if lang_line.startswith(\"LANG:\"):\n",
    "                detected_lang = lang_line[5:]\n",
    "                print(f\"Language: {detected_lang}\")\n",
    "                # Remove the language line\n",
    "                content = \"\".join(lines[1:])\n",
    "            else:\n",
    "                # No language information, treat all lines as content\n",
    "                detected_lang = \"unknown\"\n",
    "                content = \"\".join(lines)\n",
    "            \n",
    "            if not content.strip():\n",
    "                print(\"No content to read\")\n",
    "                continue\n",
    "                \n",
    "            # Print a preview of the content\n",
    "            content_preview = content[:100] + \"...\" if len(content) > 100 else content\n",
    "            print(f\"Text content: {content_preview}\")\n",
    "            \n",
    "            # Get available voices\n",
    "            voices = tts_engine.getProperty(\"voices\")\n",
    "            \n",
    "            # Try to set appropriate voice based on language\n",
    "            voice_set = False\n",
    "            for voice in voices:\n",
    "                tts_lang = lang_mapping.get(detected_lang, \"english\")\n",
    "                if (tts_lang.lower() in voice.name.lower() or \n",
    "                    detected_lang in voice.id.lower()):\n",
    "                    tts_engine.setProperty(\"voice\", voice.id)\n",
    "                    voice_set = True\n",
    "                    print(f\"Using voice: {voice.name}\")\n",
    "                    break\n",
    "            \n",
    "            if not voice_set:\n",
    "                print(f\"No specific voice found for {detected_lang}, using default\")\n",
    "            \n",
    "            # Adjust speech rate based on language\n",
    "            if detected_lang in [\"ar\", \"ur\"]:\n",
    "                # Slower for Arabic and Urdu\n",
    "                tts_engine.setProperty(\"rate\", 130)\n",
    "            else:\n",
    "                tts_engine.setProperty(\"rate\", 150)\n",
    "            \n",
    "            # Read the text aloud\n",
    "            print(f\"Reading text aloud...\")\n",
    "            tts_engine.say(content)\n",
    "            tts_engine.runAndWait()\n",
    "            \n",
    "            # Pause between files to clearly separate them\n",
    "            print(\"Finished reading file.\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {text_file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n===== All Text Files Have Been Read =====\")\n",
    "\n",
    "def get_user_confirmation():\n",
    "    \"\"\"\n",
    "    Ask the user if they want to proceed to the TTS reading phase\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        response = input(\"\\nOCR processing complete. Proceed with reading text files? (y/n): \").lower()\n",
    "        if response in ['y', 'yes']:\n",
    "            return True\n",
    "        elif response in ['n', 'no']:\n",
    "            return False\n",
    "        else:\n",
    "            print(\"Please enter 'y' or 'n'\")\n",
    "\n",
    "# Main process\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Define folders\n",
    "        images_folder = \"part_2_images\"\n",
    "        output_folder = \"extracted_text\"\n",
    "        \n",
    "        # Print GPU optimization message\n",
    "        print(\"=== Running GPU-Optimized OCR for RTX 2060 ===\")\n",
    "        \n",
    "        # Step 1: Process images with OCR and save text\n",
    "        start_time = time.time()\n",
    "        text_folder = process_images_ocr_save_text(images_folder, output_folder)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"OCR processing completed in {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Optional: Ask for user confirmation before proceeding to TTS\n",
    "        if get_user_confirmation():\n",
    "            # Step 2: Read the saved text files aloud one by one\n",
    "            read_text_files_aloud(text_folder)\n",
    "        else:\n",
    "            print(\"TTS reading canceled. Text files are saved in the output folder.\")\n",
    "        \n",
    "        print(\"Process completed successfully!\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user\")\n",
    "        # Clean up GPU memory on interrupt\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Clean up GPU memory on error\n",
    "        torch.cuda.empty_cache()\n",
    "    finally:\n",
    "        # Final cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a smaller vision model first\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\", use_fast=False)\n",
    "print(\"Downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15f199",
   "metadata": {},
   "source": [
    "# LLava Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9acecd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLAVA OCR Processing ===\n",
      "GPU: NVIDIA GeForce RTX 2050\n",
      "GPU Memory: 3.96 GB\n",
      "Using GPU acceleration\n",
      "Output folder: extracted_text/llava\n",
      "Loading LLAVA model...\n",
      "Downloading/Loading processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/3 [05:47<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 477\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# Run LLAVA OCR processing\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# Uncomment the line below if you need to install requirements\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# install_requirements()\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m     \u001b[43mprocess_images_with_llava\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 224\u001b[0m, in \u001b[0;36mprocess_images_with_llava\u001b[0;34m(images_folder, output_folder)\u001b[0m\n\u001b[1;32m    219\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    220\u001b[0m     model_name, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    221\u001b[0m )\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading/Loading model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 224\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlavaForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use fp16 only if GPU available\u001b[39;49;00m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use auto device mapping only if GPU available\u001b[39;49;00m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# Move model to appropriate device\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_gpu \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mis_cuda:\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/transformers/modeling_utils.py:3351\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   3349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   3350\u001b[0m     \u001b[38;5;66;03m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 3351\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3360\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3363\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3367\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3368\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3369\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3370\u001b[0m ):\n\u001b[1;32m   3371\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/transformers/utils/hub.py:1017\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1016\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/transformers/utils/hub.py:389\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1006\u001b[0m     )\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1161\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1161\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1174\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1725\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1719\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1720\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo, but the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1721\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to regular HTTP download. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1722\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1723\u001b[0m             )\n\u001b[0;32m-> 1725\u001b[0m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1734\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1735\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:494\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    492\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    496\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/urllib3/response.py:1091\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1091\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1094\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/urllib3/response.py:980\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 980\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/urllib3/response.py:904\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    901\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.10/site-packages/urllib3/response.py:887\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # LLAVA OCR Cell - GPU Accelerated Implementation\n",
    "\n",
    "# # Force GPU usage\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# os.environ['HF_HOME'] = 'D:/HuggingFaceCache'\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# # Check GPU availability\n",
    "# if not torch.cuda.is_available():\n",
    "#     print(\"ERROR: CUDA not available. LLAVA requires GPU acceleration.\")\n",
    "#     exit(1)\n",
    "\n",
    "# print(\"=== LLAVA OCR Processing ===\")\n",
    "# print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "# print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# def process_images_with_llava(images_folder=\"part_2_images\", output_folder=\"extracted_text/llava\"):\n",
    "#     \"\"\"\n",
    "#     Process images using LLAVA model for OCR with GPU acceleration\n",
    "#     \"\"\"\n",
    "#     # Create output directory if it doesn't exist\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     print(f\"Output folder: {output_folder}\")\n",
    "\n",
    "#     # # Clear GPU memory before starting\n",
    "#     # torch.cuda.empty_cache()\n",
    "#     # gc.collect()\n",
    "\n",
    "#     # Initialize LLAVA model and processor\n",
    "#     print(\"Loading LLAVA model...\")\n",
    "#     try:\n",
    "#         # TODO: Replace with your specific LLAVA model path/name\n",
    "#         model_name = \"llava-hf/llava-1.5-7b-hf\"  # Example model name\n",
    "\n",
    "#         processor = AutoProcessor.from_pretrained(model_name, use_fast=True)\n",
    "#         model = LlavaForConditionalGeneration.from_pretrained(\n",
    "#             model_name,\n",
    "#             torch_dtype=torch.float16,  # Use fp16 for RTX 2060 efficiency\n",
    "#             device_map=\"cuda\",\n",
    "#             low_cpu_mem_usage=True\n",
    "#         )\n",
    "#         model.eval()\n",
    "#         print(\"LLAVA model loaded successfully!\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading LLAVA model: {e}\")\n",
    "#         print(\"Please ensure you have the correct model name/path\")\n",
    "#         return\n",
    "\n",
    "#     # Get all image files\n",
    "#     image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "#     image_files = []\n",
    "\n",
    "#     for ext in image_extensions:\n",
    "#         image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "#         image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "\n",
    "#     if not image_files:\n",
    "#         print(f\"No images found in {images_folder}\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Found {len(image_files)} images to process\")\n",
    "\n",
    "#     # Process each image\n",
    "#     processed_count = 0\n",
    "#     skipped_count = 0\n",
    "\n",
    "#     for i, image_path in enumerate(image_files, 1):\n",
    "#         image_filename = os.path.basename(image_path)\n",
    "#         base_name = os.path.splitext(image_filename)[0]\n",
    "#         text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "\n",
    "#         # Check if text file already exists (efficiency check)\n",
    "#         if os.path.exists(text_file_path):\n",
    "#             print(f\"\\n[{i}/{len(image_files)}] Skipping {image_filename} - already processed\")\n",
    "#             skipped_count += 1\n",
    "#             continue\n",
    "\n",
    "#         print(f\"\\n[{i}/{len(image_files)}] Processing {image_filename}...\")\n",
    "\n",
    "#         try:\n",
    "#             # Load and preprocess image\n",
    "#             image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "#             # Resize large images to optimize GPU memory\n",
    "#             max_dim = 1024  # Adjust based on your GPU memory\n",
    "#             if max(image.size) > max_dim:\n",
    "#                 image.thumbnail((max_dim, max_dim), Image.Resampling.LANCZOS)\n",
    "#                 print(f\"Resized image to {image.size} for GPU optimization\")\n",
    "\n",
    "#             # Prepare prompt for OCR task\n",
    "#             prompt = \"USER: <image>\\nExtract all text from this image. Provide only the extracted text without any additional commentary.\\nASSISTANT:\"\n",
    "\n",
    "#             # Process with LLAVA\n",
    "#             start_time = time.time()\n",
    "\n",
    "#             # TODO: Adjust this section based on your specific LLAVA implementation\n",
    "#             inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "#             # Generate text with GPU\n",
    "#             with torch.no_grad():\n",
    "#                 generated_ids = model.generate(\n",
    "#                     **inputs,\n",
    "#                     max_new_tokens=1024,\n",
    "#                     temperature=0.1,  # Low temperature for more accurate OCR\n",
    "#                     do_sample=False,\n",
    "#                     use_cache=True\n",
    "#                 )\n",
    "\n",
    "#             # Decode the generated text\n",
    "#             extracted_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "#             # Remove the prompt from the output\n",
    "#             if \"ASSISTANT:\" in extracted_text:\n",
    "#                 extracted_text = extracted_text.split(\"ASSISTANT:\")[-1].strip()\n",
    "\n",
    "#             end_time = time.time()\n",
    "#             print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "#             if not extracted_text.strip():\n",
    "#                 print(\"No text detected in this image\")\n",
    "#                 # Save empty file to avoid reprocessing\n",
    "#                 with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                     f.write(\"\")\n",
    "#                 continue\n",
    "\n",
    "#             # Save extracted text\n",
    "#             with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(extracted_text)\n",
    "\n",
    "#             print(f\"Saved text to: {text_file_path}\")\n",
    "#             print(f\"Text preview: {extracted_text[:100]}...\" if len(extracted_text) > 100 else f\"Text: {extracted_text}\")\n",
    "\n",
    "#             processed_count += 1\n",
    "\n",
    "#             # Clear GPU memory after each image\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {image_filename}: {e}\")\n",
    "#             # Save error file to avoid reprocessing\n",
    "#             with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(f\"ERROR: {str(e)}\")\n",
    "#             torch.cuda.empty_cache()\n",
    "#             continue\n",
    "\n",
    "#     # Final cleanup\n",
    "#     del model\n",
    "#     del processor\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "#     print(f\"\\n=== LLAVA Processing Complete ===\")\n",
    "#     print(f\"Processed: {processed_count} images\")\n",
    "#     print(f\"Skipped: {skipped_count} images (already processed)\")\n",
    "#     print(f\"Total: {len(image_files)} images\")\n",
    "\n",
    "# # Run LLAVA OCR processing\n",
    "# if __name__ == \"__main__\":\n",
    "#     process_images_with_llava()\n",
    "\n",
    "\n",
    "# LLAVA OCR Cell - GPU Accelerated Implementation\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Force GPU usage\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ['HF_HOME'] = 'D:/HuggingFaceCache'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(\"=== LLAVA OCR Processing ===\")\n",
    "if use_gpu:\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\n",
    "        f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\"\n",
    "    )\n",
    "    print(\"Using GPU acceleration\")\n",
    "else:\n",
    "    print(\"GPU not available - using CPU (will be slower)\")\n",
    "    print(\"For better performance, install CUDA-compatible PyTorch\")\n",
    "\n",
    "\n",
    "def process_images_with_llava(\n",
    "    images_folder=\"part_2_images\", output_folder=\"extracted_text/llava\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Process images using LLAVA model for OCR with GPU acceleration\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "\n",
    "    # Clear GPU memory before starting (only if GPU available)\n",
    "    if use_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Initialize LLAVA model and processor\n",
    "    print(\"Loading LLAVA model...\")\n",
    "    try:\n",
    "        # Updated model name - use the correct LLAVA model\n",
    "        model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "        print(\"Downloading/Loading processor...\")\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_name, use_fast=False, trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        print(\"Downloading/Loading model...\")\n",
    "        model = LlavaForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16\n",
    "            if use_gpu\n",
    "            else torch.float32,  # Use fp16 only if GPU available\n",
    "            device_map=\"auto\"\n",
    "            if use_gpu\n",
    "            else None,  # Use auto device mapping only if GPU available\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Move model to appropriate device\n",
    "        if use_gpu and not next(model.parameters()).is_cuda:\n",
    "            model = model.to(device)\n",
    "        elif not use_gpu:\n",
    "            model = model.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        print(\"LLAVA model loaded successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LLAVA model: {e}\")\n",
    "        print(\"Trying alternative approach...\")\n",
    "\n",
    "        # Alternative: Try different model or installation\n",
    "        try:\n",
    "            # Alternative model names to try\n",
    "            alternative_models = [\n",
    "                \"llava-hf/llava-1.5-13b-hf\",\n",
    "                \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "                \"llava-hf/llava-v1.6-vicuna-7b-hf\",\n",
    "            ]\n",
    "\n",
    "            for alt_model in alternative_models:\n",
    "                try:\n",
    "                    print(f\"Trying {alt_model}...\")\n",
    "                    processor = AutoProcessor.from_pretrained(\n",
    "                        alt_model, trust_remote_code=True\n",
    "                    )\n",
    "                    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "                        alt_model,\n",
    "                        torch_dtype=torch.float16 if use_gpu else torch.float32,\n",
    "                        device_map=\"auto\" if use_gpu else None,\n",
    "                        low_cpu_mem_usage=True,\n",
    "                        trust_remote_code=True,\n",
    "                    )\n",
    "                    if use_gpu and not next(model.parameters()).is_cuda:\n",
    "                        model = model.to(device)\n",
    "                    elif not use_gpu:\n",
    "                        model = model.to(device)\n",
    "                    model.eval()\n",
    "                    print(f\"Successfully loaded {alt_model}!\")\n",
    "                    model_name = alt_model\n",
    "                    break\n",
    "                except Exception as alt_e:\n",
    "                    print(f\"Failed to load {alt_model}: {alt_e}\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(\"All model loading attempts failed.\")\n",
    "                print(\"Please ensure you have installed the requirements:\")\n",
    "                print(\"pip install transformers torch torchvision accelerate\")\n",
    "                print(\"pip install git+https://github.com/huggingface/transformers.git\")\n",
    "                return\n",
    "\n",
    "        except Exception as final_e:\n",
    "            print(f\"Final error: {final_e}\")\n",
    "            return\n",
    "\n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "\n",
    "    # Process each image\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "\n",
    "        # Check if text file already exists (efficiency check)\n",
    "        if os.path.exists(text_file_path):\n",
    "            print(\n",
    "                f\"\\n[{i}/{len(image_files)}] Skipping {image_filename} - already processed\"\n",
    "            )\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[{i}/{len(image_files)}] Processing {image_filename}...\")\n",
    "\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "            # Resize large images to optimize memory\n",
    "            max_dim = 1024 if use_gpu else 512  # Smaller images for CPU processing\n",
    "            if max(image.size) > max_dim:\n",
    "                image.thumbnail((max_dim, max_dim), Image.Resampling.LANCZOS)\n",
    "                print(\n",
    "                    f\"Resized image to {image.size} for {'GPU' if use_gpu else 'CPU'} optimization\"\n",
    "                )\n",
    "\n",
    "            # Prepare prompt for OCR task - Updated format\n",
    "            prompt = \"USER: <image>\\nExtract all text from this image. Provide only the extracted text without any additional commentary.\\nASSISTANT:\"\n",
    "\n",
    "            # Process with LLAVA\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Updated processing approach\n",
    "            inputs = processor(prompt, image, return_tensors=\"pt\")\n",
    "\n",
    "            # Move inputs to appropriate device\n",
    "            if use_gpu:\n",
    "                inputs = {\n",
    "                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                    for k, v in inputs.items()\n",
    "                }\n",
    "            else:\n",
    "                inputs = {\n",
    "                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                    for k, v in inputs.items()\n",
    "                }\n",
    "\n",
    "            # Generate text with appropriate device\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=1024 if use_gpu else 512,  # Reduce tokens for CPU\n",
    "                    temperature=0.1,  # Low temperature for more accurate OCR\n",
    "                    do_sample=False,\n",
    "                    use_cache=True,\n",
    "                    pad_token_id=processor.tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            # Decode the generated text\n",
    "            generated_text = processor.decode(\n",
    "                generated_ids[0], skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Clean up the output - remove prompt\n",
    "            if \"ASSISTANT:\" in generated_text:\n",
    "                extracted_text = generated_text.split(\"ASSISTANT:\")[-1].strip()\n",
    "            else:\n",
    "                extracted_text = generated_text.strip()\n",
    "\n",
    "            # Remove any remaining prompt artifacts\n",
    "            if \"USER:\" in extracted_text:\n",
    "                extracted_text = extracted_text.split(\"USER:\")[-1].strip()\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                # Save empty file to avoid reprocessing\n",
    "                with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(\"\")\n",
    "                continue\n",
    "\n",
    "            # Save extracted text\n",
    "            with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(extracted_text)\n",
    "\n",
    "            print(f\"Saved text to: {text_file_path}\")\n",
    "            print(\n",
    "                f\"Text preview: {extracted_text[:100]}...\"\n",
    "                if len(extracted_text) > 100\n",
    "                else f\"Text: {extracted_text}\"\n",
    "            )\n",
    "\n",
    "            processed_count += 1\n",
    "\n",
    "            # Clear memory after each image (only if GPU available)\n",
    "            if use_gpu:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_filename}: {e}\")\n",
    "            # Save error file to avoid reprocessing\n",
    "            with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"ERROR: {str(e)}\")\n",
    "            if use_gpu:\n",
    "                torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "    # Final cleanup\n",
    "    del model\n",
    "    del processor\n",
    "    if use_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"\\n=== LLAVA Processing Complete ===\")\n",
    "    print(f\"Device used: {'GPU' if use_gpu else 'CPU'}\")\n",
    "    print(f\"Processed: {processed_count} images\")\n",
    "    print(f\"Skipped: {skipped_count} images (already processed)\")\n",
    "    print(f\"Total: {len(image_files)} images\")\n",
    "\n",
    "\n",
    "# Install requirements function\n",
    "def install_requirements():\n",
    "    \"\"\"Install required packages if not already installed\"\"\"\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    required_packages = [\n",
    "        \"transformers>=4.36.0\",\n",
    "        \"torch\",\n",
    "        \"torchvision\",\n",
    "        \"accelerate\",\n",
    "        \"pillow\",\n",
    "        \"bitsandbytes\",  # For efficient loading\n",
    "    ]\n",
    "\n",
    "    print(\"Installing/updating required packages...\")\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {package}: {e}\")\n",
    "\n",
    "    # Install latest transformers from git for LLAVA support\n",
    "    try:\n",
    "        subprocess.check_call(\n",
    "            [\n",
    "                sys.executable,\n",
    "                \"-m\",\n",
    "                \"pip\",\n",
    "                \"install\",\n",
    "                \"git+https://github.com/huggingface/transformers.git\",\n",
    "            ]\n",
    "        )\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install transformers from git: {e}\")\n",
    "\n",
    "\n",
    "# Run LLAVA OCR processing\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment the line below if you need to install requirements\n",
    "    # install_requirements()\n",
    "\n",
    "    process_images_with_llava()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03004154",
   "metadata": {},
   "source": [
    "# Shikra Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3bec15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osama/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.auto.processing_auto because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/transformers/utils/import_utils.py:1863\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1862\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1863\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1864\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:995\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/transformers/models/auto/processing_auto.py:28\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeatureExtractionMixin\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_processing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageProcessingMixin\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProcessorMixin\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/transformers/image_processing_utils.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_processing_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchFeature, ImageProcessingMixin\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension, get_image_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/transformers/image_transforms.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     ChannelDimension,\n\u001b[32m     24\u001b[39m     ImageInput,\n\u001b[32m     25\u001b[39m     get_channel_dimension_axis,\n\u001b[32m     26\u001b[39m     get_image_size,\n\u001b[32m     27\u001b[39m     infer_channel_dimension_format,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExplicitEnum, TensorType, is_jax_tensor, is_tf_tensor, is_torch_tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/transformers/image_utils.py:65\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torchvision_available():\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m io \u001b[38;5;28;01mas\u001b[39;00m torchvision_io\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InterpolationMode\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/torchvision/__init__.py:10\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/torchvision/_meta_registrations.py:163\u001b[39m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grad.new_empty((batch_size, channels, height, width))\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;129;43m@torch\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorchvision::nms\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mmeta_nms\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mboxes should be a 2d tensor, got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43mD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/torch/library.py:654\u001b[39m, in \u001b[36mregister_fake.<locals>.register\u001b[39m\u001b[34m(func)\u001b[39m\n\u001b[32m    653\u001b[39m     use_lib = lib\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m \u001b[43muse_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_register_fake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/torch/library.py:154\u001b[39m, in \u001b[36mLibrary._register_fake\u001b[39m\u001b[34m(self, op_name, fn, _stacklevel)\u001b[39m\n\u001b[32m    152\u001b[39m     func_to_register = fn\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m handle = \u001b[43mentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabstract_impl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc_to_register\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28mself\u001b[39m._registration_handles.append(handle)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/torch/_library/abstract_impl.py:31\u001b[39m, in \u001b[36mAbstractImplHolder.register\u001b[39m\u001b[34m(self, func, source)\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     27\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.qualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33malready has an fake impl registered at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.kernel.source\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dispatch_has_kernel_for_dispatch_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqualname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     33\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mregister_fake(...): the operator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.qualname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33malready has an DispatchKey::Meta implementation via a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mregister_fake.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     38\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: operator torchvision::nms does not exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 229\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor, AutoModelForVision2Seq\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/transformers/utils/import_utils.py:1852\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1850\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m   1851\u001b[39m     module = \u001b[38;5;28mself\u001b[39m._get_module(\u001b[38;5;28mself\u001b[39m._class_to_module[name])\n\u001b[32m-> \u001b[39m\u001b[32m1852\u001b[39m     value = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m   1854\u001b[39m     value = \u001b[38;5;28mself\u001b[39m._get_module(name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/transformers/utils/import_utils.py:1851\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1849\u001b[39m     value = Placeholder\n\u001b[32m   1850\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1851\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1852\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1853\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Apziva Projects/Project 4/Mon-Reader/env/lib/python3.12/site-packages/transformers/utils/import_utils.py:1865\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1863\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   1864\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1865\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1866\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1867\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1868\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.models.auto.processing_auto because of the following error (look up to see its traceback):\noperator torchvision::nms does not exist"
     ]
    }
   ],
   "source": [
    "# # SHIKRA OCR Cell - GPU Accelerated Implementation\n",
    "# import os\n",
    "# import glob\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# import time\n",
    "# import gc\n",
    "# from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# # Force GPU usage\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# # Check GPU availability\n",
    "# if not torch.cuda.is_available():\n",
    "#     print(\"ERROR: CUDA not available. SHIKRA requires GPU acceleration.\")\n",
    "#     exit(1)\n",
    "\n",
    "# print(\"=== SHIKRA OCR Processing ===\")\n",
    "# print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "# print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# def process_images_with_shikra(images_folder=\"part_2_images\", output_folder=\"extracted_text/shikra\"):\n",
    "#     \"\"\"\n",
    "#     Process images using SHIKRA model for OCR with GPU acceleration\n",
    "#     \"\"\"\n",
    "#     # Create output directory if it doesn't exist\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "#     # Clear GPU memory before starting\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "    \n",
    "#     # Initialize SHIKRA model and processor\n",
    "#     print(\"Loading SHIKRA model...\")\n",
    "#     try:\n",
    "#         # TODO: Replace with your specific SHIKRA model path/name\n",
    "#         model_name = \"shikras/shikra-7b-delta-v1\"  # Example model name\n",
    "        \n",
    "#         processor = AutoProcessor.from_pretrained(model_name, use_fast=False)\n",
    "#         model = AutoModelForVision2Seq.from_pretrained(\n",
    "#             model_name,\n",
    "#             torch_dtype=torch.float16,  # Use fp16 for RTX 2060 efficiency\n",
    "#             device_map=\"cuda\",\n",
    "#             low_cpu_mem_usage=True\n",
    "#         )\n",
    "#         model.eval()\n",
    "#         print(\"SHIKRA model loaded successfully!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading SHIKRA model: {e}\")\n",
    "#         print(\"Please ensure you have the correct model name/path\")\n",
    "#         return\n",
    "    \n",
    "#     # Get all image files\n",
    "#     image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "#     image_files = []\n",
    "    \n",
    "#     for ext in image_extensions:\n",
    "#         image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "#         image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "#     if not image_files:\n",
    "#         print(f\"No images found in {images_folder}\")\n",
    "#         return\n",
    "    \n",
    "#     print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "#     # Process each image\n",
    "#     processed_count = 0\n",
    "#     skipped_count = 0\n",
    "    \n",
    "#     for i, image_path in enumerate(image_files, 1):\n",
    "#         image_filename = os.path.basename(image_path)\n",
    "#         base_name = os.path.splitext(image_filename)[0]\n",
    "#         text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "#         # Check if text file already exists (efficiency check)\n",
    "#         if os.path.exists(text_file_path):\n",
    "#             print(f\"\\n[{i}/{len(image_files)}] Skipping {image_filename} - already processed\")\n",
    "#             skipped_count += 1\n",
    "#             continue\n",
    "        \n",
    "#         print(f\"\\n[{i}/{len(image_files)}] Processing {image_filename}...\")\n",
    "        \n",
    "#         try:\n",
    "#             # Load and preprocess image\n",
    "#             image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "#             # Resize large images to optimize GPU memory\n",
    "#             max_dim = 1024  # Adjust based on your GPU memory\n",
    "#             if max(image.size) > max_dim:\n",
    "#                 image.thumbnail((max_dim, max_dim), Image.Resampling.LANCZOS)\n",
    "#                 print(f\"Resized image to {image.size} for GPU optimization\")\n",
    "            \n",
    "#             # Prepare prompt for OCR task\n",
    "#             # SHIKRA might use a different prompt format - adjust as needed\n",
    "#             prompt = \"<image> Extract and transcribe all text visible in this image.\"\n",
    "            \n",
    "#             # Process with SHIKRA\n",
    "#             start_time = time.time()\n",
    "            \n",
    "#             # TODO: Adjust this section based on your specific SHIKRA implementation\n",
    "#             inputs = processor(\n",
    "#                 text=prompt,\n",
    "#                 images=image,\n",
    "#                 return_tensors=\"pt\"\n",
    "#             ).to(\"cuda\")\n",
    "            \n",
    "#             # Generate text with GPU\n",
    "#             with torch.no_grad():\n",
    "#                 generated_ids = model.generate(\n",
    "#                     **inputs,\n",
    "#                     max_new_tokens=1024,\n",
    "#                     temperature=0.1,  # Low temperature for more accurate OCR\n",
    "#                     do_sample=False,\n",
    "#                     num_beams=1,  # Adjust based on accuracy vs speed tradeoff\n",
    "#                     use_cache=True\n",
    "#                 )\n",
    "            \n",
    "#             # Decode the generated text\n",
    "#             extracted_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "#             # Clean up the output (remove prompt if included)\n",
    "#             if prompt in extracted_text:\n",
    "#                 extracted_text = extracted_text.replace(prompt, \"\").strip()\n",
    "            \n",
    "#             end_time = time.time()\n",
    "#             print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "            \n",
    "#             if not extracted_text.strip():\n",
    "#                 print(\"No text detected in this image\")\n",
    "#                 # Save empty file to avoid reprocessing\n",
    "#                 with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                     f.write(\"\")\n",
    "#                 continue\n",
    "            \n",
    "#             # Save extracted text\n",
    "#             with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(extracted_text)\n",
    "            \n",
    "#             print(f\"Saved text to: {text_file_path}\")\n",
    "#             print(f\"Text preview: {extracted_text[:100]}...\" if len(extracted_text) > 100 else f\"Text: {extracted_text}\")\n",
    "            \n",
    "#             processed_count += 1\n",
    "            \n",
    "#             # Clear GPU memory after each image\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {image_filename}: {e}\")\n",
    "#             # Save error file to avoid reprocessing\n",
    "#             with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(f\"ERROR: {str(e)}\")\n",
    "#             torch.cuda.empty_cache()\n",
    "#             continue\n",
    "    \n",
    "#     # Final cleanup\n",
    "#     del model\n",
    "#     del processor\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "    \n",
    "#     print(f\"\\n=== SHIKRA Processing Complete ===\")\n",
    "#     print(f\"Processed: {processed_count} images\")\n",
    "#     print(f\"Skipped: {skipped_count} images (already processed)\")\n",
    "#     print(f\"Total: {len(image_files)} images\")\n",
    "\n",
    "# # Run SHIKRA OCR processing\n",
    "# if __name__ == \"__main__\":\n",
    "#     process_images_with_shikra()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "\n",
    "# # Initialize model and processor from Hugging Face\n",
    "# model_name = \"shikras/shikra-7b-delta-v1-0708\"\n",
    "# processor = AutoProcessor.from_pretrained(model_name)\n",
    "# model = AutoModelForVision2Seq.from_pretrained(model_name)\n",
    "\n",
    "# # Path to the folder containing images\n",
    "# image_folder = 'part_2_images'\n",
    "\n",
    "# # Function to process and extract text from images\n",
    "# def extract_text_from_images(image_folder):\n",
    "#     # List all image files in the folder\n",
    "#     image_files = [f for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "#     extracted_text = {}\n",
    "\n",
    "#     for img_file in image_files:\n",
    "#         img_path = os.path.join(image_folder, img_file)\n",
    "\n",
    "#         # Open image\n",
    "#         image = Image.open(img_path)\n",
    "\n",
    "#         # Preprocess the image and make predictions\n",
    "#         inputs = processor(images=image, return_tensors=\"pt\")\n",
    "#         pixel_values = inputs[\"pixel_values\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "#         # Get predictions from the model\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.generate(pixel_values=pixel_values)\n",
    "        \n",
    "#         # Decode the generated output (i.e., text)\n",
    "#         extracted_text[img_file] = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#         print(f\"Extracted text from {img_file}:\")\n",
    "#         print(extracted_text[img_file])\n",
    "#         print('-' * 50)\n",
    "\n",
    "#     # Save extracted text to a file\n",
    "#     with open(\"extracted_text.txt\", \"w\") as f:\n",
    "#         for img_file, text in extracted_text.items():\n",
    "#             f.write(f\"Text from {img_file}:\\n{text}\\n\\n\")\n",
    "\n",
    "# # Call the function\n",
    "# extract_text_from_images(image_folder)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get variables from the .env file\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\")\n",
    "IMAGE_FOLDER = os.getenv(\"IMAGE_FOLDER\")\n",
    "OUTPUT_FILE = os.getenv(\"OUTPUT_FILE\")\n",
    "\n",
    "# Check if variables are loaded correctly\n",
    "if not MODEL_NAME or not IMAGE_FOLDER or not OUTPUT_FILE:\n",
    "    raise ValueError(\"Missing one or more environment variables.\")\n",
    "\n",
    "# Initialize model and processor from Hugging Face\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForVision2Seq.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Function to process and extract text from images\n",
    "def extract_text_from_images(image_folder):\n",
    "    image_files = [f for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    extracted_text = {}\n",
    "\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        # Preprocess the image\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "        # Get predictions from the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(pixel_values=pixel_values)\n",
    "\n",
    "        # Decode the generated output (i.e., text)\n",
    "        extracted_text[img_file] = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"Extracted text from {img_file}:\")\n",
    "        print(extracted_text[img_file])\n",
    "        print('-' * 50)\n",
    "\n",
    "    # Save extracted text to a file\n",
    "    with open(OUTPUT_FILE, \"w\") as f:\n",
    "        for img_file, text in extracted_text.items():\n",
    "            f.write(f\"Text from {img_file}:\\n{text}\\n\\n\")\n",
    "\n",
    "# Call the function\n",
    "extract_text_from_images(IMAGE_FOLDER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc8c7b",
   "metadata": {},
   "source": [
    "# Hugging face version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "x5vxmvv74qb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "# Load the model in half-precision\n",
    "print(\"Loading LLAVA model for batch processing...\")\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"liuhaotian/llava-v1.6-vicuna-7b\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", use_fast=False)\n",
    "\n",
    "def process_images_batch(images_folder=\"part_2_images\", output_folder=\"extracted_text/llava_batch\", batch_size=2):\n",
    "    \"\"\"\n",
    "    Process images from folder using batch processing with LLAVA\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process in batches of {batch_size}\")\n",
    "    \n",
    "    # Process images in batches\n",
    "    for i in range(0, len(image_files), batch_size):\n",
    "        batch_files = image_files[i:i + batch_size]\n",
    "        print(f\"\\nProcessing batch {i//batch_size + 1}: {len(batch_files)} images\")\n",
    "        \n",
    "        # Prepare conversations for this batch\n",
    "        conversations = []\n",
    "        valid_files = []\n",
    "        \n",
    "        for image_path in batch_files:\n",
    "            try:\n",
    "                # Load and validate image\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                \n",
    "                # Create conversation for this image\n",
    "                conversation = [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"image\", \"image\": image},\n",
    "                            {\"type\": \"text\", \"text\": \"Extract all text from this image. Provide only the extracted text without any additional commentary.\"},\n",
    "                        ],\n",
    "                    },\n",
    "                ]\n",
    "                \n",
    "                conversations.append(conversation)\n",
    "                valid_files.append(image_path)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {image_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not conversations:\n",
    "            print(\"No valid images in this batch, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Process batch\n",
    "            print(f\"Processing {len(conversations)} images...\")\n",
    "            \n",
    "            inputs = processor.apply_chat_template(\n",
    "                conversations,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_dict=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device, torch.float16)\n",
    "            \n",
    "            # Generate text for all images in batch\n",
    "            with torch.no_grad():\n",
    "                generate_ids = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
    "            \n",
    "            # Decode results\n",
    "            results = processor.batch_decode(generate_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Save results for each image\n",
    "            for j, (image_path, result) in enumerate(zip(valid_files, results)):\n",
    "                filename = os.path.basename(image_path)\n",
    "                base_name = os.path.splitext(filename)[0]\n",
    "                text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "                \n",
    "                # Clean up the result text\n",
    "                if \"assistant\" in result.lower():\n",
    "                    # Find the last occurrence of assistant and take text after it\n",
    "                    parts = result.lower().split(\"assistant\")\n",
    "                    if len(parts) > 1:\n",
    "                        extracted_text = result[result.lower().rfind(\"assistant\") + len(\"assistant\"):].strip()\n",
    "                    else:\n",
    "                        extracted_text = result.strip()\n",
    "                else:\n",
    "                    extracted_text = result.strip()\n",
    "                \n",
    "                # Remove any remaining artifacts\n",
    "                lines = extracted_text.split('\\n')\n",
    "                cleaned_lines = []\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    if line and not line.lower().startswith(('user:', 'assistant:', 'extract all text')):\n",
    "                        cleaned_lines.append(line)\n",
    "                \n",
    "                final_text = '\\n'.join(cleaned_lines).strip()\n",
    "                \n",
    "                # Save to file\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(final_text)\n",
    "                \n",
    "                print(f\"Saved: {filename} -> {text_file_path}\")\n",
    "                if final_text:\n",
    "                    preview = final_text[:100] + \"...\" if len(final_text) > 100 else final_text\n",
    "                    print(f\"Preview: {preview}\")\n",
    "                else:\n",
    "                    print(\"No text extracted\")\n",
    "            \n",
    "            # Clear GPU memory after each batch\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "            # Save error files for this batch\n",
    "            for image_path in valid_files:\n",
    "                filename = os.path.basename(image_path)\n",
    "                base_name = os.path.splitext(filename)[0]\n",
    "                text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"ERROR: {str(e)}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n=== Batch Processing Complete ===\")\n",
    "    print(f\"Results saved to: {output_folder}\")\n",
    "\n",
    "# Run the batch processing\n",
    "if __name__ == \"__main__\":\n",
    "    process_images_batch(batch_size=2)  # Adjust batch_size based on your GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0456593c",
   "metadata": {},
   "source": [
    "# LLava via Olama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edc7b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 images to process\n",
      "[1/3] Processing: IMG_20250629_214514_439.jpg\n",
      "‚úÖ Saved: extracted_text/ollama_direct/IMG_20250629_214514_439.txt\n",
      "üìù Text: The image is blurry and rotated, making it difficult to read the text with certainty. However, I wil...\n",
      "[2/3] Processing: IMG_20250629_214324_528.jpg\n",
      "‚úÖ Saved: extracted_text/ollama_direct/IMG_20250629_214324_528.txt\n",
      "üìù Text: The image shows a page with handwritten notes, and it appears to be a personal study or workbook. He...\n",
      "[3/3] Processing: textbook_img.jpg\n",
      "‚úÖ Saved: extracted_text/ollama_direct/textbook_img.txt\n",
      "üìù Text: The text in the image reads:\n",
      "\n",
      "\"Episode 25: When the Moon Splits\n",
      "\n",
      "Synopsis:\n",
      "\n",
      "The war for Ryloth rages...\n",
      "\n",
      "üéâ OCR Complete! Successfully processed 3/3 images\n",
      "üìÅ Results saved to: extracted_text/ollama_direct\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import ollama\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def direct_ollama_llava_ocr(\n",
    "    images_folder=\"part_2_images\", output_folder=\"extracted_text/ollama_direct\"\n",
    "):\n",
    "    \"\"\"Process images with Ollama LLaVA directly (no HTTP requests)\"\"\"\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "\n",
    "    successful = 0\n",
    "\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "\n",
    "        print(f\"[{i}/{len(image_files)}] Processing: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # Use Ollama directly with the image file\n",
    "            response = ollama.generate(\n",
    "                model=\"llava\",\n",
    "                prompt=\"Extract all text from this image. Provide only the extracted text without any commentary.\",\n",
    "                images=[image_path],\n",
    "            )\n",
    "\n",
    "            extracted_text = response[\"response\"].strip()\n",
    "\n",
    "            # Save extracted text\n",
    "            with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(extracted_text)\n",
    "\n",
    "            print(f\"‚úÖ Saved: {text_file_path}\")\n",
    "            if extracted_text:\n",
    "                preview = (\n",
    "                    extracted_text[:100] + \"...\"\n",
    "                    if len(extracted_text) > 100\n",
    "                    else extracted_text\n",
    "                )\n",
    "                print(f\"üìù Text: {preview}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No text extracted\")\n",
    "\n",
    "            successful += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "            with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"ERROR: {str(e)}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nüéâ OCR Complete! Successfully processed {successful}/{len(image_files)} images\"\n",
    "    )\n",
    "    print(f\"üìÅ Results saved to: {output_folder}\")\n",
    "\n",
    "\n",
    "# Run the OCR processing\n",
    "if __name__ == \"__main__\":\n",
    "    direct_ollama_llava_ocr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ebf917",
   "metadata": {},
   "source": [
    "# Gemini Api Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d89b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! This is an excellent question, as Gemini's multimodal capabilities make it a powerful, but different, kind of tool for OCR.\n",
      "\n",
      "Here‚Äôs a breakdown of which Gemini model to use and how it compares to traditional OCR services.\n",
      "\n",
      "### The Short Answer: The Model to Use\n",
      "\n",
      "For OCR tasks, you should use the Gemini models with **vision capabilities**. As of now, your primary choices are:\n",
      "\n",
      "1.  **Gemini 1.5 Pro:** **This is the recommended choice.** It's the latest and most capable publicly available model. It has a massive context window (up to 1 million tokens), which means it can analyze very large documents, multiple documents at once, or even frames from a video for text. Its reasoning and accuracy are state-of-the-art.\n",
      "2.  **Gemini Pro Vision:** This was the first widely available multimodal model in the Gemini family. It is still very powerful, reliable, and a great choice for most standard OCR tasks. If `1.5 Pro` is not available to you or seems like overkill, this is your go-to model.\n",
      "\n",
      "You would **not** use `Gemini Pro`, as it is a text-only model and cannot process images.\n",
      "\n",
      "---\n",
      "\n",
      "### How Gemini Performs OCR (It's More Than Just OCR)\n",
      "\n",
      "It's crucial to understand that Gemini is not a dedicated OCR engine like Google's Cloud Vision API (Text Detection feature) or Tesseract.\n",
      "\n",
      "*   **Traditional OCR:** Scans an image pixel by pixel to identify characters and words. Its primary output is the raw text, often with coordinates (bounding boxes).\n",
      "*   **Gemini (Multimodal AI):** *Understands* the image contextually. It sees the image, reads the text, and can reason about the content. This means you can ask it to do much more than just extract text.\n",
      "\n",
      "This \"OCR+\" capability is Gemini's biggest strength.\n",
      "\n",
      "#### Use Cases where Gemini Excels:\n",
      "\n",
      "*   **Structured Data Extraction:** Instead of just getting a wall of text, you can ask for formatted output.\n",
      "    *   **Prompt:** `\"From this invoice image, extract the invoice number, total amount, and due date, and return it as a JSON object.\"`\n",
      "*   **Handwriting and Noisy Images:** It's often much better at deciphering messy handwriting, stylized fonts, or text on crumpled or low-quality images because it uses context to guess characters.\n",
      "*   **Understanding Layout:** It can understand tables, forms, and columns without pre-processing.\n",
      "    *   **Prompt:** `\"Transcribe the text in this two-column document, keeping the columns separate.\"`\n",
      "*   **OCR + Other Tasks in One Step:** Combine OCR with analysis, translation, or summarization.\n",
      "    *   **Prompt:** `\"Extract the text from this restaurant menu, translate all the dish names to Spanish, and list the vegetarian options.\"`\n",
      "\n",
      "---\n",
      "\n",
      "### How to Use Gemini for OCR (Practical Steps)\n",
      "\n",
      "You can access Gemini Pro Vision / 1.5 Pro through two main platforms:\n",
      "\n",
      "#### 1. Google AI Studio (for Prototyping)\n",
      "\n",
      "This is the easiest way to get started and test your prompts.\n",
      "1.  Go to [Google AI Studio](https://aistudio.google.com/).\n",
      "2.  Create a new prompt.\n",
      "3.  Select the model (e.g., `Gemini 1.5 Pro`).\n",
      "4.  Click the \"Image\" button to upload your image file.\n",
      "5.  Write your prompt in the text box (e.g., `\"Extract all text from this image.\"`).\n",
      "6.  Run the prompt and see the result.\n",
      "\n",
      "#### 2. Vertex AI on Google Cloud (for Production Apps)\n",
      "\n",
      "This is the production-ready, scalable way to build applications.\n",
      "\n",
      "Here is a Python code example using the Vertex AI SDK:\n",
      "\n",
      "```python\n",
      "import vertexai\n",
      "from vertexai.generative_models import GenerativeModel, Part\n",
      "\n",
      "# Initialize Vertex AI\n",
      "# Make sure you have authenticated with 'gcloud auth application-default login'\n",
      "# and set your project ID.\n",
      "# vertexai.init(project=\"your-gcp-project-id\", location=\"us-central1\")\n",
      "\n",
      "def ocr_with_gemini(image_path: str, prompt: str):\n",
      "    \"\"\"\n",
      "    Performs OCR and other tasks on an image using Gemini 1.5 Pro.\n",
      "    \"\"\"\n",
      "    # Use gemini-1.5-pro-preview-0409 or the latest 1.5 model\n",
      "    # Or use gemini-1.0-pro-vision-001 for Gemini Pro Vision\n",
      "    model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n",
      "\n",
      "    # Load the image file\n",
      "    with open(image_path, \"rb\") as f:\n",
      "        image_data = f.read()\n",
      "\n",
      "    image_part = Part.from_data(data=image_data, mime_type=\"image/jpeg\")\n",
      "\n",
      "    # The prompt and image are sent together\n",
      "    response = model.generate_content([image_part, prompt])\n",
      "\n",
      "    return response.text\n",
      "\n",
      "# --- Example Usage ---\n",
      "\n",
      "# 1. Simple OCR\n",
      "simple_prompt = \"Extract all visible text from this image. Transcribe it exactly as it appears.\"\n",
      "extracted_text = ocr_with_gemini(\"path/to/your/receipt.jpg\", simple_prompt)\n",
      "print(\"--- Simple OCR Result ---\")\n",
      "print(extracted_text)\n",
      "\n",
      "# 2. Structured Data Extraction (Gemini's superpower)\n",
      "structured_prompt = \"\"\"\n",
      "Analyze this invoice image and extract the following information.\n",
      "Return the result as a clean JSON object:\n",
      "{\n",
      "  \"vendor_name\": \"string\",\n",
      "  \"invoice_id\": \"string\",\n",
      "  \"total_amount\": \"float\",\n",
      "  \"due_date\": \"YYYY-MM-DD\"\n",
      "}\n",
      "\"\"\"\n",
      "json_data = ocr_with_gemini(\"path/to/your/invoice.png\", structured_prompt)\n",
      "print(\"\\n--- Structured Data Result ---\")\n",
      "print(json_data)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Comparison: Gemini vs. Dedicated OCR (Google Cloud Vision API)\n",
      "\n",
      "| Feature / Use Case | Gemini 1.5 Pro / Pro Vision | Dedicated OCR (Cloud Vision API - Text Detection) |\n",
      "| :--- | :--- | :--- |\n",
      "| **Primary Task** | **Contextual understanding** of text within an image. | **High-precision text extraction**. |\n",
      "| **Best For** | Complex documents, forms, handwriting, structured data extraction (JSON), combined tasks (OCR + summary). | High-volume, simple, per-page text extraction. Scanned documents, license plates, signs. |\n",
      "| **Output** | Formatted text, JSON, summaries, answers to questions. | Raw text string and detailed bounding box coordinates for each word and line. |\n",
      "| **Prompting** | **Required.** You must tell the model what you want it to do with the text. | **Not required.** The API has a single purpose: detect text. |\n",
      "| **Speed & Cost** | Generally slower and can be more expensive per call. | Extremely fast and optimized for low cost-per-page at high volume. |\n",
      "\n",
      "### Conclusion: Which Should You Choose?\n",
      "\n",
      "*   **Choose Gemini 1.5 Pro** if your task involves more than just transcribing text. If you need to understand the layout, extract specific fields into a structured format like JSON, read handwriting, or analyze the content, Gemini is vastly superior.\n",
      "*   **Choose a dedicated OCR service (like Google Cloud Vision API)** if you have a high-volume pipeline that just needs fast, cheap, and accurate raw text from simple documents, and you need precise word-level bounding boxes.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY` automatically\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro\", contents=\"which gemini model to use for ocr?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da775898",
   "metadata": {},
   "source": [
    "# Gemini OCR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7695cdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing IMG_20250629_214514_439.jpg...\n",
      "‚úì Extracted text saved to gemini 2.5 pro/extracted_text/IMG_20250629_214514_439_extracted.txt\n",
      "Processing textbook_img.jpg...\n",
      "‚úì Extracted text saved to gemini 2.5 pro/extracted_text/textbook_img_extracted.txt\n",
      "Processing IMG_20250629_214324_528.jpg...\n",
      "‚úì Extracted text saved to gemini 2.5 pro/extracted_text/IMG_20250629_214324_528_extracted.txt\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "from pathlib import Path\n",
    "from google import genai\n",
    "\n",
    "def extract_text_from_images():\n",
    "    # Initialize the Gemini client\n",
    "    client = genai.Client()\n",
    "    \n",
    "    # Define paths\n",
    "    images_dir = Path(\"part_2_images\")\n",
    "    output_dir = Path(\"gemini 2.5 pro/extracted_text\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process each image file\n",
    "    for image_file in images_dir.glob(\"*.jpg\"):\n",
    "        print(f\"Processing {image_file.name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Read the image file\n",
    "            with open(image_file, \"rb\") as f:\n",
    "                image_data = f.read()\n",
    "            \n",
    "            # Create the prompt for OCR\n",
    "            import base64\n",
    "            image_b64 = base64.b64encode(image_data).decode()\n",
    "            \n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.5-pro\",\n",
    "                contents=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"parts\": [\n",
    "                            {\n",
    "                                \"text\": \"Extract all text from this image. Return only the text content, no additional commentary.\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"inline_data\": {\n",
    "                                    \"mime_type\": \"image/jpeg\",\n",
    "                                    \"data\": image_b64\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Save extracted text to file\n",
    "            output_file = output_dir / f\"{image_file.stem}_extracted.txt\"\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text)\n",
    "            \n",
    "            print(f\"‚úì Extracted text saved to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error processing {image_file.name}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_text_from_images()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7c9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
