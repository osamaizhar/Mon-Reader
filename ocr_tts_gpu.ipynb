{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env-report-2025",
   "metadata": {},
   "source": [
    "## Environment Report\n",
    "\n",
    "**Current Package Versions (Baseline before LLAVA/SHIKRA integration):**\n",
    "\n",
    "- **Python**: 3.13.3\n",
    "- **torch**: 2.7.1+cu128\n",
    "- **torchvision**: 0.22.1+cu128\n",
    "- **torchaudio**: 2.7.1+cu128\n",
    "- **easyocr**: 1.7.2\n",
    "- **opencv-python**: 4.12.0.88\n",
    "- **pillow**: 11.2.1\n",
    "- **numpy**: 2.2.6\n",
    "- **pyttsx3**: 2.99\n",
    "- **langdetect**: 1.0.9\n",
    "- **transformers**: 4.52.3\n",
    "- **gradio**: 5.31.0\n",
    "- **CUDA Available**: Yes\n",
    "- **CUDA Version**: 12.8\n",
    "- **GPU**: NVIDIA GeForce RTX 2060\n",
    "- **GPU Memory**: 12.88 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05407b00",
   "metadata": {},
   "source": [
    "# Imports and Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932d8684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLAVA OCR Processing ===\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "GPU Memory: 12.88 GB\n",
      "HuggingFace cache location: D:/HuggingFaceCache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import easyocr\n",
    "import pyttsx3\n",
    "from langdetect import detect\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "## Moving Hugging Face default Download Dir\n",
    "\n",
    "# # LLAVA OCR Cell - GPU Accelerated Implementation\n",
    "# import os\n",
    "\n",
    "# IMPORTANT: Set cache BEFORE any imports from transformers/huggingface\n",
    "os.environ['HF_HOME'] = 'D:/HuggingFaceCache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'D:/HuggingFaceCache/transformers'\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = 'D:/HuggingFaceCache/hub'\n",
    "\n",
    "# # Now import everything else\n",
    "# import glob\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# import time\n",
    "# import gc\n",
    "# from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "# Force GPU usage\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Check GPU availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: CUDA not available. LLAVA requires GPU acceleration.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"=== LLAVA OCR Processing ===\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"HuggingFace cache location: {os.environ.get('HF_HOME', 'default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8207ed",
   "metadata": {},
   "source": [
    "# Simple TTS using pyttsx3 Gpu based:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force GPU usage and optimization for RTX 2060\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU\n",
    "torch.backends.cudnn.benchmark = True  # Enable cudnn autotuner for performance\n",
    "\n",
    "# Check GPU and setup\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: CUDA not available. This script requires GPU acceleration.\")\n",
    "    print(\"Please check your NVIDIA drivers and PyTorch installation.\")\n",
    "    exit(1)\n",
    "\n",
    "# Display GPU information\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Initialize EasyOCR reader with explicit GPU settings\n",
    "print(\"Initializing EasyOCR with GPU acceleration...\")\n",
    "reader = easyocr.Reader(\n",
    "    [\"ar\", \"ur\", \"en\"], \n",
    "    gpu=True,\n",
    "    verbose=False,\n",
    "    # For RTX 2060, set reasonable batch size and model parameters\n",
    "    detector=True,\n",
    "    recognizer=True\n",
    ")\n",
    "\n",
    "# Initialize TTS engine\n",
    "tts_engine = pyttsx3.init()\n",
    "\n",
    "def log_gpu_memory():\n",
    "    \"\"\"Log current GPU memory usage\"\"\"\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    print(f\"GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "\n",
    "def process_images_ocr_save_text(images_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process all images in the folder with GPU-accelerated OCR and save to text files\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output folder: {output_folder}\")\n",
    "    \n",
    "    # Get all image files in the folder\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Report initial GPU memory\n",
    "    log_gpu_memory()\n",
    "\n",
    "    # Process files\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "        print(f\"\\n--- Processing Image {i}/{len(image_files)}: {image_filename} ---\")\n",
    "\n",
    "        try:\n",
    "            # Read image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Could not read image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Optimize image for GPU processing (resize large images)\n",
    "            h, w = image.shape[:2]\n",
    "            max_dim = 2000  # Optimal for RTX 2060 memory\n",
    "            if max(h, w) > max_dim:\n",
    "                scale = max_dim / max(h, w)\n",
    "                image = cv2.resize(image, (int(w * scale), int(h * scale)))\n",
    "                print(f\"Resized image to {image.shape[1]}x{image.shape[0]} to optimize GPU memory\")\n",
    "            \n",
    "            # Report GPU memory before OCR\n",
    "            log_gpu_memory()\n",
    "\n",
    "            # Extract text using GPU-accelerated EasyOCR\n",
    "            print(\"Extracting text with GPU acceleration...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # For RTX 2060, use appropriate batch size\n",
    "            results = reader.readtext(\n",
    "                image,\n",
    "                batch_size=2,  # Adjust based on your GPU memory\n",
    "                paragraph=True,  # Group text into paragraphs\n",
    "                detail=0  # 0 for more accuracy\n",
    "            )\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "            # Combine all detected text with confidence filtering\n",
    "            extracted_text = \" \".join(\n",
    "                [result[1] for result in results if result[2] > 0.5]\n",
    "            )  # confidence > 0.5\n",
    "\n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Extracted text: {extracted_text}\")\n",
    "            \n",
    "            # Try to detect language\n",
    "            try:\n",
    "                detected_lang = detect(extracted_text)\n",
    "                print(f\"Detected language: {detected_lang}\")\n",
    "                \n",
    "                # Save text with language information to file\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                    text_file.write(f\"LANG:{detected_lang}\\n\")\n",
    "                    text_file.write(extracted_text)\n",
    "                \n",
    "                print(f\"Saved text to: {text_file_path}\")\n",
    "                \n",
    "            except Exception as lang_error:\n",
    "                print(f\"Language detection failed: {lang_error}\")\n",
    "                # Save text without language information\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                    text_file.write(f\"LANG:unknown\\n\")\n",
    "                    text_file.write(extracted_text)\n",
    "                \n",
    "                print(f\"Saved text to: {text_file_path} (language unknown)\")\n",
    "\n",
    "            # Clear GPU memory after each image\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            # Clear GPU memory on error\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "    # Final GPU memory cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"\\n--- OCR Processing Complete ---\")\n",
    "    log_gpu_memory()\n",
    "    \n",
    "    return output_folder\n",
    "\n",
    "def read_text_files_aloud(text_folder):\n",
    "    \"\"\"\n",
    "    Read all text files in the folder aloud using TTS, one by one with clear separation\n",
    "    \"\"\"\n",
    "    # Get all text files\n",
    "    text_files = glob.glob(os.path.join(text_folder, \"*.txt\"))\n",
    "    \n",
    "    if not text_files:\n",
    "        print(f\"No text files found in {text_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(text_files)} text files to read\")\n",
    "    \n",
    "    # Language mapping for TTS\n",
    "    lang_mapping = {\n",
    "        \"en\": \"english\",\n",
    "        \"ar\": \"arabic\",\n",
    "        \"ur\": \"urdu\",\n",
    "        \"hi\": \"hindi\",\n",
    "        \"fa\": \"persian\",\n",
    "        \"ps\": \"pashto\"\n",
    "    }\n",
    "    \n",
    "    # Sort text files alphabetically to ensure consistent reading order\n",
    "    text_files.sort()\n",
    "    \n",
    "    for i, text_file_path in enumerate(text_files, 1):\n",
    "        file_name = os.path.basename(text_file_path)\n",
    "        print(f\"\\n===== Reading File {i}/{len(text_files)}: {file_name} =====\")\n",
    "        \n",
    "        # Announce the file being read (optional)\n",
    "        tts_engine.setProperty(\"rate\", 150)\n",
    "        announcement = f\"Reading file {i} of {len(text_files)}: {os.path.splitext(file_name)[0]}\"\n",
    "        print(announcement)\n",
    "        tts_engine.say(announcement)\n",
    "        tts_engine.runAndWait()\n",
    "        \n",
    "        # Pause between announcement and content\n",
    "        time.sleep(1)\n",
    "        \n",
    "        try:\n",
    "            # Read text file\n",
    "            with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                \n",
    "            if not lines:\n",
    "                print(f\"File is empty: {text_file_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Extract language information from first line\n",
    "            lang_line = lines[0].strip()\n",
    "            if lang_line.startswith(\"LANG:\"):\n",
    "                detected_lang = lang_line[5:]\n",
    "                print(f\"Language: {detected_lang}\")\n",
    "                # Remove the language line\n",
    "                content = \"\".join(lines[1:])\n",
    "            else:\n",
    "                # No language information, treat all lines as content\n",
    "                detected_lang = \"unknown\"\n",
    "                content = \"\".join(lines)\n",
    "            \n",
    "            if not content.strip():\n",
    "                print(\"No content to read\")\n",
    "                continue\n",
    "                \n",
    "            # Print a preview of the content\n",
    "            content_preview = content[:100] + \"...\" if len(content) > 100 else content\n",
    "            print(f\"Text content: {content_preview}\")\n",
    "            \n",
    "            # Get available voices\n",
    "            voices = tts_engine.getProperty(\"voices\")\n",
    "            \n",
    "            # Try to set appropriate voice based on language\n",
    "            voice_set = False\n",
    "            for voice in voices:\n",
    "                tts_lang = lang_mapping.get(detected_lang, \"english\")\n",
    "                if (tts_lang.lower() in voice.name.lower() or \n",
    "                    detected_lang in voice.id.lower()):\n",
    "                    tts_engine.setProperty(\"voice\", voice.id)\n",
    "                    voice_set = True\n",
    "                    print(f\"Using voice: {voice.name}\")\n",
    "                    break\n",
    "            \n",
    "            if not voice_set:\n",
    "                print(f\"No specific voice found for {detected_lang}, using default\")\n",
    "            \n",
    "            # Adjust speech rate based on language\n",
    "            if detected_lang in [\"ar\", \"ur\"]:\n",
    "                # Slower for Arabic and Urdu\n",
    "                tts_engine.setProperty(\"rate\", 130)\n",
    "            else:\n",
    "                tts_engine.setProperty(\"rate\", 150)\n",
    "            \n",
    "            # Read the text aloud\n",
    "            print(f\"Reading text aloud...\")\n",
    "            tts_engine.say(content)\n",
    "            tts_engine.runAndWait()\n",
    "            \n",
    "            # Pause between files to clearly separate them\n",
    "            print(\"Finished reading file.\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {text_file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n===== All Text Files Have Been Read =====\")\n",
    "\n",
    "def get_user_confirmation():\n",
    "    \"\"\"\n",
    "    Ask the user if they want to proceed to the TTS reading phase\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        response = input(\"\\nOCR processing complete. Proceed with reading text files? (y/n): \").lower()\n",
    "        if response in ['y', 'yes']:\n",
    "            return True\n",
    "        elif response in ['n', 'no']:\n",
    "            return False\n",
    "        else:\n",
    "            print(\"Please enter 'y' or 'n'\")\n",
    "\n",
    "# Main process\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Define folders\n",
    "        images_folder = \"part_2_images\"\n",
    "        output_folder = \"extracted_text\"\n",
    "        \n",
    "        # Print GPU optimization message\n",
    "        print(\"=== Running GPU-Optimized OCR for RTX 2060 ===\")\n",
    "        \n",
    "        # Step 1: Process images with OCR and save text\n",
    "        start_time = time.time()\n",
    "        text_folder = process_images_ocr_save_text(images_folder, output_folder)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"OCR processing completed in {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Optional: Ask for user confirmation before proceeding to TTS\n",
    "        if get_user_confirmation():\n",
    "            # Step 2: Read the saved text files aloud one by one\n",
    "            read_text_files_aloud(text_folder)\n",
    "        else:\n",
    "            print(\"TTS reading canceled. Text files are saved in the output folder.\")\n",
    "        \n",
    "        print(\"Process completed successfully!\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user\")\n",
    "        # Clean up GPU memory on interrupt\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        # Clean up GPU memory on error\n",
    "        torch.cuda.empty_cache()\n",
    "    finally:\n",
    "        # Final cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a smaller vision model first\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\", use_fast=True)\n",
    "print(\"Downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15f199",
   "metadata": {},
   "source": [
    "# LLava Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acecd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLAVA OCR Processing ===\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "GPU Memory: 12.88 GB\n",
      "Using GPU acceleration\n",
      "Output folder: extracted_text/llava\n",
      "Loading LLAVA model...\n",
      "Downloading/Loading processor...\n",
      "Downloading/Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# # LLAVA OCR Cell - GPU Accelerated Implementation\n",
    "\n",
    "# # Force GPU usage\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# os.environ['HF_HOME'] = 'D:/HuggingFaceCache' \n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# # Check GPU availability\n",
    "# if not torch.cuda.is_available():\n",
    "#     print(\"ERROR: CUDA not available. LLAVA requires GPU acceleration.\")\n",
    "#     exit(1)\n",
    "\n",
    "# print(\"=== LLAVA OCR Processing ===\")\n",
    "# print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "# print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# def process_images_with_llava(images_folder=\"part_2_images\", output_folder=\"extracted_text/llava\"):\n",
    "#     \"\"\"\n",
    "#     Process images using LLAVA model for OCR with GPU acceleration\n",
    "#     \"\"\"\n",
    "#     # Create output directory if it doesn't exist\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "#     # # Clear GPU memory before starting\n",
    "#     # torch.cuda.empty_cache()\n",
    "#     # gc.collect()\n",
    "    \n",
    "#     # Initialize LLAVA model and processor\n",
    "#     print(\"Loading LLAVA model...\")\n",
    "#     try:\n",
    "#         # TODO: Replace with your specific LLAVA model path/name\n",
    "#         model_name = \"llava-hf/llava-1.5-7b-hf\"  # Example model name\n",
    "        \n",
    "#         processor = AutoProcessor.from_pretrained(model_name, use_fast=True)\n",
    "#         model = LlavaForConditionalGeneration.from_pretrained(\n",
    "#             model_name,\n",
    "#             torch_dtype=torch.float16,  # Use fp16 for RTX 2060 efficiency\n",
    "#             device_map=\"cuda\",\n",
    "#             low_cpu_mem_usage=True\n",
    "#         )\n",
    "#         model.eval()\n",
    "#         print(\"LLAVA model loaded successfully!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading LLAVA model: {e}\")\n",
    "#         print(\"Please ensure you have the correct model name/path\")\n",
    "#         return\n",
    "    \n",
    "#     # Get all image files\n",
    "#     image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "#     image_files = []\n",
    "    \n",
    "#     for ext in image_extensions:\n",
    "#         image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "#         image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "#     if not image_files:\n",
    "#         print(f\"No images found in {images_folder}\")\n",
    "#         return\n",
    "    \n",
    "#     print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "#     # Process each image\n",
    "#     processed_count = 0\n",
    "#     skipped_count = 0\n",
    "    \n",
    "#     for i, image_path in enumerate(image_files, 1):\n",
    "#         image_filename = os.path.basename(image_path)\n",
    "#         base_name = os.path.splitext(image_filename)[0]\n",
    "#         text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "#         # Check if text file already exists (efficiency check)\n",
    "#         if os.path.exists(text_file_path):\n",
    "#             print(f\"\\n[{i}/{len(image_files)}] Skipping {image_filename} - already processed\")\n",
    "#             skipped_count += 1\n",
    "#             continue\n",
    "        \n",
    "#         print(f\"\\n[{i}/{len(image_files)}] Processing {image_filename}...\")\n",
    "        \n",
    "#         try:\n",
    "#             # Load and preprocess image\n",
    "#             image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "#             # Resize large images to optimize GPU memory\n",
    "#             max_dim = 1024  # Adjust based on your GPU memory\n",
    "#             if max(image.size) > max_dim:\n",
    "#                 image.thumbnail((max_dim, max_dim), Image.Resampling.LANCZOS)\n",
    "#                 print(f\"Resized image to {image.size} for GPU optimization\")\n",
    "            \n",
    "#             # Prepare prompt for OCR task\n",
    "#             prompt = \"USER: <image>\\nExtract all text from this image. Provide only the extracted text without any additional commentary.\\nASSISTANT:\"\n",
    "            \n",
    "#             # Process with LLAVA\n",
    "#             start_time = time.time()\n",
    "            \n",
    "#             # TODO: Adjust this section based on your specific LLAVA implementation\n",
    "#             inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda\")\n",
    "            \n",
    "#             # Generate text with GPU\n",
    "#             with torch.no_grad():\n",
    "#                 generated_ids = model.generate(\n",
    "#                     **inputs,\n",
    "#                     max_new_tokens=1024,\n",
    "#                     temperature=0.1,  # Low temperature for more accurate OCR\n",
    "#                     do_sample=False,\n",
    "#                     use_cache=True\n",
    "#                 )\n",
    "            \n",
    "#             # Decode the generated text\n",
    "#             extracted_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "#             # Remove the prompt from the output\n",
    "#             if \"ASSISTANT:\" in extracted_text:\n",
    "#                 extracted_text = extracted_text.split(\"ASSISTANT:\")[-1].strip()\n",
    "            \n",
    "#             end_time = time.time()\n",
    "#             print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "            \n",
    "#             if not extracted_text.strip():\n",
    "#                 print(\"No text detected in this image\")\n",
    "#                 # Save empty file to avoid reprocessing\n",
    "#                 with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                     f.write(\"\")\n",
    "#                 continue\n",
    "            \n",
    "#             # Save extracted text\n",
    "#             with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(extracted_text)\n",
    "            \n",
    "#             print(f\"Saved text to: {text_file_path}\")\n",
    "#             print(f\"Text preview: {extracted_text[:100]}...\" if len(extracted_text) > 100 else f\"Text: {extracted_text}\")\n",
    "            \n",
    "#             processed_count += 1\n",
    "            \n",
    "#             # Clear GPU memory after each image\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {image_filename}: {e}\")\n",
    "#             # Save error file to avoid reprocessing\n",
    "#             with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(f\"ERROR: {str(e)}\")\n",
    "#             torch.cuda.empty_cache()\n",
    "#             continue\n",
    "    \n",
    "#     # Final cleanup\n",
    "#     del model\n",
    "#     del processor\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "    \n",
    "#     print(f\"\\n=== LLAVA Processing Complete ===\")\n",
    "#     print(f\"Processed: {processed_count} images\")\n",
    "#     print(f\"Skipped: {skipped_count} images (already processed)\")\n",
    "#     print(f\"Total: {len(image_files)} images\")\n",
    "\n",
    "# # Run LLAVA OCR processing\n",
    "# if __name__ == \"__main__\":\n",
    "#     process_images_with_llava()\n",
    "\n",
    "\n",
    "\n",
    "# LLAVA OCR Cell - GPU Accelerated Implementation\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Force GPU usage\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['HF_HOME'] = 'D:/HuggingFaceCache' \n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(\"=== LLAVA OCR Processing ===\")\n",
    "if use_gpu:\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(\"Using GPU acceleration\")\n",
    "else:\n",
    "    print(\"GPU not available - using CPU (will be slower)\")\n",
    "    print(\"For better performance, install CUDA-compatible PyTorch\")\n",
    "\n",
    "def process_images_with_llava(images_folder=\"part_2_images\", output_folder=\"extracted_text/llava\"):\n",
    "    \"\"\"\n",
    "    Process images using LLAVA model for OCR with GPU acceleration\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "    # Clear GPU memory before starting (only if GPU available)\n",
    "    if use_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Initialize LLAVA model and processor\n",
    "    print(\"Loading LLAVA model...\")\n",
    "    try:\n",
    "        # Updated model name - use the correct LLAVA model\n",
    "        model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "        \n",
    "        print(\"Downloading/Loading processor...\")\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            model_name, \n",
    "            use_fast=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(\"Downloading/Loading model...\")\n",
    "        model = LlavaForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if use_gpu else torch.float32,  # Use fp16 only if GPU available\n",
    "            device_map=\"auto\" if use_gpu else None,  # Use auto device mapping only if GPU available\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Move model to appropriate device\n",
    "        if use_gpu and not next(model.parameters()).is_cuda:\n",
    "            model = model.to(device)\n",
    "        elif not use_gpu:\n",
    "            model = model.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        print(\"LLAVA model loaded successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LLAVA model: {e}\")\n",
    "        print(\"Trying alternative approach...\")\n",
    "        \n",
    "        # Alternative: Try different model or installation\n",
    "        try:\n",
    "            # Alternative model names to try\n",
    "            alternative_models = [\n",
    "                \"llava-hf/llava-1.5-13b-hf\",\n",
    "                \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "                \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "            ]\n",
    "            \n",
    "            for alt_model in alternative_models:\n",
    "                try:\n",
    "                    print(f\"Trying {alt_model}...\")\n",
    "                    processor = AutoProcessor.from_pretrained(alt_model, trust_remote_code=True)\n",
    "                    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "                        alt_model,\n",
    "                        torch_dtype=torch.float16 if use_gpu else torch.float32,\n",
    "                        device_map=\"auto\" if use_gpu else None,\n",
    "                        low_cpu_mem_usage=True,\n",
    "                        trust_remote_code=True\n",
    "                    )\n",
    "                    if use_gpu and not next(model.parameters()).is_cuda:\n",
    "                        model = model.to(device)\n",
    "                    elif not use_gpu:\n",
    "                        model = model.to(device)\n",
    "                    model.eval()\n",
    "                    print(f\"Successfully loaded {alt_model}!\")\n",
    "                    model_name = alt_model\n",
    "                    break\n",
    "                except Exception as alt_e:\n",
    "                    print(f\"Failed to load {alt_model}: {alt_e}\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(\"All model loading attempts failed.\")\n",
    "                print(\"Please ensure you have installed the requirements:\")\n",
    "                print(\"pip install transformers torch torchvision accelerate\")\n",
    "                print(\"pip install git+https://github.com/huggingface/transformers.git\")\n",
    "                return\n",
    "                \n",
    "        except Exception as final_e:\n",
    "            print(f\"Final error: {final_e}\")\n",
    "            return\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Process each image\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "        # Check if text file already exists (efficiency check)\n",
    "        if os.path.exists(text_file_path):\n",
    "            print(f\"\\n[{i}/{len(image_files)}] Skipping {image_filename} - already processed\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[{i}/{len(image_files)}] Processing {image_filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Resize large images to optimize memory\n",
    "            max_dim = 1024 if use_gpu else 512  # Smaller images for CPU processing\n",
    "            if max(image.size) > max_dim:\n",
    "                image.thumbnail((max_dim, max_dim), Image.Resampling.LANCZOS)\n",
    "                print(f\"Resized image to {image.size} for {'GPU' if use_gpu else 'CPU'} optimization\")\n",
    "            \n",
    "            # Prepare prompt for OCR task - Updated format\n",
    "            prompt = \"USER: <image>\\nExtract all text from this image. Provide only the extracted text without any additional commentary.\\nASSISTANT:\"\n",
    "            \n",
    "            # Process with LLAVA\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Updated processing approach\n",
    "            inputs = processor(prompt, image, return_tensors=\"pt\")\n",
    "            \n",
    "            # Move inputs to appropriate device\n",
    "            if use_gpu:\n",
    "                inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "            else:\n",
    "                inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate text with appropriate device\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=1024 if use_gpu else 512,  # Reduce tokens for CPU\n",
    "                    temperature=0.1,  # Low temperature for more accurate OCR\n",
    "                    do_sample=False,\n",
    "                    use_cache=True,\n",
    "                    pad_token_id=processor.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            generated_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Clean up the output - remove prompt\n",
    "            if \"ASSISTANT:\" in generated_text:\n",
    "                extracted_text = generated_text.split(\"ASSISTANT:\")[-1].strip()\n",
    "            else:\n",
    "                extracted_text = generated_text.strip()\n",
    "            \n",
    "            # Remove any remaining prompt artifacts\n",
    "            if \"USER:\" in extracted_text:\n",
    "                extracted_text = extracted_text.split(\"USER:\")[-1].strip()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "            \n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                # Save empty file to avoid reprocessing\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(\"\")\n",
    "                continue\n",
    "            \n",
    "            # Save extracted text\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(extracted_text)\n",
    "            \n",
    "            print(f\"Saved text to: {text_file_path}\")\n",
    "            print(f\"Text preview: {extracted_text[:100]}...\" if len(extracted_text) > 100 else f\"Text: {extracted_text}\")\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "            # Clear memory after each image (only if GPU available)\n",
    "            if use_gpu:\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_filename}: {e}\")\n",
    "            # Save error file to avoid reprocessing\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"ERROR: {str(e)}\")\n",
    "            if use_gpu:\n",
    "                torch.cuda.empty_cache()\n",
    "            continue\n",
    "    \n",
    "    # Final cleanup\n",
    "    del model\n",
    "    del processor\n",
    "    if use_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n=== LLAVA Processing Complete ===\")\n",
    "    print(f\"Device used: {'GPU' if use_gpu else 'CPU'}\")\n",
    "    print(f\"Processed: {processed_count} images\")\n",
    "    print(f\"Skipped: {skipped_count} images (already processed)\")\n",
    "    print(f\"Total: {len(image_files)} images\")\n",
    "\n",
    "# Install requirements function\n",
    "def install_requirements():\n",
    "    \"\"\"Install required packages if not already installed\"\"\"\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    required_packages = [\n",
    "        \"transformers>=4.36.0\",\n",
    "        \"torch\",\n",
    "        \"torchvision\", \n",
    "        \"accelerate\",\n",
    "        \"pillow\",\n",
    "        \"bitsandbytes\"  # For efficient loading\n",
    "    ]\n",
    "    \n",
    "    print(\"Installing/updating required packages...\")\n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to install {package}: {e}\")\n",
    "    \n",
    "    # Install latest transformers from git for LLAVA support\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/huggingface/transformers.git\"])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install transformers from git: {e}\")\n",
    "\n",
    "# Run LLAVA OCR processing\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment the line below if you need to install requirements\n",
    "    # install_requirements()\n",
    "    \n",
    "    process_images_with_llava()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03004154",
   "metadata": {},
   "source": [
    "# Shikra Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3bec15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SHIKRA OCR Processing ===\n",
      "GPU: NVIDIA GeForce RTX 2060\n",
      "GPU Memory: 12.88 GB\n",
      "Output folder: extracted_text/shikra\n",
      "Loading SHIKRA model...\n",
      "Error loading SHIKRA model: shikras/shikra-7b-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "Please ensure you have the correct model name/path\n"
     ]
    }
   ],
   "source": [
    "# SHIKRA OCR Cell - GPU Accelerated Implementation\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# Force GPU usage\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Check GPU availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"ERROR: CUDA not available. SHIKRA requires GPU acceleration.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"=== SHIKRA OCR Processing ===\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "def process_images_with_shikra(images_folder=\"part_2_images\", output_folder=\"extracted_text/shikra\"):\n",
    "    \"\"\"\n",
    "    Process images using SHIKRA model for OCR with GPU acceleration\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"Output folder: {output_folder}\")\n",
    "    \n",
    "    # Clear GPU memory before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Initialize SHIKRA model and processor\n",
    "    print(\"Loading SHIKRA model...\")\n",
    "    try:\n",
    "        # TODO: Replace with your specific SHIKRA model path/name\n",
    "        model_name = \"shikras/shikra-7b-v1\"  # Example model name\n",
    "        \n",
    "        processor = AutoProcessor.from_pretrained(model_name, use_fast=True)\n",
    "        model = AutoModelForVision2Seq.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,  # Use fp16 for RTX 2060 efficiency\n",
    "            device_map=\"cuda\",\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        model.eval()\n",
    "        print(\"SHIKRA model loaded successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SHIKRA model: {e}\")\n",
    "        print(\"Please ensure you have the correct model name/path\")\n",
    "        return\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Process each image\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "        # Check if text file already exists (efficiency check)\n",
    "        if os.path.exists(text_file_path):\n",
    "            print(f\"\\n[{i}/{len(image_files)}] Skipping {image_filename} - already processed\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n[{i}/{len(image_files)}] Processing {image_filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Resize large images to optimize GPU memory\n",
    "            max_dim = 1024  # Adjust based on your GPU memory\n",
    "            if max(image.size) > max_dim:\n",
    "                image.thumbnail((max_dim, max_dim), Image.Resampling.LANCZOS)\n",
    "                print(f\"Resized image to {image.size} for GPU optimization\")\n",
    "            \n",
    "            # Prepare prompt for OCR task\n",
    "            # SHIKRA might use a different prompt format - adjust as needed\n",
    "            prompt = \"<image> Extract and transcribe all text visible in this image.\"\n",
    "            \n",
    "            # Process with SHIKRA\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # TODO: Adjust this section based on your specific SHIKRA implementation\n",
    "            inputs = processor(\n",
    "                text=prompt,\n",
    "                images=image,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            # Generate text with GPU\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=1024,\n",
    "                    temperature=0.1,  # Low temperature for more accurate OCR\n",
    "                    do_sample=False,\n",
    "                    num_beams=1,  # Adjust based on accuracy vs speed tradeoff\n",
    "                    use_cache=True\n",
    "                )\n",
    "            \n",
    "            # Decode the generated text\n",
    "            extracted_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Clean up the output (remove prompt if included)\n",
    "            if prompt in extracted_text:\n",
    "                extracted_text = extracted_text.replace(prompt, \"\").strip()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            print(f\"OCR completed in {end_time - start_time:.2f} seconds\")\n",
    "            \n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                # Save empty file to avoid reprocessing\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(\"\")\n",
    "                continue\n",
    "            \n",
    "            # Save extracted text\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(extracted_text)\n",
    "            \n",
    "            print(f\"Saved text to: {text_file_path}\")\n",
    "            print(f\"Text preview: {extracted_text[:100]}...\" if len(extracted_text) > 100 else f\"Text: {extracted_text}\")\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "            # Clear GPU memory after each image\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_filename}: {e}\")\n",
    "            # Save error file to avoid reprocessing\n",
    "            with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"ERROR: {str(e)}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "    \n",
    "    # Final cleanup\n",
    "    del model\n",
    "    del processor\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n=== SHIKRA Processing Complete ===\")\n",
    "    print(f\"Processed: {processed_count} images\")\n",
    "    print(f\"Skipped: {skipped_count} images (already processed)\")\n",
    "    print(f\"Total: {len(image_files)} images\")\n",
    "\n",
    "# Run SHIKRA OCR processing\n",
    "if __name__ == \"__main__\":\n",
    "    process_images_with_shikra()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
