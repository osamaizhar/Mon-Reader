{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b431ab",
   "metadata": {},
   "source": [
    "# Mon-Reader: Detecting Page Flipping, Text Extraction, and Speech Synthesis\n",
    "This notebook covers the following workflow:\n",
    "1. **Detect page flipping vs. still pages using CNNs** (ResNet, MobileNet, EfficientNet, and a custom Osama Net) on the provided image dataset.\n",
    "2. **Extract text from detected still page frames** (future step, not implemented in this notebook).\n",
    "3. **Synthesize speech from extracted text (TTS)** (future step, not implemented in this notebook).\n",
    "\n",
    "## Notebook Outline\n",
    "- Import Required Libraries (with GPU support)\n",
    "- Load and Preprocess Dataset (images/training and images/testing, with flip/notflip subfolders)\n",
    "- Prepare Data Generators (with augmentation)\n",
    "- Build and Evaluate ResNet Model\n",
    "- Build and Evaluate MobileNet Model\n",
    "- Build and Evaluate EfficientNet Model\n",
    "- Define and Train Custom CNN (Osama Net)\n",
    "- Evaluate Osama Net on Test Set and Show Predictions\n",
    "- Compare Model Accuracies and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8469c28",
   "metadata": {},
   "source": [
    "# 1. Import Required Libraries and Enable GPU Support\n",
    "\n",
    "In this section, we import PyTorch and check for GPU availability. PyTorch will automatically use the GPU when available, which significantly speeds up model training.\n",
    "\n",
    "## CPU vs GPU Tips:\n",
    "- **GPU Acceleration**: Deep learning models train much faster on GPU than CPU.\n",
    "- **Device Selection**: We use `torch.device()` to automatically select GPU when available.\n",
    "- **Memory Management**: \n",
    "  - For large models, use smaller batch sizes on GPU with limited memory\n",
    "  - Use `pin_memory=True` for faster GPU data transfers\n",
    "  - Call `model.to(device)` to move the model to GPU/CPU\n",
    "  - Ensure tensors are on the same device with `.to(device)`\n",
    "- **Switching Between Devices**: To switch between CPU/GPU, just change the device variable and move tensors accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a80cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu128\n",
      "CUDA available: True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59e8489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import random\n",
    "from torch import nn, optim\n",
    "from torchvision import models\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7bbcee",
   "metadata": {},
   "source": [
    "# 2. Load and Preprocess Dataset\n",
    "\n",
    "Here we define our image size and batch size parameters, and load the training and testing datasets from their respective directories. The dataset has two classes: 'flip' and 'notflip'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1829858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['flip', 'notflip']\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dir = \"images/training\"\n",
    "test_dir = \"images/testing\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f0916c",
   "metadata": {},
   "source": [
    "# 3. Prepare Data Generators with Augmentation\n",
    "\n",
    "Data augmentation helps prevent overfitting by creating variations of our training images. \n",
    "This is especially important for smaller datasets. The transformations include:\n",
    "- Random horizontal flipping\n",
    "- Random rotation (up to 10 degrees)\n",
    "- Random affine transformations (translation and scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52bd66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomAffine(0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "aug_train_dataset = ImageFolder(root=train_dir, transform=aug_transform)\n",
    "aug_train_loader = DataLoader(aug_train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb1d415",
   "metadata": {},
   "source": [
    "# 4. Build and Evaluate Models\n",
    "\n",
    "## Using BCEWithLogitsLoss Instead of BCELoss\n",
    "\n",
    "In our updated models, we'll use `BCEWithLogitsLoss` instead of `BCELoss` for the following reasons:\n",
    "\n",
    "1. **Numerical Stability**: BCEWithLogitsLoss combines sigmoid and binary cross-entropy in one operation, which is more numerically stable.\n",
    "\n",
    "2. **Performance**: It's more efficient, especially on GPU, since it can leverage optimized implementations.\n",
    "\n",
    "3. **Avoiding Vanishing Gradients**: By incorporating the sigmoid operation, it prevents extreme values that could lead to vanishing gradients.\n",
    "\n",
    "4. **Simplified Model Architecture**: We can remove the final Sigmoid layer from our models, as BCEWithLogitsLoss applies it internally.\n",
    "\n",
    "All of our models will output raw logits rather than probabilities, with the loss function handling the sigmoid transformation.\n",
    "\n",
    "## ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c4fa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Test Accuracy: 0.7069\n",
      "Epoch 2/10, Test Accuracy: 0.7605\n",
      "Epoch 3/10, Test Accuracy: 0.8208\n",
      "Epoch 4/10, Test Accuracy: 0.8476\n",
      "Epoch 5/10, Test Accuracy: 0.8191\n",
      "Epoch 6/10, Test Accuracy: 0.8040\n",
      "Epoch 7/10, Test Accuracy: 0.8007\n",
      "Epoch 8/10, Test Accuracy: 0.7420\n",
      "Epoch 9/10, Test Accuracy: 0.7822\n",
      "Epoch 10/10, Test Accuracy: 0.8576\n",
      "ResNet Test Accuracy: 0.8576\n"
     ]
    }
   ],
   "source": [
    "def build_resnet(num_classes=1):\n",
    "    model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(128, num_classes),\n",
    "        # No sigmoid - using BCEWithLogitsLoss\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_loader, test_loader, device, epochs=10):\n",
    "    model = model.to(device)\n",
    "    # Using BCEWithLogitsLoss for numerical stability\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.float().to(device)\n",
    "            labels = labels.unsqueeze(1)  # Add channel dimension for binary classification\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.float().to(device)\n",
    "                labels = labels.unsqueeze(1)\n",
    "                outputs = model(images)\n",
    "                # Threshold logits at 0 (equivalent to 0.5 for sigmoid)\n",
    "                preds = (outputs > 0).int()\n",
    "                correct += (preds == labels.int()).sum().item()\n",
    "                total += labels.size(0)\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Test Accuracy: {acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "    return best_acc\n",
    "\n",
    "resnet_model = build_resnet()\n",
    "resnet_acc = train_model(resnet_model, aug_train_loader, test_loader, device, epochs=10)\n",
    "print(f\"ResNet Test Accuracy: {resnet_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653c707",
   "metadata": {},
   "source": [
    "# 5. Build and Evaluate MobileNet Model\n",
    "\n",
    "MobileNet is a lightweight CNN architecture designed for mobile and embedded vision applications.\n",
    "It's significantly smaller than ResNet while still providing good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97ff8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     97\u001b[39m mobilenet_model = build_mobilenet()\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m mobilenet_acc = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmobilenet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m    102\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Print the final accuracy\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMobileNet Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmobilenet_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, test_loader, device, epochs, loss_type)\u001b[39m\n\u001b[32m     28\u001b[39m model.train()\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     images, labels = \u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels.float().to(device)\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m loss_type == \u001b[33m\"\u001b[39m\u001b[33mbce\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     32\u001b[39m         labels = labels.unsqueeze(\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "def build_mobilenet(num_classes=1):\n",
    "    model = models.mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
    "    # Freeze all the parameters in the pre-trained model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the classifier with a new one\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(model.classifier[1].in_features, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(128, num_classes),\n",
    "        # No sigmoid - using BCEWithLogitsLoss\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, device, epochs=10):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Ensure only the classifier parameters are being trained\n",
    "    optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "    # Using BCEWithLogitsLoss for numerical stability\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.float().to(device)\n",
    "            labels = labels.unsqueeze(1)  # Add channel dimension for binary classification\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.float().to(device)\n",
    "                labels = labels.unsqueeze(1)\n",
    "\n",
    "                outputs = model(images)\n",
    "                # Threshold logits at 0 (equivalent to 0.5 for sigmoid)\n",
    "                preds = (outputs > 0).int()\n",
    "\n",
    "                correct += (preds == labels.int()).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Test Accuracy: {acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "# Build the MobileNet model\n",
    "mobilenet_model = build_mobilenet()\n",
    "\n",
    "# Train the model\n",
    "mobilenet_acc = train_model(\n",
    "    mobilenet_model, aug_train_loader, test_loader, device, epochs=10\n",
    ")\n",
    "\n",
    "# Print the final accuracy\n",
    "print(f\"MobileNet Test Accuracy: {mobilenet_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18fde1d",
   "metadata": {},
   "source": [
    "# 6. Build and Evaluate EfficientNet Model\n",
    "\n",
    "EfficientNet uses a compound scaling method that uniformly scales network width, depth, and resolution\n",
    "to balance model size and accuracy. It's known for achieving state-of-the-art accuracy with fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef22290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficientnet(num_classes=1):\n",
    "    model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(model.classifier[1].in_features, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, num_classes),\n",
    "        # No sigmoid - using BCEWithLogitsLoss\n",
    "    )\n",
    "    return model\n",
    "\n",
    "efficientnet_model = build_efficientnet()\n",
    "efficientnet_acc = train_model(efficientnet_model, aug_train_loader, test_loader, device, epochs=10)\n",
    "print(f\"EfficientNet Test Accuracy: {efficientnet_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2283c",
   "metadata": {},
   "source": [
    "# 7. Define and Train Custom CNN (Osama Net)\n",
    "\n",
    "Here we define a custom CNN architecture called \"Osama Net\" with three convolutional blocks\n",
    "followed by a classifier with dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a72fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OsamaNet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(OsamaNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes),\n",
    "            # No sigmoid - using BCEWithLogitsLoss\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "osama_net = OsamaNet()\n",
    "osama_acc = train_model(osama_net, aug_train_loader, test_loader, device, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7f468",
   "metadata": {},
   "source": [
    "# 8. Evaluate Models and Compare Results\n",
    "\n",
    "Finally, we evaluate our models on the test dataset and visualize some predictions.\n",
    "We also compare the performance of all models to identify the best architecture for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3236d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Osama Net on Test Set and Show Predictions\n",
    "osama_net.eval()\n",
    "correct, total = 0, 0\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.float().to(device)\n",
    "        outputs = osama_net(images)\n",
    "        # Threshold logits at 0 (equivalent to 0.5 for sigmoid)\n",
    "        preds = (outputs > 0).int()\n",
    "        correct += (preds == labels.unsqueeze(1).int()).sum().item()\n",
    "        total += labels.size(0)\n",
    "        all_preds.extend(preds.cpu().numpy().flatten())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten())\n",
    "osama_acc = correct / total\n",
    "print(f\"Osama Net Test Accuracy: {osama_acc:.4f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "indices = random.sample(range(len(all_labels)), 6)\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, idx in enumerate(indices):\n",
    "    img_path, true_label, pred_label = test_dataset.imgs[idx][0], int(all_labels[idx]), int(all_preds[idx])\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare Model Accuracies and Analysis\n",
    "print(f\"ResNet Test Accuracy: {resnet_acc:.4f}\")\n",
    "print(f\"MobileNet Test Accuracy: {mobilenet_acc:.4f}\")\n",
    "print(f\"EfficientNet Test Accuracy: {efficientnet_acc:.4f}\")\n",
    "print(f\"Osama Net Test Accuracy: {osama_acc:.4f}\")\n",
    "\n",
    "accuracies = [resnet_acc, mobilenet_acc, efficientnet_acc, osama_acc]\n",
    "model_names = [\"ResNet\", \"MobileNet\", \"EfficientNet\", \"Osama Net\"]\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(model_names, accuracies, color=[\"royalblue\", \"orange\", \"green\", \"purple\"])\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"Model Comparison on Flipping Classification\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "print(\"Analysis:\")\n",
    "print(\"Compare the results above. The best model is the one with the highest test accuracy. Consider model complexity, training time, and overfitting when choosing the best architecture for this task.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
