{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pbyd9jj45vc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cpu\n",
      "CUDA available: False\n",
      "CUDA version: None\n",
      "NVIDIA GPU detected by system:\n",
      "|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:01:00.0  On |                  N/A |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    }
   ],
   "source": [
    "# Check CUDA installation and GPU detection\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Check if nvidia-smi works\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"NVIDIA GPU detected by system:\")\n",
    "        print(result.stdout.split(\"\\n\")[8])  # GPU info line\n",
    "    else:\n",
    "        print(\"nvidia-smi failed - no NVIDIA driver detected\")\n",
    "except FileNotFoundError:\n",
    "    print(\"nvidia-smi not found - NVIDIA drivers may not be installed\")\n",
    "\n",
    "\n",
    "import easyocr\n",
    "reader = easyocr.Reader(['en'], gpu=True)\n",
    "# If no errors, GPU should be working with EasyOCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ef0d6",
   "metadata": {},
   "source": [
    "# 1. With Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b886e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-05 15:06:25,076] [ WARNING] easyocr.py:71 - Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 6 images to process\n",
      "\n",
      "--- Processing Image 1/6: IMG_20250629_214324_528.jpg ---\n",
      "Extracting text...\n",
      "Extracted text: 4: Employee Hierarchy ٥. Employee ( parent ): 0 Manager, like name, id; method like get_ role specifie methods Code: lass Employee : def init ( self, self .name _i0): name self . emp_id emp_id def get_details(self): print (f\"Name : {self name}, I0: {self ,emp_id}\" ) tlass Manager ( Employee)= def calculatesalary(self): def assign_task(self): print ( \"Manager assigns tasks ) rlass Developer ( Employee ) def calculatesalary(self): 6000  U$D\") def assign_task(self): Intern Employee ) : def calculatesalary(self): print ( Intern salary: 2000 U$0\") def assign task(self): print( \"Intern assists in tasks\" ) Manager ( \"Alice 101) Lelculate_salary( 10?) Task attributes Developer , implement assign_ name , dass\n",
      "Detected language: en\n",
      "Reading text aloud in en...\n",
      "\n",
      "--- Processing Image 2/6: IMG_20250629_214328_880.jpg ---\n",
      "Extracting text...\n",
      "Extracted text: 11 Pyiion Perform all the Code: the output # Polymorphism math Calculation class def area(self): 'Subclasses class Rectangle( Shape ): def init _ (self, width, height): self width width self height height def area(self): return self width self height class Circle( Shape ): def init _ (self, radius ): self radius radius def area(self): return math pi * self radius ** 2 def area( shape ): print(f The area is: {shape area()} \") rectangle Rectangle(5, 10) Circle(3) print _ area(rectangle ) print _ area( circle ) Computing and Emerging Sciences Shape import Shape: print circle Applied\n",
      "Detected language: en\n",
      "Reading text aloud in en...\n",
      "\n",
      "--- Processing Image 3/6: IMG_20250629_214514_439.jpg ---\n",
      "Extracting text...\n",
      "Extracted text: ٤ ؟ ي ا 0 0 ٤ ؟ [ ؟ ٠٤؟ ٤ 0 ٦٠ سی\n",
      "Detected language: fa\n",
      "Reading text aloud in fa...\n",
      "\n",
      "--- Processing Image 4/6: IMG_20250629_214324_528.jpg ---\n",
      "Extracting text...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 123\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Process images in the part_2_images folder\u001b[39;00m\n\u001b[32m    122\u001b[39m images_folder = \u001b[33m\"\u001b[39m\u001b[33mpart_2_images\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[43mprocess_images_with_ocr_tts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mprocess_images_with_ocr_tts\u001b[39m\u001b[34m(images_folder)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Extract text using EasyOCR\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExtracting text...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m results = \u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Combine all detected text\u001b[39;00m\n\u001b[32m     57\u001b[39m extracted_text = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m     58\u001b[39m     [result[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[32m2\u001b[39m] > \u001b[32m0.5\u001b[39m]\n\u001b[32m     59\u001b[39m )  \u001b[38;5;66;03m# confidence > 0.5\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\easyocr\\easyocr.py:456\u001b[39m, in \u001b[36mReader.readtext\u001b[39m\u001b[34m(self, image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, y_ths, x_ths, add_margin, threshold, bbox_min_score, bbox_min_size, max_candidates, output_format)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[33;03mParameters:\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[33;03mimage: file path or numpy-array or a byte stream object\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    454\u001b[39m img, img_cv_grey = reformat_input(image)\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m horizontal_list, free_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mmin_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mslope_ths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mslope_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mycenter_ths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mycenter_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mheight_ths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth_ths\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43madd_margin\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_margin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreformat\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# get the 1st result from hor & free list as self.detect returns a list of depth 3\u001b[39;00m\n\u001b[32m    467\u001b[39m horizontal_list, free_list = horizontal_list[\u001b[32m0\u001b[39m], free_list[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\easyocr\\easyocr.py:321\u001b[39m, in \u001b[36mReader.detect\u001b[39m\u001b[34m(self, img, min_size, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, add_margin, reformat, optimal_num_chars, threshold, bbox_min_score, bbox_min_size, max_candidates)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reformat:\n\u001b[32m    319\u001b[39m     img, img_cv_grey = reformat_input(img)\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m text_box_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_textbox\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m                            \u001b[49m\u001b[43moptimal_num_chars\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimal_num_chars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m horizontal_list_agg, free_list_agg = [], []\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_box \u001b[38;5;129;01min\u001b[39;00m text_box_list:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\easyocr\\detection.py:95\u001b[39m, in \u001b[36mget_textbox\u001b[39m\u001b[34m(detector, image, canvas_size, mag_ratio, text_threshold, link_threshold, low_text, poly, device, optimal_num_chars, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m result = []\n\u001b[32m     94\u001b[39m estimate_num_chars = optimal_num_chars \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m bboxes_list, polys_list = \u001b[43mtest_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_num_chars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimate_num_chars:\n\u001b[32m    100\u001b[39m     polys_list = [[p \u001b[38;5;28;01mfor\u001b[39;00m p, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(polys, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mabs\u001b[39m(optimal_num_chars - x[\u001b[32m1\u001b[39m]))]\n\u001b[32m    101\u001b[39m                   \u001b[38;5;28;01mfor\u001b[39;00m polys \u001b[38;5;129;01min\u001b[39;00m polys_list]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\easyocr\\detection.py:46\u001b[39m, in \u001b[36mtest_net\u001b[39m\u001b[34m(canvas_size, mag_ratio, net, image, text_threshold, link_threshold, low_text, poly, device, estimate_num_chars)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     y, feature = \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m boxes_list, polys_list = [], []\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m y:\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# make score and link map\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\easyocr\\craft.py:60\u001b[39m, in \u001b[36mCRAFT.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" Base network \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     sources = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbasenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" U network \"\"\"\u001b[39;00m\n\u001b[32m     63\u001b[39m     y = torch.cat([sources[\u001b[32m0\u001b[39m], sources[\u001b[32m1\u001b[39m]], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\easyocr\\model\\modules.py:68\u001b[39m, in \u001b[36mvgg16_bn.forward\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mslice1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     h_relu2_2 = h\n\u001b[32m     70\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.slice2(h)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Repositories\\Apziva Projects\\Project 4\\Mon-Reader\\envs\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import easyocr\n",
    "import torch\n",
    "import pyttsx3\n",
    "from langdetect import detect\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Check for GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize EasyOCR reader with GPU support\n",
    "reader = easyocr.Reader([\"ar\", \"ur\", \"en\"], gpu=torch.cuda.is_available())\n",
    "\n",
    "# Initialize TTS engine\n",
    "tts_engine = pyttsx3.init()\n",
    "\n",
    "\n",
    "def process_images_with_ocr_tts(images_folder):\n",
    "    \"\"\"\n",
    "    Process all images in the folder, extract text, detect language, and read aloud\n",
    "    \"\"\"\n",
    "    # Get all image files in the folder\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        print(\n",
    "            f\"\\n--- Processing Image {i}/{len(image_files)}: {os.path.basename(image_path)} ---\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Read image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Could not read image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Extract text using EasyOCR\n",
    "            print(\"Extracting text...\")\n",
    "            results = reader.readtext(image)\n",
    "\n",
    "            # Combine all detected text\n",
    "            extracted_text = \" \".join(\n",
    "                [result[1] for result in results if result[2] > 0.5]\n",
    "            )  # confidence > 0.5\n",
    "\n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Extracted text: {extracted_text}\")\n",
    "\n",
    "            # Detect language\n",
    "            try:\n",
    "                detected_lang = detect(extracted_text)\n",
    "                print(f\"Detected language: {detected_lang}\")\n",
    "\n",
    "                # Language mapping for TTS\n",
    "                lang_mapping = {\n",
    "                    \"en\": \"english\",\n",
    "                    \"ar\": \"arabic\",\n",
    "                    \"ur\": \"urdu\",\n",
    "                    \"hi\": \"hindi\",\n",
    "                }\n",
    "\n",
    "                # Set TTS language if available\n",
    "                tts_lang = lang_mapping.get(detected_lang, \"english\")\n",
    "\n",
    "                # Get available voices\n",
    "                voices = tts_engine.getProperty(\"voices\")\n",
    "\n",
    "                # Try to set appropriate voice based on language\n",
    "                voice_set = False\n",
    "                for voice in voices:\n",
    "                    if (\n",
    "                        tts_lang.lower() in voice.name.lower()\n",
    "                        or detected_lang in voice.id.lower()\n",
    "                    ):\n",
    "                        tts_engine.setProperty(\"voice\", voice.id)\n",
    "                        voice_set = True\n",
    "                        break\n",
    "\n",
    "                if not voice_set:\n",
    "                    print(f\"No specific voice found for {detected_lang}, using default\")\n",
    "\n",
    "                # Adjust speech rate\n",
    "                tts_engine.setProperty(\"rate\", 150)  # Adjust speed as needed\n",
    "\n",
    "                # Read the text aloud\n",
    "                print(f\"Reading text aloud in {detected_lang}...\")\n",
    "                tts_engine.say(extracted_text)\n",
    "                tts_engine.runAndWait()\n",
    "\n",
    "            except Exception as lang_error:\n",
    "                print(f\"Language detection failed: {lang_error}\")\n",
    "                print(\"Reading with default language...\")\n",
    "                tts_engine.say(extracted_text)\n",
    "                tts_engine.runAndWait()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n--- Processing Complete ---\")\n",
    "\n",
    "\n",
    "# Process images in the part_2_images folder\n",
    "images_folder = \"part_2_images\"\n",
    "process_images_with_ocr_tts(images_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676cd64e",
   "metadata": {},
   "source": [
    "# V2 saving in file and reading aloud from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "974d15e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 6 images to process\n",
      "\n",
      "--- Processing Image 1/6: IMG_20250629_214324_528.jpg ---\n",
      "Extracting text...\n",
      "Extracted text: 4: Employee Hierarchy ٥. Employee ( parent ): 0 Manager, like name, id; method like get_ role specifie methods Code: lass Employee : def init ( self, self .name _i0): name self . emp_id emp_id def get_details(self): print (f\"Name : {self name}, I0: {self ,emp_id}\" ) tlass Manager ( Employee) = def calculatesalary(self): def assign_task(self): print ( \"Manager assigns tasks ) rlass Developer ( Employee ) def calculatesalary(self): 6000  U$D\") def assign_task(self): print( \"Developer writes code\") Intern Employee ) : def calculatesalary(self): print ( Intern salary: 2000 U$0\") def assign task(self): print( \"Intern assists in tasks\" ) Manager ( \"Alice 101) Lelculate_salary( 10?) Task attributes Developer , implement assign_ name , dass\n",
      "Detected language: en\n",
      "Saved text to: extracted_text\\IMG_20250629_214324_528.txt\n",
      "\n",
      "--- Processing Image 2/6: IMG_20250629_214514_439.jpg ---\n",
      "Extracting text...\n",
      "Extracted text: ٤ ؟ ي ا 0 0 ٤ ؟ [ ؟ ٠٤؟ ٤ 0 ٦٠ سی\n",
      "Detected language: fa\n",
      "Saved text to: extracted_text\\IMG_20250629_214514_439.txt\n",
      "\n",
      "--- Processing Image 3/6: textbook_img.jpg ---\n",
      "Extracting text...\n",
      "Extracted text: to the tribe or Sad bin Bakr bin Hawazen Their children became the Prophet s foster brothers and sisters Their names were Abdullah, Anisa, Prophet Haleemah's house is unexpectedly blessed Haleemah and her husband found their lives changed the moment took Muhammad home They had traveled 10 Makkah on frail female donkey that could barely keep up with thcir caravan the return journey however, as Haleemah rode with the infant in her arms, the same animal moved so swiftly that it left the caravan behind While Muhammad with Haleemah s family, the house overflowed with blessings Haleemah herself narrated that she brought Muhammad to her home during drought  Her she camel would not give a of milk Haleemahs child would cry the whole night out of hunger With the child so distraught, Haleemah and Harith found it hard [0 night changed, however, when Haleemah brought Muhammad home and placed him on her Her breasts overfilowed with milk so that both Muhammad and her own child drank their fill of milk and fell fast When Harith went to the she camel he was amazed at what he saw The she camels udders were full of milk and ready to overflow It gave so much milk that Haleemahs family was able to that night on full stomachs Haleemahs household suddenly appeared [0 be untouched by the drought, although in Dayar Banu Sa`d, the most drought stricken spot in the region The familys goats would return from grazing with their stomachs full of grass and their udders bursting with WHEN THE MOON 23 and thcy On stayed sleep Things lap  asleep  sleep they\n",
      "Detected language: en\n",
      "Saved text to: extracted_text\\textbook_img.txt\n",
      "\n",
      "--- Processing Image 4/6: IMG_20250629_214324_528.jpg ---\n",
      "Extracting text...\n",
      "Extracted text: 4: Employee Hierarchy ٥. Employee ( parent ): 0 Manager, like name, id; method like get_ role specifie methods Code: lass Employee : def init ( self, self .name _i0): name self . emp_id emp_id def get_details(self): print (f\"Name : {self name}, I0: {self ,emp_id}\" ) tlass Manager ( Employee) = def calculatesalary(self): def assign_task(self): print ( \"Manager assigns tasks ) rlass Developer ( Employee ) def calculatesalary(self): 6000  U$D\") def assign_task(self): print( \"Developer writes code\") Intern Employee ) : def calculatesalary(self): print ( Intern salary: 2000 U$0\") def assign task(self): print( \"Intern assists in tasks\" ) Manager ( \"Alice 101) Lelculate_salary( 10?) Task attributes Developer , implement assign_ name , dass\n",
      "Detected language: en\n",
      "Saved text to: extracted_text\\IMG_20250629_214324_528.txt\n",
      "\n",
      "--- Processing Image 5/6: IMG_20250629_214514_439.jpg ---\n",
      "Extracting text...\n",
      "Extracted text: ٤ ؟ ي ا 0 0 ٤ ؟ [ ؟ ٠٤؟ ٤ 0 ٦٠ سی\n",
      "Detected language: fa\n",
      "Saved text to: extracted_text\\IMG_20250629_214514_439.txt\n",
      "\n",
      "--- Processing Image 6/6: textbook_img.jpg ---\n",
      "Extracting text...\n",
      "Extracted text: to the tribe or Sad bin Bakr bin Hawazen Their children became the Prophet s foster brothers and sisters Their names were Abdullah, Anisa, Prophet Haleemah's house is unexpectedly blessed Haleemah and her husband found their lives changed the moment took Muhammad home They had traveled 10 Makkah on frail female donkey that could barely keep up with thcir caravan the return journey however, as Haleemah rode with the infant in her arms, the same animal moved so swiftly that it left the caravan behind While Muhammad with Haleemah s family, the house overflowed with blessings Haleemah herself narrated that she brought Muhammad to her home during drought  Her she camel would not give a of milk Haleemahs child would cry the whole night out of hunger With the child so distraught, Haleemah and Harith found it hard [0 night changed, however, when Haleemah brought Muhammad home and placed him on her Her breasts overfilowed with milk so that both Muhammad and her own child drank their fill of milk and fell fast When Harith went to the she camel he was amazed at what he saw The she camels udders were full of milk and ready to overflow It gave so much milk that Haleemahs family was able to that night on full stomachs Haleemahs household suddenly appeared [0 be untouched by the drought, although in Dayar Banu Sa`d, the most drought stricken spot in the region The familys goats would return from grazing with their stomachs full of grass and their udders bursting with WHEN THE MOON 23 and thcy On stayed sleep Things lap  asleep  sleep they\n",
      "Detected language: en\n",
      "Saved text to: extracted_text\\textbook_img.txt\n",
      "\n",
      "--- OCR Processing Complete ---\n",
      "\n",
      "Found 3 text files to read\n",
      "\n",
      "--- Reading File 1/3: IMG_20250629_214324_528.txt ---\n",
      "Language: en\n",
      "Text content: 4: Employee Hierarchy ٥. Employee ( parent ): 0 Manager, like name, id; method like get_ role specif...\n",
      "Using voice: Microsoft David Desktop - English (United States)\n",
      "Reading text aloud...\n",
      "\n",
      "--- Reading File 2/3: IMG_20250629_214514_439.txt ---\n",
      "Language: fa\n",
      "Text content: ٤ ؟ ي ا 0 0 ٤ ؟ [ ؟ ٠٤؟ ٤ 0 ٦٠ سی\n",
      "Using voice: Microsoft David Desktop - English (United States)\n",
      "Reading text aloud...\n",
      "\n",
      "--- Reading File 3/3: textbook_img.txt ---\n",
      "Language: en\n",
      "Text content: to the tribe or Sad bin Bakr bin Hawazen Their children became the Prophet s foster brothers and sis...\n",
      "Using voice: Microsoft David Desktop - English (United States)\n",
      "Reading text aloud...\n",
      "\n",
      "--- TTS Reading Complete ---\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import easyocr\n",
    "import torch\n",
    "import pyttsx3\n",
    "from langdetect import detect\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Check for GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize EasyOCR reader with GPU support\n",
    "reader = easyocr.Reader([\"ar\", \"ur\", \"en\"], gpu=torch.cuda.is_available())\n",
    "\n",
    "# Initialize TTS engine\n",
    "tts_engine = pyttsx3.init()\n",
    "\n",
    "def process_images_ocr_save_text(images_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process all images in the folder, extract text with OCR, and save to text files\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "        print(f\"Created output folder: {output_folder}\")\n",
    "    \n",
    "    # Get all image files in the folder\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        image_filename = os.path.basename(image_path)\n",
    "        base_name = os.path.splitext(image_filename)[0]\n",
    "        text_file_path = os.path.join(output_folder, f\"{base_name}.txt\")\n",
    "        \n",
    "        print(f\"\\n--- Processing Image {i}/{len(image_files)}: {image_filename} ---\")\n",
    "\n",
    "        try:\n",
    "            # Read image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Could not read image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Extract text using EasyOCR\n",
    "            print(\"Extracting text...\")\n",
    "            results = reader.readtext(image)\n",
    "\n",
    "            # Combine all detected text\n",
    "            extracted_text = \" \".join(\n",
    "                [result[1] for result in results if result[2] > 0.5]\n",
    "            )  # confidence > 0.5\n",
    "\n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Extracted text: {extracted_text}\")\n",
    "            \n",
    "            # Try to detect language\n",
    "            try:\n",
    "                detected_lang = detect(extracted_text)\n",
    "                print(f\"Detected language: {detected_lang}\")\n",
    "                \n",
    "                # Save text with language information to file\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                    text_file.write(f\"LANG:{detected_lang}\\n\")\n",
    "                    text_file.write(extracted_text)\n",
    "                \n",
    "                print(f\"Saved text to: {text_file_path}\")\n",
    "                \n",
    "            except Exception as lang_error:\n",
    "                print(f\"Language detection failed: {lang_error}\")\n",
    "                # Save text without language information\n",
    "                with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
    "                    text_file.write(f\"LANG:unknown\\n\")\n",
    "                    text_file.write(extracted_text)\n",
    "                \n",
    "                print(f\"Saved text to: {text_file_path} (language unknown)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n--- OCR Processing Complete ---\")\n",
    "    return output_folder\n",
    "\n",
    "def read_text_files_aloud(text_folder):\n",
    "    \"\"\"\n",
    "    Read all text files in the folder aloud using TTS\n",
    "    \"\"\"\n",
    "    # Get all text files\n",
    "    text_files = glob.glob(os.path.join(text_folder, \"*.txt\"))\n",
    "    \n",
    "    if not text_files:\n",
    "        print(f\"No text files found in {text_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(text_files)} text files to read\")\n",
    "    \n",
    "    # Language mapping for TTS\n",
    "    lang_mapping = {\n",
    "        \"en\": \"english\",\n",
    "        \"ar\": \"arabic\",\n",
    "        \"ur\": \"urdu\",\n",
    "        \"hi\": \"hindi\",\n",
    "    }\n",
    "    \n",
    "    for i, text_file_path in enumerate(text_files, 1):\n",
    "        file_name = os.path.basename(text_file_path)\n",
    "        print(f\"\\n--- Reading File {i}/{len(text_files)}: {file_name} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Read text file\n",
    "            with open(text_file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                \n",
    "            if not lines:\n",
    "                print(f\"File is empty: {text_file_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Extract language information from first line\n",
    "            lang_line = lines[0].strip()\n",
    "            if lang_line.startswith(\"LANG:\"):\n",
    "                detected_lang = lang_line[5:]\n",
    "                print(f\"Language: {detected_lang}\")\n",
    "                # Remove the language line\n",
    "                content = \"\".join(lines[1:])\n",
    "            else:\n",
    "                # No language information, treat all lines as content\n",
    "                detected_lang = \"unknown\"\n",
    "                content = \"\".join(lines)\n",
    "            \n",
    "            if not content.strip():\n",
    "                print(\"No content to read\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Text content: {content[:100]}...\" if len(content) > 100 else f\"Text content: {content}\")\n",
    "            \n",
    "            # Get available voices\n",
    "            voices = tts_engine.getProperty(\"voices\")\n",
    "            \n",
    "            # Try to set appropriate voice based on language\n",
    "            voice_set = False\n",
    "            for voice in voices:\n",
    "                tts_lang = lang_mapping.get(detected_lang, \"english\")\n",
    "                if (tts_lang.lower() in voice.name.lower() or \n",
    "                    detected_lang in voice.id.lower()):\n",
    "                    tts_engine.setProperty(\"voice\", voice.id)\n",
    "                    voice_set = True\n",
    "                    print(f\"Using voice: {voice.name}\")\n",
    "                    break\n",
    "            \n",
    "            if not voice_set:\n",
    "                print(f\"No specific voice found for {detected_lang}, using default\")\n",
    "            \n",
    "            # Adjust speech rate\n",
    "            tts_engine.setProperty(\"rate\", 150)  # Adjust speed as needed\n",
    "            \n",
    "            # Read the text aloud\n",
    "            print(f\"Reading text aloud...\")\n",
    "            tts_engine.say(content)\n",
    "            tts_engine.runAndWait()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {text_file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n--- TTS Reading Complete ---\")\n",
    "\n",
    "# Main process\n",
    "if __name__ == \"__main__\":\n",
    "    # Define folders\n",
    "    images_folder = \"part_2_images\"\n",
    "    output_folder = \"extracted_text\"\n",
    "    \n",
    "    # Step 1: Process images with OCR and save text\n",
    "    text_folder = process_images_ocr_save_text(images_folder, output_folder)\n",
    "    \n",
    "    # Step 2: Read the saved text files aloud\n",
    "    read_text_files_aloud(text_folder)\n",
    "    \n",
    "    print(\"Process completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
