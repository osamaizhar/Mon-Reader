{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pbyd9jj45vc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "NVIDIA GPU detected by system:\n",
      "|   0  NVIDIA GeForce RTX 2060      WDDM  |   00000000:01:00.0  On |                  N/A |\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA installation and GPU detection\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Check if nvidia-smi works\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"NVIDIA GPU detected by system:\")\n",
    "        print(result.stdout.split(\"\\n\")[8])  # GPU info line\n",
    "    else:\n",
    "        print(\"nvidia-smi failed - no NVIDIA driver detected\")\n",
    "except FileNotFoundError:\n",
    "    print(\"nvidia-smi not found - NVIDIA drivers may not be installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ef0d6",
   "metadata": {},
   "source": [
    "# 1. With Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b886e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import easyocr\n",
    "import torch\n",
    "import pyttsx3\n",
    "from langdetect import detect\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Check for GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize EasyOCR reader with GPU support\n",
    "reader = easyocr.Reader([\"ar\", \"ur\", \"en\"], gpu=torch.cuda.is_available())\n",
    "\n",
    "# Initialize TTS engine\n",
    "tts_engine = pyttsx3.init()\n",
    "\n",
    "\n",
    "def process_images_with_ocr_tts(images_folder):\n",
    "    \"\"\"\n",
    "    Process all images in the folder, extract text, detect language, and read aloud\n",
    "    \"\"\"\n",
    "    # Get all image files in the folder\n",
    "    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\"]\n",
    "    image_files = []\n",
    "\n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext)))\n",
    "        image_files.extend(glob.glob(os.path.join(images_folder, ext.upper())))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_folder}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "\n",
    "    for i, image_path in enumerate(image_files, 1):\n",
    "        print(\n",
    "            f\"\\n--- Processing Image {i}/{len(image_files)}: {os.path.basename(image_path)} ---\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Read image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Could not read image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Extract text using EasyOCR\n",
    "            print(\"Extracting text...\")\n",
    "            results = reader.readtext(image)\n",
    "\n",
    "            # Combine all detected text\n",
    "            extracted_text = \" \".join(\n",
    "                [result[1] for result in results if result[2] > 0.5]\n",
    "            )  # confidence > 0.5\n",
    "\n",
    "            if not extracted_text.strip():\n",
    "                print(\"No text detected in this image\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Extracted text: {extracted_text}\")\n",
    "\n",
    "            # Detect language\n",
    "            try:\n",
    "                detected_lang = detect(extracted_text)\n",
    "                print(f\"Detected language: {detected_lang}\")\n",
    "\n",
    "                # Language mapping for TTS\n",
    "                lang_mapping = {\n",
    "                    \"en\": \"english\",\n",
    "                    \"ar\": \"arabic\",\n",
    "                    \"ur\": \"urdu\",\n",
    "                    \"hi\": \"hindi\",\n",
    "                }\n",
    "\n",
    "                # Set TTS language if available\n",
    "                tts_lang = lang_mapping.get(detected_lang, \"english\")\n",
    "\n",
    "                # Get available voices\n",
    "                voices = tts_engine.getProperty(\"voices\")\n",
    "\n",
    "                # Try to set appropriate voice based on language\n",
    "                voice_set = False\n",
    "                for voice in voices:\n",
    "                    if (\n",
    "                        tts_lang.lower() in voice.name.lower()\n",
    "                        or detected_lang in voice.id.lower()\n",
    "                    ):\n",
    "                        tts_engine.setProperty(\"voice\", voice.id)\n",
    "                        voice_set = True\n",
    "                        break\n",
    "\n",
    "                if not voice_set:\n",
    "                    print(f\"No specific voice found for {detected_lang}, using default\")\n",
    "\n",
    "                # Adjust speech rate\n",
    "                tts_engine.setProperty(\"rate\", 150)  # Adjust speed as needed\n",
    "\n",
    "                # Read the text aloud\n",
    "                print(f\"Reading text aloud in {detected_lang}...\")\n",
    "                tts_engine.say(extracted_text)\n",
    "                tts_engine.runAndWait()\n",
    "\n",
    "            except Exception as lang_error:\n",
    "                print(f\"Language detection failed: {lang_error}\")\n",
    "                print(\"Reading with default language...\")\n",
    "                tts_engine.say(extracted_text)\n",
    "                tts_engine.runAndWait()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\n--- Processing Complete ---\")\n",
    "\n",
    "\n",
    "# Process images in the part_2_images folder\n",
    "images_folder = \"part_2_images\"\n",
    "process_images_with_ocr_tts(images_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2fc556",
   "metadata": {},
   "source": [
    "# Paddle OCR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15762af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Temp\\ipykernel_42524\\208229499.py:17: DeprecationWarning: The parameter `use_angle_cls` has been deprecated and will be removed in the future. Please use `use_textline_orientation` instead.\n",
      "  ocr_en = PaddleOCR(use_angle_cls=True, lang=\"en\")\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 3692.17it/s]\n",
      "\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[33mThe model(UVDoc) is not supported to run in MKLDNN mode! Using `paddle` instead!\u001b[0m\n",
      "\u001b[32mUsing official model (UVDoc), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[33mThe model(UVDoc) is not supported to run in MKLDNN mode! Using `paddle` instead!\u001b[0m\n",
      "\u001b[32mUsing official model (UVDoc), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 859.72it/s]\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 8 files: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 14454.81it/s]\n",
      "\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_rec', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_rec', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "\n",
      "C:\\Users\\Osama\\AppData\\Local\\Temp\\ipykernel_42524\\208229499.py:18: DeprecationWarning: The parameter `use_angle_cls` has been deprecated and will be removed in the future. Please use `use_textline_orientation` instead.\n",
      "  ocr_ar = PaddleOCR(use_angle_cls=True, lang=\"ar\")\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "C:\\Users\\Osama\\AppData\\Local\\Temp\\ipykernel_42524\\208229499.py:18: DeprecationWarning: The parameter `use_angle_cls` has been deprecated and will be removed in the future. Please use `use_textline_orientation` instead.\n",
      "  ocr_ar = PaddleOCR(use_angle_cls=True, lang=\"ar\")\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[33mThe model(UVDoc) is not supported to run in MKLDNN mode! Using `paddle` instead!\u001b[0m\n",
      "\u001b[32mUsing official model (UVDoc), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[33mThe model(UVDoc) is not supported to run in MKLDNN mode! Using `paddle` instead!\u001b[0m\n",
      "\u001b[32mUsing official model (UVDoc), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 8 files: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[32mCreating model: ('PP-OCRv3_mobile_det', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv3_mobile_det), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv3_mobile_det', None)\u001b[0m\n",
      "\u001b[32mUsing official model (PP-OCRv3_mobile_det), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[32mCreating model: ('arabic_PP-OCRv3_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mUsing official model (arabic_PP-OCRv3_mobile_rec), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "\u001b[32mCreating model: ('arabic_PP-OCRv3_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mUsing official model (arabic_PP-OCRv3_mobile_rec), the model files will be automatically downloaded and saved in C:\\Users\\Osama\\.paddlex\\official_models.\u001b[0m\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing: IMG_20250629_214324_528.jpg ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Temp\\ipykernel_42524\\208229499.py:21: DeprecationWarning: Please use `predict` instead.\n",
      "  result_en = ocr_en.ocr(image_path)\n",
      "C:\\Users\\Osama\\AppData\\Local\\Temp\\ipykernel_42524\\208229499.py:27: DeprecationWarning: Please use `predict` instead.\n",
      "  result_ar = ocr_ar.ocr(image_path)\n",
      "C:\\Users\\Osama\\AppData\\Local\\Temp\\ipykernel_42524\\208229499.py:27: DeprecationWarning: Please use `predict` instead.\n",
      "  result_ar = ocr_ar.ocr(image_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language (Groq): en\n",
      "Extracted text: n a o t o e e e e e e i e e\n",
      "Speaking with Groq TTS (Adelaide-PlayAI)...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BinaryAPIResponse' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Save each speech file with a unique name per image\u001b[39;00m\n\u001b[32m     82\u001b[39m wav_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtts_outputs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath(img_file).stem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.wav\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[43mtts_play\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwav_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mtts_play\u001b[39m\u001b[34m(text, wav_path)\u001b[39m\n\u001b[32m     60\u001b[39m speech_file_path.parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(speech_file_path, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     f.write(\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m)\n\u001b[32m     63\u001b[39m playsound(\u001b[38;5;28mstr\u001b[39m(speech_file_path))\n",
      "\u001b[31mAttributeError\u001b[39m: 'BinaryAPIResponse' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from paddleocr import PaddleOCR\n",
    "from groq import Groq\n",
    "from playsound import playsound\n",
    "import requests\n",
    "\n",
    "# --- Set up ---\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_API_URL = os.getenv(\n",
    "    \"GROQ_API_URL\", \"https://api.groq.com/openai/v1/chat/completions\"\n",
    ")\n",
    "images_folder = \"part_2_images\"\n",
    "image_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\"]\n",
    "\n",
    "# PaddleOCR for English and Arabic (Urdu best handled by Arabic model)\n",
    "ocr_en = PaddleOCR(use_angle_cls=True, lang=\"en\")\n",
    "ocr_ar = PaddleOCR(use_angle_cls=True, lang=\"ar\")\n",
    "\n",
    "def extract_text(image_path):\n",
    "    result_en = ocr_en.ocr(image_path)\n",
    "    text_en = (\n",
    "        \" \".join([line[1][0] for line in result_en[0]])\n",
    "        if result_en and result_en[0]\n",
    "        else \"\"\n",
    "    )\n",
    "    result_ar = ocr_ar.ocr(image_path)\n",
    "    text_ar = (\n",
    "        \" \".join([line[1][0] for line in result_ar[0]])\n",
    "        if result_ar and result_ar[0]\n",
    "        else \"\"\n",
    "    )\n",
    "    return text_ar if len(text_ar) > len(text_en) else text_en\n",
    "\n",
    "def detect_language_groq(text):\n",
    "    if not GROQ_API_KEY:\n",
    "        raise RuntimeError(\"Set your GROQ_API_KEY environment variable!\")\n",
    "    prompt = f\"Detect the language of the following text and respond with only the ISO 639-1 language code:\\n\\n{text}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {GROQ_API_KEY}\"}\n",
    "    data = {\n",
    "        \"model\": \"llama3-8b-8192\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    }\n",
    "    response = requests.post(GROQ_API_URL, headers=headers, json=data)\n",
    "    response.raise_for_status()\n",
    "    lang_code = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    return lang_code\n",
    "\n",
    "def tts_play(text, wav_path=\"tts_outputs/speech.wav\"):\n",
    "    print(\"Speaking with Groq TTS (Adelaide-PlayAI)...\")\n",
    "    client = Groq(api_key=GROQ_API_KEY)\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"playai-tts\",\n",
    "        voice=\"Adelaide-PlayAI\",\n",
    "        response_format=\"wav\",\n",
    "        input=text,\n",
    "    )\n",
    "    # Ensure output directory exists\n",
    "    speech_file_path = Path(wav_path)\n",
    "    speech_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(speech_file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    playsound(str(speech_file_path))\n",
    "\n",
    "for img_file in os.listdir(images_folder):\n",
    "    if not any(img_file.lower().endswith(ext) for ext in image_extensions):\n",
    "        continue\n",
    "    img_path = os.path.join(images_folder, img_file)\n",
    "    print(f\"\\n--- Processing: {img_file} ---\")\n",
    "    text = extract_text(img_path)\n",
    "    if not text.strip():\n",
    "        print(\"No text detected.\")\n",
    "        continue\n",
    "    try:\n",
    "        lang = detect_language_groq(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Language detection failed: {e}\")\n",
    "        lang = \"unknown\"\n",
    "    print(f\"Detected language (Groq): {lang}\")\n",
    "    print(f\"Extracted text: {text}\")\n",
    "    # Save each speech file with a unique name per image\n",
    "    wav_path = f\"tts_outputs/{Path(img_file).stem}_{lang}.wav\"\n",
    "    tts_play(text, wav_path=wav_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527c05a",
   "metadata": {},
   "source": [
    "# Groq + Live Kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d5e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import pytesseract\n",
    "# import requests\n",
    "\n",
    "# # Load secrets from .env or set directly\n",
    "# GROQ_API_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "# GROQ_API_KEY = \"your_groq_api_key\"\n",
    "# LIVEKIT_TTS_URL = \"https://your-livekit-server.com/tts\"\n",
    "# LIVEKIT_API_KEY = \"your_livekit_api_key\"\n",
    "\n",
    "# img_folder = \"part_2_images\"\n",
    "# output_folder = \"tts_outputs\"\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "# def detect_language_groq(text):\n",
    "#     prompt = f\"Detect the language of the following text and respond with only the ISO 639-1 language code:\\n\\n{text}\"\n",
    "#     headers = {\"Authorization\": f\"Bearer {GROQ_API_KEY}\"}\n",
    "#     data = {\n",
    "#         \"model\": \"llama3-8b-8192\",  # Or another Groq-supported model\n",
    "#         \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "#     }\n",
    "#     response = requests.post(GROQ_API_URL, headers=headers, json=data)\n",
    "#     lang_code = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "#     return lang_code\n",
    "\n",
    "\n",
    "# for img_file in os.listdir(img_folder):\n",
    "#     if img_file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "#         img_path = os.path.join(img_folder, img_file)\n",
    "#         img = cv2.imread(img_path)\n",
    "#         text = pytesseract.image_to_string(img)\n",
    "#         print(f\"\\nText from {img_file}:\\n{text}\")\n",
    "\n",
    "#         if text.strip():\n",
    "#             try:\n",
    "#                 lang = detect_language_groq(text)\n",
    "#                 print(f\"Detected language (Groq): {lang}\")\n",
    "\n",
    "#                 # --- LiveKit TTS API Call ---\n",
    "#                 tts_payload = {\"text\": text, \"voice\": \"default\", \"lang\": lang}\n",
    "#                 headers = {\"Authorization\": f\"Bearer {LIVEKIT_API_KEY}\"}\n",
    "#                 response = requests.post(\n",
    "#                     LIVEKIT_TTS_URL, json=tts_payload, headers=headers\n",
    "#                 )\n",
    "#                 if response.status_code == 200:\n",
    "#                     audio_path = os.path.join(\n",
    "#                         output_folder, f\"{os.path.splitext(img_file)[0]}_{lang}.mp3\"\n",
    "#                     )\n",
    "#                     with open(audio_path, \"wb\") as f:\n",
    "#                         f.write(response.content)\n",
    "#                     print(f\"TTS audio saved: {audio_path}\")\n",
    "#                 else:\n",
    "#                     print(f\"LiveKit TTS error: {response.status_code} {response.text}\")\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {img_file}: {e}\")\n",
    "#         else:\n",
    "#             print(f\"No text detected in {img_file}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
